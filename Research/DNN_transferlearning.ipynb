{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 載入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_coordinates = {\n",
    "    \"1-1\": (0, 0), \"1-2\": (0.6, 0), \"1-3\": (1.2, 0), \"1-4\": (1.8, 0), \"1-5\": (2.4, 0), \"1-6\": (3.0, 0),\"1-7\": (3.6, 0), \"1-8\": (4.2, 0), \"1-9\": (4.8, 0), \"1-10\": (5.4, 0), \"1-11\": (6.0, 0),\n",
    "    \"2-1\": (0, 0.6), \"2-11\": (6.0, 0.6),\n",
    "    \"3-1\": (0, 1.2), \"3-11\": (6.0, 1.2),\n",
    "    \"4-1\": (0, 1.8), \"4-11\": (6.0, 1.8),\n",
    "    \"5-1\": (0, 2.4), \"5-11\": (6.0, 2.4),\n",
    "    \"6-1\": (0, 3.0), \"6-2\": (0.6, 3.0), \"6-3\": (1.2, 3.0), \"6-4\": (1.8, 3.0), \"6-5\": (2.4, 3.0),\"6-6\": (3.0, 3.0), \"6-7\": (3.6, 3.0), \"6-8\": (4.2, 3.0), \"6-9\": (4.8, 3.0), \"6-10\": (5.4, 3.0), \"6-11\": (6.0, 3.0),\n",
    "    \"7-1\": (0, 3.6), \"7-11\": (6.0, 3.6),\n",
    "    \"8-1\": (0, 4.2), \"8-11\": (6.0, 4.2),\n",
    "    \"9-1\": (0, 4.8), \"9-11\": (6.0, 4.8),\n",
    "    \"10-1\": (0, 5.4), \"10-11\": (6.0, 5.4),\n",
    "    \"11-1\": (0, 6.0), \"11-2\": (0.6, 6.0), \"11-3\": (1.2, 6.0), \"11-4\": (1.8, 6.0), \"11-5\": (2.4, 6.0),\"11-6\": (3.0, 6.0), \"11-7\": (3.6, 6.0), \"11-8\": (4.2, 6.0), \"11-9\": (4.8, 6.0), \"11-10\": (5.4, 6.0), \"11-11\": (6.0, 6.0)\n",
    "}\n",
    "label_mapping = {\n",
    "    '11': '1-1','10': '1-2','9': '1-3','8': '1-4','7': '1-5','6': '1-6','5': '1-7','4': '1-8','3': '1-9','2': '1-10','1': '1-11',\n",
    "    '12': '2-1','30': '2-11',\n",
    "    '13': '3-1','29': '3-11',\n",
    "    '14': '4-1','28': '4-11',\n",
    "    '15': '5-1','27': '5-11',\n",
    "    '16': '6-1','17': '6-2','18': '6-3','19': '6-4','20': '6-5','21': '6-6','22': '6-7','23': '6-8','24': '6-9','25': '6-10','26': '6-11',\n",
    "    '49': '7-1','31': '7-11',\n",
    "    '48': '8-1','32': '8-11',\n",
    "    '47': '9-1','33': '9-11',\n",
    "    '46': '10-1','34': '10-11',\n",
    "    '45': '11-1','44': '11-2','43': '11-3','42': '11-4','41': '11-5','40': '11-6','39': '11-7','38': '11-8','37': '11-9','36': '11-10','35': '11-11'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import joblib  # 用於保存模型\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入模型與標準化器\n",
    "# base_model = load_model('Model_SAVE/4 mc AP RSSI FTM StdDev 2024-12-14 data train/DNN_best_model4mcAP.h5')\n",
    "base_model = load_model('transfer model/DNN_best_model4mcAP_1week_20dataperRP.h5')\n",
    "\n",
    "scaler = joblib.load('Model_SAVE/4 mc AP RSSI FTM StdDev 2024-12-14 data train/scaler_4mcAP.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_44 (Dense)            (None, 64)                832       \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 49)                3185      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,593\n",
      "Trainable params: 3,185\n",
      "Non-trainable params: 17,408\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 凍結所有層\n",
    "for layer in base_model.layers[:-1]:  # 除了最後一層 (Output Layer)\n",
    "    layer.trainable = False\n",
    "\n",
    "# 確認哪些層可訓練\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加入新資料 & 資料前處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = ['Label',\n",
    "                    'AP1_Distance (mm)','AP2_Distance (mm)','AP3_Distance (mm)','AP4_Distance (mm)',\n",
    "'AP1_StdDev (mm)','AP2_StdDev (mm)','AP3_StdDev (mm)','AP4_StdDev (mm)',\n",
    "                                'AP1_Rssi','AP2_Rssi','AP3_Rssi','AP4_Rssi']  \n",
    "\n",
    "X_testing_selected_columns = [\n",
    "        'AP1_Distance (mm)','AP2_Distance (mm)','AP3_Distance (mm)','AP4_Distance (mm)',\n",
    "        'AP1_StdDev (mm)','AP2_StdDev (mm)','AP3_StdDev (mm)','AP4_StdDev (mm)',\n",
    "                                'AP1_Rssi','AP2_Rssi','AP3_Rssi','AP4_Rssi']  \n",
    "\n",
    "label_column = 'Label'\n",
    "target_column = 'Label'\n",
    "#  'AP1_Rssi','AP2_Rssi','AP3_Rssi','AP4_Rssi'\n",
    "# 'AP1_Distance (mm)','AP2_Distance (mm)','AP3_Distance (mm)','AP4_Distance (mm)',\n",
    "# 'AP1_StdDev (mm)','AP2_StdDev (mm)','AP3_StdDev (mm)','AP4_StdDev (mm)',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>AP1_Distance (mm)</th>\n",
       "      <th>AP2_Distance (mm)</th>\n",
       "      <th>AP3_Distance (mm)</th>\n",
       "      <th>AP4_Distance (mm)</th>\n",
       "      <th>AP1_Rssi</th>\n",
       "      <th>AP2_Rssi</th>\n",
       "      <th>AP3_Rssi</th>\n",
       "      <th>AP4_Rssi</th>\n",
       "      <th>AP1_StdDev (mm)</th>\n",
       "      <th>AP2_StdDev (mm)</th>\n",
       "      <th>AP3_StdDev (mm)</th>\n",
       "      <th>AP4_StdDev (mm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1-1</td>\n",
       "      <td>3999.0</td>\n",
       "      <td>7339.0</td>\n",
       "      <td>14219.0</td>\n",
       "      <td>-303.0</td>\n",
       "      <td>-65.0</td>\n",
       "      <td>-68.0</td>\n",
       "      <td>-65.0</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>986.0</td>\n",
       "      <td>295.0</td>\n",
       "      <td>580.0</td>\n",
       "      <td>968.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1-1</td>\n",
       "      <td>3921.0</td>\n",
       "      <td>7368.0</td>\n",
       "      <td>12835.0</td>\n",
       "      <td>-420.0</td>\n",
       "      <td>-65.0</td>\n",
       "      <td>-68.0</td>\n",
       "      <td>-64.0</td>\n",
       "      <td>-54.0</td>\n",
       "      <td>1071.0</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>765.0</td>\n",
       "      <td>671.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1-1</td>\n",
       "      <td>4038.0</td>\n",
       "      <td>6928.0</td>\n",
       "      <td>12510.0</td>\n",
       "      <td>-651.0</td>\n",
       "      <td>-65.0</td>\n",
       "      <td>-69.0</td>\n",
       "      <td>-64.0</td>\n",
       "      <td>-52.0</td>\n",
       "      <td>986.0</td>\n",
       "      <td>375.0</td>\n",
       "      <td>562.0</td>\n",
       "      <td>770.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1-1</td>\n",
       "      <td>4038.0</td>\n",
       "      <td>7163.0</td>\n",
       "      <td>13955.0</td>\n",
       "      <td>-381.0</td>\n",
       "      <td>-65.0</td>\n",
       "      <td>-68.0</td>\n",
       "      <td>-65.0</td>\n",
       "      <td>-54.0</td>\n",
       "      <td>1042.0</td>\n",
       "      <td>267.0</td>\n",
       "      <td>527.0</td>\n",
       "      <td>555.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1-1</td>\n",
       "      <td>4077.0</td>\n",
       "      <td>7163.0</td>\n",
       "      <td>13281.0</td>\n",
       "      <td>-342.0</td>\n",
       "      <td>-65.0</td>\n",
       "      <td>-68.0</td>\n",
       "      <td>-64.0</td>\n",
       "      <td>-54.0</td>\n",
       "      <td>996.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>826.0</td>\n",
       "      <td>428.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20673</th>\n",
       "      <td>9-11</td>\n",
       "      <td>1694.0</td>\n",
       "      <td>5083.0</td>\n",
       "      <td>2666.0</td>\n",
       "      <td>5438.0</td>\n",
       "      <td>-55.0</td>\n",
       "      <td>-67.0</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>1093.0</td>\n",
       "      <td>369.0</td>\n",
       "      <td>2384.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20674</th>\n",
       "      <td>9-11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5229.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-65.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1125.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20675</th>\n",
       "      <td>9-11</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>5346.0</td>\n",
       "      <td>2627.0</td>\n",
       "      <td>4696.0</td>\n",
       "      <td>-55.0</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>1073.0</td>\n",
       "      <td>482.0</td>\n",
       "      <td>463.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20676</th>\n",
       "      <td>9-11</td>\n",
       "      <td>1929.0</td>\n",
       "      <td>5434.0</td>\n",
       "      <td>2549.0</td>\n",
       "      <td>4735.0</td>\n",
       "      <td>-55.0</td>\n",
       "      <td>-65.0</td>\n",
       "      <td>-51.0</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>964.0</td>\n",
       "      <td>568.0</td>\n",
       "      <td>481.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20677</th>\n",
       "      <td>9-11</td>\n",
       "      <td>1811.0</td>\n",
       "      <td>5376.0</td>\n",
       "      <td>2165.0</td>\n",
       "      <td>4705.0</td>\n",
       "      <td>-55.0</td>\n",
       "      <td>-65.0</td>\n",
       "      <td>-46.0</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>925.0</td>\n",
       "      <td>726.0</td>\n",
       "      <td>591.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20678 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Label  AP1_Distance (mm)  AP2_Distance (mm)  AP3_Distance (mm)  \\\n",
       "0       1-1             3999.0             7339.0            14219.0   \n",
       "1       1-1             3921.0             7368.0            12835.0   \n",
       "2       1-1             4038.0             6928.0            12510.0   \n",
       "3       1-1             4038.0             7163.0            13955.0   \n",
       "4       1-1             4077.0             7163.0            13281.0   \n",
       "...     ...                ...                ...                ...   \n",
       "20673  9-11             1694.0             5083.0             2666.0   \n",
       "20674  9-11                NaN             5229.0                NaN   \n",
       "20675  9-11             2007.0             5346.0             2627.0   \n",
       "20676  9-11             1929.0             5434.0             2549.0   \n",
       "20677  9-11             1811.0             5376.0             2165.0   \n",
       "\n",
       "       AP4_Distance (mm)  AP1_Rssi  AP2_Rssi  AP3_Rssi  AP4_Rssi  \\\n",
       "0                 -303.0     -65.0     -68.0     -65.0     -50.0   \n",
       "1                 -420.0     -65.0     -68.0     -64.0     -54.0   \n",
       "2                 -651.0     -65.0     -69.0     -64.0     -52.0   \n",
       "3                 -381.0     -65.0     -68.0     -65.0     -54.0   \n",
       "4                 -342.0     -65.0     -68.0     -64.0     -54.0   \n",
       "...                  ...       ...       ...       ...       ...   \n",
       "20673             5438.0     -55.0     -67.0     -50.0     -66.0   \n",
       "20674                NaN       NaN     -65.0       NaN       NaN   \n",
       "20675             4696.0     -55.0     -66.0     -50.0     -66.0   \n",
       "20676             4735.0     -55.0     -65.0     -51.0     -66.0   \n",
       "20677             4705.0     -55.0     -65.0     -46.0     -66.0   \n",
       "\n",
       "       AP1_StdDev (mm)  AP2_StdDev (mm)  AP3_StdDev (mm)  AP4_StdDev (mm)  \n",
       "0                986.0            295.0            580.0            968.0  \n",
       "1               1071.0           1013.0            765.0            671.0  \n",
       "2                986.0            375.0            562.0            770.0  \n",
       "3               1042.0            267.0            527.0            555.0  \n",
       "4                996.0            137.0            826.0            428.0  \n",
       "...                ...              ...              ...              ...  \n",
       "20673            294.0           1093.0            369.0           2384.0  \n",
       "20674              NaN           1125.0              NaN              NaN  \n",
       "20675            152.0           1073.0            482.0            463.0  \n",
       "20676            216.0            964.0            568.0            481.0  \n",
       "20677            224.0            925.0            726.0            591.0  \n",
       "\n",
       "[20678 rows x 13 columns]"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 讀取測試資料\n",
    "test_file_path = \"timestamp_allignment_Balanced_2024_12_27_rtt_logs.csv\"  # 測試資料的檔案名稱\n",
    "date_test = \"2024_12_27\"\n",
    "modelname = \"DNN 4 mcAPs BEST\"\n",
    "test_data = pd.read_csv(test_file_path, usecols=selected_columns)\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with NaN values: 974\n",
      "Number of rows with NaN values: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>level_1</th>\n",
       "      <th>AP1_Distance (mm)</th>\n",
       "      <th>AP2_Distance (mm)</th>\n",
       "      <th>AP3_Distance (mm)</th>\n",
       "      <th>AP4_Distance (mm)</th>\n",
       "      <th>AP1_Rssi</th>\n",
       "      <th>AP2_Rssi</th>\n",
       "      <th>AP3_Rssi</th>\n",
       "      <th>AP4_Rssi</th>\n",
       "      <th>AP1_StdDev (mm)</th>\n",
       "      <th>AP2_StdDev (mm)</th>\n",
       "      <th>AP3_StdDev (mm)</th>\n",
       "      <th>AP4_StdDev (mm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1-1</td>\n",
       "      <td>0</td>\n",
       "      <td>4038.0</td>\n",
       "      <td>7358.0</td>\n",
       "      <td>13018.0</td>\n",
       "      <td>-1855.0</td>\n",
       "      <td>-65.0</td>\n",
       "      <td>-68.0</td>\n",
       "      <td>-64.0</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>965.0</td>\n",
       "      <td>877.0</td>\n",
       "      <td>314.0</td>\n",
       "      <td>2003.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1-1</td>\n",
       "      <td>1</td>\n",
       "      <td>3960.0</td>\n",
       "      <td>7241.0</td>\n",
       "      <td>13252.0</td>\n",
       "      <td>-342.0</td>\n",
       "      <td>-65.0</td>\n",
       "      <td>-68.0</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>-54.0</td>\n",
       "      <td>1056.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>483.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1-1</td>\n",
       "      <td>2</td>\n",
       "      <td>3843.0</td>\n",
       "      <td>7202.0</td>\n",
       "      <td>13838.0</td>\n",
       "      <td>-303.0</td>\n",
       "      <td>-65.0</td>\n",
       "      <td>-68.0</td>\n",
       "      <td>-64.0</td>\n",
       "      <td>-54.0</td>\n",
       "      <td>1053.0</td>\n",
       "      <td>876.0</td>\n",
       "      <td>714.0</td>\n",
       "      <td>583.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1-1</td>\n",
       "      <td>3</td>\n",
       "      <td>3960.0</td>\n",
       "      <td>7397.0</td>\n",
       "      <td>14307.0</td>\n",
       "      <td>-342.0</td>\n",
       "      <td>-65.0</td>\n",
       "      <td>-68.0</td>\n",
       "      <td>-64.0</td>\n",
       "      <td>-54.0</td>\n",
       "      <td>938.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>265.0</td>\n",
       "      <td>522.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1-1</td>\n",
       "      <td>4</td>\n",
       "      <td>3999.0</td>\n",
       "      <td>7075.0</td>\n",
       "      <td>13174.0</td>\n",
       "      <td>-460.0</td>\n",
       "      <td>-65.0</td>\n",
       "      <td>-68.0</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>-54.0</td>\n",
       "      <td>959.0</td>\n",
       "      <td>303.0</td>\n",
       "      <td>341.0</td>\n",
       "      <td>637.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19693</th>\n",
       "      <td>9-11</td>\n",
       "      <td>19693</td>\n",
       "      <td>1850.0</td>\n",
       "      <td>5464.0</td>\n",
       "      <td>2666.0</td>\n",
       "      <td>4852.0</td>\n",
       "      <td>-55.0</td>\n",
       "      <td>-65.0</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>-67.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>959.0</td>\n",
       "      <td>409.0</td>\n",
       "      <td>497.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19694</th>\n",
       "      <td>9-11</td>\n",
       "      <td>19694</td>\n",
       "      <td>1967.0</td>\n",
       "      <td>5112.0</td>\n",
       "      <td>2353.0</td>\n",
       "      <td>4881.0</td>\n",
       "      <td>-55.0</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>-67.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>1158.0</td>\n",
       "      <td>709.0</td>\n",
       "      <td>577.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19695</th>\n",
       "      <td>9-11</td>\n",
       "      <td>19695</td>\n",
       "      <td>1968.0</td>\n",
       "      <td>5024.0</td>\n",
       "      <td>2366.0</td>\n",
       "      <td>4705.0</td>\n",
       "      <td>-55.0</td>\n",
       "      <td>-67.0</td>\n",
       "      <td>-48.0</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>1166.0</td>\n",
       "      <td>671.0</td>\n",
       "      <td>491.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19696</th>\n",
       "      <td>9-11</td>\n",
       "      <td>19696</td>\n",
       "      <td>1772.0</td>\n",
       "      <td>5434.0</td>\n",
       "      <td>2666.0</td>\n",
       "      <td>5008.0</td>\n",
       "      <td>-55.0</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>-52.0</td>\n",
       "      <td>-67.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>861.0</td>\n",
       "      <td>519.0</td>\n",
       "      <td>483.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19697</th>\n",
       "      <td>9-11</td>\n",
       "      <td>19697</td>\n",
       "      <td>1850.0</td>\n",
       "      <td>5229.0</td>\n",
       "      <td>2627.0</td>\n",
       "      <td>5789.0</td>\n",
       "      <td>-55.0</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>-51.0</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>1062.0</td>\n",
       "      <td>572.0</td>\n",
       "      <td>2298.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19698 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Label  level_1  AP1_Distance (mm)  AP2_Distance (mm)  AP3_Distance (mm)  \\\n",
       "0       1-1        0             4038.0             7358.0            13018.0   \n",
       "1       1-1        1             3960.0             7241.0            13252.0   \n",
       "2       1-1        2             3843.0             7202.0            13838.0   \n",
       "3       1-1        3             3960.0             7397.0            14307.0   \n",
       "4       1-1        4             3999.0             7075.0            13174.0   \n",
       "...     ...      ...                ...                ...                ...   \n",
       "19693  9-11    19693             1850.0             5464.0             2666.0   \n",
       "19694  9-11    19694             1967.0             5112.0             2353.0   \n",
       "19695  9-11    19695             1968.0             5024.0             2366.0   \n",
       "19696  9-11    19696             1772.0             5434.0             2666.0   \n",
       "19697  9-11    19697             1850.0             5229.0             2627.0   \n",
       "\n",
       "       AP4_Distance (mm)  AP1_Rssi  AP2_Rssi  AP3_Rssi  AP4_Rssi  \\\n",
       "0                -1855.0     -65.0     -68.0     -64.0     -50.0   \n",
       "1                 -342.0     -65.0     -68.0     -66.0     -54.0   \n",
       "2                 -303.0     -65.0     -68.0     -64.0     -54.0   \n",
       "3                 -342.0     -65.0     -68.0     -64.0     -54.0   \n",
       "4                 -460.0     -65.0     -68.0     -66.0     -54.0   \n",
       "...                  ...       ...       ...       ...       ...   \n",
       "19693             4852.0     -55.0     -65.0     -50.0     -67.0   \n",
       "19694             4881.0     -55.0     -66.0     -50.0     -67.0   \n",
       "19695             4705.0     -55.0     -67.0     -48.0     -66.0   \n",
       "19696             5008.0     -55.0     -66.0     -52.0     -67.0   \n",
       "19697             5789.0     -55.0     -66.0     -51.0     -66.0   \n",
       "\n",
       "       AP1_StdDev (mm)  AP2_StdDev (mm)  AP3_StdDev (mm)  AP4_StdDev (mm)  \n",
       "0                965.0            877.0            314.0           2003.0  \n",
       "1               1056.0            880.0            160.0            483.0  \n",
       "2               1053.0            876.0            714.0            583.0  \n",
       "3                938.0            105.0            265.0            522.0  \n",
       "4                959.0            303.0            341.0            637.0  \n",
       "...                ...              ...              ...              ...  \n",
       "19693            215.0            959.0            409.0            497.0  \n",
       "19694            141.0           1158.0            709.0            577.0  \n",
       "19695            175.0           1166.0            671.0            491.0  \n",
       "19696            151.0            861.0            519.0            483.0  \n",
       "19697            197.0           1062.0            572.0           2298.0  \n",
       "\n",
       "[19698 rows x 14 columns]"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 資料前處理 (一): 刪除前後n筆資料\n",
    "n = 10\n",
    "# 確保依據Label排序\n",
    "test_data = test_data.sort_values(by=label_column).reset_index(drop=True)\n",
    "\n",
    "# 建立一個空的 DataFrame 用於存放處理後的資料\n",
    "test_processed_data = pd.DataFrame(columns=test_data.columns)\n",
    "\n",
    "# 針對每個Label群組進行處理\n",
    "for label, group in test_data.groupby(label_column):\n",
    "    # 刪除前n筆和後n筆資料\n",
    "    if len(group) > 2 * n:  # 確保群組資料足夠\n",
    "        group = group.iloc[n:-n]\n",
    "    else:\n",
    "        group = pd.DataFrame()  # 若資料不足，刪除整個群組\n",
    "    # 將處理後的群組資料加入\n",
    "    test_processed_data = pd.concat([test_processed_data, group], ignore_index=True)\n",
    "\n",
    "# test_processed_data\n",
    "# Calculate the number of rows with NaN values\n",
    "nan_rows = test_processed_data.isnull().any(axis=1).sum()\n",
    "\n",
    "# Print the result\n",
    "print(f\"Number of rows with NaN values: {nan_rows}\")\n",
    "\n",
    "# 找出包含 NaN 的列\n",
    "rows_with_nan = test_processed_data[test_processed_data.isnull().any(axis=1)]\n",
    "\n",
    "# # 印出這些列\n",
    "# print(\"Rows with NaN values:\")\n",
    "# print(rows_with_nan)\n",
    "test_data_imputed = test_processed_data.groupby(label_column).apply(\n",
    "    lambda group: group.fillna(group.mean())\n",
    ").reset_index()\n",
    "\n",
    "# Calculate the number of rows with NaN values\n",
    "nan_rows = test_data_imputed.isnull().any(axis=1).sum()\n",
    "\n",
    "# Print the result\n",
    "print(f\"Number of rows with NaN values: {nan_rows}\")\n",
    "\n",
    "# 找出包含 NaN 的列\n",
    "rows_with_nan = test_data_imputed[test_data_imputed.isnull().any(axis=1)]\n",
    "\n",
    "test_data_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_label_mapping = {v: int(k) - 1 for k, v in label_mapping.items()}  # 讓數字標籤 -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final reverse_label_mapping in DNN: {'1-1': 10, '1-2': 9, '1-3': 8, '1-4': 7, '1-5': 6, '1-6': 5, '1-7': 4, '1-8': 3, '1-9': 2, '1-10': 1, '1-11': 0, '2-1': 11, '2-11': 29, '3-1': 12, '3-11': 28, '4-1': 13, '4-11': 27, '5-1': 14, '5-11': 26, '6-1': 15, '6-2': 16, '6-3': 17, '6-4': 18, '6-5': 19, '6-6': 20, '6-7': 21, '6-8': 22, '6-9': 23, '6-10': 24, '6-11': 25, '7-1': 48, '7-11': 30, '8-1': 47, '8-11': 31, '9-1': 46, '9-11': 32, '10-1': 45, '10-11': 33, '11-1': 44, '11-2': 43, '11-3': 42, '11-4': 41, '11-5': 40, '11-6': 39, '11-7': 38, '11-8': 37, '11-9': 36, '11-10': 35, '11-11': 34}\n",
      "y_numeric unique values in DNN: [10  1  0  9  8  7  6  5  4  3  2 45 33 44 35 34 43 42 41 40 39 38 37 36\n",
      " 11 29 12 28 13 27 14 26 15 24 25 16 17 18 19 20 21 22 23 48 30 47 31 46\n",
      " 32]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        10\n",
       "1        10\n",
       "2        10\n",
       "3        10\n",
       "4        10\n",
       "         ..\n",
       "19693    32\n",
       "19694    32\n",
       "19695    32\n",
       "19696    32\n",
       "19697    32\n",
       "Name: Label, Length: 19698, dtype: int64"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 建立 Label 映射\n",
    "y_test = test_data_imputed[target_column]\n",
    "y_test_numeric = y_test.map(reverse_label_mapping)\n",
    "\n",
    "print(\"Final reverse_label_mapping in DNN:\", reverse_label_mapping)\n",
    "print(\"y_numeric unique values in DNN:\", y_test_numeric.unique())\n",
    "\n",
    "y_test_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# 把label部分拿掉\n",
    "X_test = test_data_imputed.drop(columns=['level_1','Label'])\n",
    "\n",
    "# 確保測試資料的特徵與訓練資料的特徵一致\n",
    "X_test = X_test[X_testing_selected_columns]  # 選取相同的特徵\n",
    "\n",
    "print(type(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用之前訓練時的標準化器 (scaler) 來標準化測試數據\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 重新訓練model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "402\n"
     ]
    }
   ],
   "source": [
    "# 會發現如果用 train_test_split的方法會有資料分布不平均問題，解決辦法如下\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# 設定每個類別的固定樣本數\n",
    "N_train = 20 # 訓練集每個類別至少要有 N_train 筆資料\n",
    "test_val_ratio = 0.5  # 剩餘資料中，50% 作為驗證集，50% 作為測試集\n",
    "\n",
    "# 轉為 DataFrame 方便操作\n",
    "data = pd.DataFrame(X_test_scaled)\n",
    "data['label'] = y_test_numeric  # 加入 label 欄位\n",
    "\n",
    "print((data['label'] == 10).sum())  # 直接計算 True 的數量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.533468</td>\n",
       "      <td>0.727178</td>\n",
       "      <td>2.342175</td>\n",
       "      <td>-1.876067</td>\n",
       "      <td>0.086528</td>\n",
       "      <td>0.183418</td>\n",
       "      <td>-0.767232</td>\n",
       "      <td>2.021751</td>\n",
       "      <td>-0.973450</td>\n",
       "      <td>-0.144781</td>\n",
       "      <td>-1.105064</td>\n",
       "      <td>1.190134</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.500544</td>\n",
       "      <td>0.687173</td>\n",
       "      <td>2.431013</td>\n",
       "      <td>-1.250919</td>\n",
       "      <td>0.232493</td>\n",
       "      <td>0.188003</td>\n",
       "      <td>-1.072440</td>\n",
       "      <td>-0.747345</td>\n",
       "      <td>-0.973450</td>\n",
       "      <td>-0.144781</td>\n",
       "      <td>-1.484683</td>\n",
       "      <td>0.570201</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.451158</td>\n",
       "      <td>0.673838</td>\n",
       "      <td>2.653488</td>\n",
       "      <td>-1.234804</td>\n",
       "      <td>0.227681</td>\n",
       "      <td>0.181890</td>\n",
       "      <td>0.025515</td>\n",
       "      <td>-0.565168</td>\n",
       "      <td>-0.973450</td>\n",
       "      <td>-0.144781</td>\n",
       "      <td>-1.105064</td>\n",
       "      <td>0.570201</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.500544</td>\n",
       "      <td>0.740513</td>\n",
       "      <td>2.831544</td>\n",
       "      <td>-1.250919</td>\n",
       "      <td>0.043220</td>\n",
       "      <td>-0.996340</td>\n",
       "      <td>-0.864344</td>\n",
       "      <td>-0.676296</td>\n",
       "      <td>-0.973450</td>\n",
       "      <td>-0.144781</td>\n",
       "      <td>-1.105064</td>\n",
       "      <td>0.570201</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.517006</td>\n",
       "      <td>0.630414</td>\n",
       "      <td>2.401401</td>\n",
       "      <td>-1.299675</td>\n",
       "      <td>0.076904</td>\n",
       "      <td>-0.693759</td>\n",
       "      <td>-0.713722</td>\n",
       "      <td>-0.466792</td>\n",
       "      <td>-0.973450</td>\n",
       "      <td>-0.144781</td>\n",
       "      <td>-1.484683</td>\n",
       "      <td>0.570201</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19693</th>\n",
       "      <td>-0.390092</td>\n",
       "      <td>0.079578</td>\n",
       "      <td>-1.587958</td>\n",
       "      <td>0.895163</td>\n",
       "      <td>-1.116482</td>\n",
       "      <td>0.308729</td>\n",
       "      <td>-0.578955</td>\n",
       "      <td>-0.721841</td>\n",
       "      <td>1.158218</td>\n",
       "      <td>0.403943</td>\n",
       "      <td>1.552270</td>\n",
       "      <td>-1.444580</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19694</th>\n",
       "      <td>-0.340706</td>\n",
       "      <td>-0.040779</td>\n",
       "      <td>-1.706788</td>\n",
       "      <td>0.907146</td>\n",
       "      <td>-1.235179</td>\n",
       "      <td>0.612838</td>\n",
       "      <td>0.015606</td>\n",
       "      <td>-0.576099</td>\n",
       "      <td>1.158218</td>\n",
       "      <td>0.221035</td>\n",
       "      <td>1.552270</td>\n",
       "      <td>-1.444580</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19695</th>\n",
       "      <td>-0.340284</td>\n",
       "      <td>-0.070868</td>\n",
       "      <td>-1.701852</td>\n",
       "      <td>0.834425</td>\n",
       "      <td>-1.180643</td>\n",
       "      <td>0.625063</td>\n",
       "      <td>-0.059705</td>\n",
       "      <td>-0.732771</td>\n",
       "      <td>1.158218</td>\n",
       "      <td>0.038127</td>\n",
       "      <td>1.931890</td>\n",
       "      <td>-1.289597</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19696</th>\n",
       "      <td>-0.423016</td>\n",
       "      <td>0.069320</td>\n",
       "      <td>-1.587958</td>\n",
       "      <td>0.959620</td>\n",
       "      <td>-1.219139</td>\n",
       "      <td>0.158967</td>\n",
       "      <td>-0.360949</td>\n",
       "      <td>-0.747345</td>\n",
       "      <td>1.158218</td>\n",
       "      <td>0.221035</td>\n",
       "      <td>1.172651</td>\n",
       "      <td>-1.444580</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19697</th>\n",
       "      <td>-0.390092</td>\n",
       "      <td>-0.000774</td>\n",
       "      <td>-1.602764</td>\n",
       "      <td>1.282317</td>\n",
       "      <td>-1.145354</td>\n",
       "      <td>0.466132</td>\n",
       "      <td>-0.255910</td>\n",
       "      <td>2.559174</td>\n",
       "      <td>1.158218</td>\n",
       "      <td>0.221035</td>\n",
       "      <td>1.362461</td>\n",
       "      <td>-1.289597</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19698 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "0      0.533468  0.727178  2.342175 -1.876067  0.086528  0.183418 -0.767232   \n",
       "1      0.500544  0.687173  2.431013 -1.250919  0.232493  0.188003 -1.072440   \n",
       "2      0.451158  0.673838  2.653488 -1.234804  0.227681  0.181890  0.025515   \n",
       "3      0.500544  0.740513  2.831544 -1.250919  0.043220 -0.996340 -0.864344   \n",
       "4      0.517006  0.630414  2.401401 -1.299675  0.076904 -0.693759 -0.713722   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "19693 -0.390092  0.079578 -1.587958  0.895163 -1.116482  0.308729 -0.578955   \n",
       "19694 -0.340706 -0.040779 -1.706788  0.907146 -1.235179  0.612838  0.015606   \n",
       "19695 -0.340284 -0.070868 -1.701852  0.834425 -1.180643  0.625063 -0.059705   \n",
       "19696 -0.423016  0.069320 -1.587958  0.959620 -1.219139  0.158967 -0.360949   \n",
       "19697 -0.390092 -0.000774 -1.602764  1.282317 -1.145354  0.466132 -0.255910   \n",
       "\n",
       "              7         8         9        10        11  label  \n",
       "0      2.021751 -0.973450 -0.144781 -1.105064  1.190134     10  \n",
       "1     -0.747345 -0.973450 -0.144781 -1.484683  0.570201     10  \n",
       "2     -0.565168 -0.973450 -0.144781 -1.105064  0.570201     10  \n",
       "3     -0.676296 -0.973450 -0.144781 -1.105064  0.570201     10  \n",
       "4     -0.466792 -0.973450 -0.144781 -1.484683  0.570201     10  \n",
       "...         ...       ...       ...       ...       ...    ...  \n",
       "19693 -0.721841  1.158218  0.403943  1.552270 -1.444580     32  \n",
       "19694 -0.576099  1.158218  0.221035  1.552270 -1.444580     32  \n",
       "19695 -0.732771  1.158218  0.038127  1.931890 -1.289597     32  \n",
       "19696 -0.747345  1.158218  0.221035  1.172651 -1.444580     32  \n",
       "19697  2.559174  1.158218  0.221035  1.362461 -1.289597     32  \n",
       "\n",
       "[19698 rows x 13 columns]"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# 儲存訓練集\n",
    "train_data = data.groupby('label', group_keys=False).sample(n=N_train, replace=False, random_state=42)\n",
    "# print(train_data.index)  # 查看 train_data 的索引\n",
    "\n",
    "print((train_data['label'] == 10).sum())  # 計算 label 為 10 的數量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "392\n"
     ]
    }
   ],
   "source": [
    "remaining_data = data.drop(train_data.index)\n",
    "print(remaining_data['label'].value_counts().get(10, 0))  # 應該是 348\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "392\n"
     ]
    }
   ],
   "source": [
    "# 取得剩餘資料的 features 和 labels\n",
    "X_remaining = remaining_data.drop(columns=['label'])\n",
    "y_remaining = remaining_data['label']\n",
    "\n",
    "print(y_remaining.value_counts().get(10, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 490 samples, 49 unique labels\n",
      "Validation set: 9604 samples, 49 unique labels\n",
      "Test set: 9604 samples, 49 unique labels\n"
     ]
    }
   ],
   "source": [
    "# 使用 StratifiedShuffleSplit 來確保驗證集與測試集的類別比例一致\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=test_val_ratio, random_state=42)\n",
    "for val_index, test_index in sss.split(X_remaining, y_remaining):\n",
    "    X_val, X_test = X_remaining.iloc[val_index], X_remaining.iloc[test_index]\n",
    "    y_val, y_test = y_remaining.iloc[val_index], y_remaining.iloc[test_index]\n",
    "\n",
    "# 轉換回 NumPy 陣列\n",
    "X_train_small, y_train_small = train_data.drop(columns=['label']).values, train_data['label'].values\n",
    "X_val, y_val = X_val.values, y_val.values\n",
    "X_test, y_test = X_test.values, y_test.values\n",
    "\n",
    "# 確認切分後的數據數量\n",
    "print(f\"Training set: {len(X_train_small)} samples, {len(np.unique(y_train_small))} unique labels\")\n",
    "print(f\"Validation set: {len(X_val)} samples, {len(np.unique(y_val))} unique labels\")\n",
    "print(f\"Test set: {len(X_test)} samples, {len(np.unique(y_test))} unique labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 取出 10% 資料作為訓練資料，剩下的 90% 作為測試資料\n",
    "# X_train_small, X_unused, y_train_small, y_unused = train_test_split(\n",
    "#     X_test_scaled, y_test_numeric, test_size=0.99, random_state=42, stratify=y_test_numeric\n",
    "# )\n",
    "# # 再切出一半作為validation set and test set\n",
    "# X_val, X_test, y_val, y_test = train_test_split(\n",
    "#     X_unused, y_unused, test_size=0.5, random_state=42, stratify=y_unused\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 490 samples\n",
      "Validation set: 9604 samples\n",
      "Test set: 9604 samples\n"
     ]
    }
   ],
   "source": [
    "# 檢查各組資料比例\n",
    "print(f\"Training set: {len(X_train_small)} samples\")\n",
    "print(f\"Validation set: {len(X_val)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Set</th>\n",
       "      <th>Validation Set</th>\n",
       "      <th>Test Set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>196</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>196</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>196</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>196</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>196</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>196</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>196</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>196</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>196</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>196</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>196</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>196</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10</td>\n",
       "      <td>196</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10</td>\n",
       "      <td>196</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10</td>\n",
       "      <td>196</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10</td>\n",
       "      <td>196</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10</td>\n",
       "      <td>196</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10</td>\n",
       "      <td>196</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10</td>\n",
       "      <td>196</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10</td>\n",
       "      <td>196</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>10</td>\n",
       "      <td>196</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>10</td>\n",
       "      <td>196</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>10</td>\n",
       "      <td>196</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>10</td>\n",
       "      <td>196</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10</td>\n",
       "      <td>196</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10</td>\n",
       "      <td>196</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>10</td>\n",
       "      <td>196</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>10</td>\n",
       "      <td>196</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>10</td>\n",
       "      <td>196</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>10</td>\n",
       "      <td>196</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>10</td>\n",
       "      <td>196</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>10</td>\n",
       "      <td>196</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>10</td>\n",
       "      <td>196</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>10</td>\n",
       "      <td>196</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>10</td>\n",
       "      <td>196</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>10</td>\n",
       "      <td>196</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>10</td>\n",
       "      <td>196</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>10</td>\n",
       "      <td>196</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>10</td>\n",
       "      <td>196</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>10</td>\n",
       "      <td>196</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>10</td>\n",
       "      <td>196</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>10</td>\n",
       "      <td>196</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>10</td>\n",
       "      <td>196</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>10</td>\n",
       "      <td>196</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>10</td>\n",
       "      <td>196</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>10</td>\n",
       "      <td>196</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>10</td>\n",
       "      <td>196</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>10</td>\n",
       "      <td>196</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>10</td>\n",
       "      <td>196</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Training Set  Validation Set  Test Set\n",
       "0             10             196       196\n",
       "1             10             196       196\n",
       "2             10             196       196\n",
       "3             10             196       196\n",
       "4             10             196       196\n",
       "5             10             196       196\n",
       "6             10             196       196\n",
       "7             10             196       196\n",
       "8             10             196       196\n",
       "9             10             196       196\n",
       "10            10             196       196\n",
       "11            10             196       196\n",
       "12            10             196       196\n",
       "13            10             196       196\n",
       "14            10             196       196\n",
       "15            10             196       196\n",
       "16            10             196       196\n",
       "17            10             196       196\n",
       "18            10             196       196\n",
       "19            10             196       196\n",
       "20            10             196       196\n",
       "21            10             196       196\n",
       "22            10             196       196\n",
       "23            10             196       196\n",
       "24            10             196       196\n",
       "25            10             196       196\n",
       "26            10             196       196\n",
       "27            10             196       196\n",
       "28            10             196       196\n",
       "29            10             196       196\n",
       "30            10             196       196\n",
       "31            10             196       196\n",
       "32            10             196       196\n",
       "33            10             196       196\n",
       "34            10             196       196\n",
       "35            10             196       196\n",
       "36            10             196       196\n",
       "37            10             196       196\n",
       "38            10             196       196\n",
       "39            10             196       196\n",
       "40            10             196       196\n",
       "41            10             196       196\n",
       "42            10             196       196\n",
       "43            10             196       196\n",
       "44            10             196       196\n",
       "45            10             196       196\n",
       "46            10             196       196\n",
       "47            10             196       196\n",
       "48            10             196       196"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 計算每個 Set 內各 Label 的資料數量\n",
    "train_label_counts = pd.Series(y_train_small).value_counts().sort_index()\n",
    "val_label_counts = pd.Series(y_val).value_counts().sort_index()\n",
    "test_label_counts = pd.Series(y_test).value_counts().sort_index()\n",
    "\n",
    "# # 印出統計結果\n",
    "# print(\"Training Set - Label Counts:\")\n",
    "# print(train_label_counts)\n",
    "\n",
    "# print(\"\\nValidation Set - Label Counts:\")\n",
    "# print(val_label_counts)\n",
    "\n",
    "# print(\"\\nTest Set - Label Counts:\")\n",
    "# print(test_label_counts)\n",
    "\n",
    "# 確保所有 Labels 都有出現在三個 Set 裡\n",
    "all_labels = sorted(set(train_label_counts.index) | set(val_label_counts.index) | set(test_label_counts.index))\n",
    "label_distribution = pd.DataFrame(index=all_labels)\n",
    "\n",
    "label_distribution[\"Training Set\"] = train_label_counts\n",
    "label_distribution[\"Validation Set\"] = val_label_counts\n",
    "label_distribution[\"Test Set\"] = test_label_counts\n",
    "\n",
    "# 用 0 填補缺失值（表示該 Label 在該 Set 中沒有數據）\n",
    "label_distribution = label_distribution.fillna(0).astype(int)\n",
    "\n",
    "from IPython.display import display\n",
    "display(label_distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# 記錄開始時間\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 5.2216 - accuracy: 0.4388 - val_loss: 4.2195 - val_accuracy: 0.4767\n",
      "Epoch 2/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 4.3590 - accuracy: 0.4959 - val_loss: 3.5654 - val_accuracy: 0.5249\n",
      "Epoch 3/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3.7908 - accuracy: 0.5429 - val_loss: 3.1375 - val_accuracy: 0.5646\n",
      "Epoch 4/10000\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 3.3888 - accuracy: 0.5776 - val_loss: 2.8073 - val_accuracy: 0.5928\n",
      "Epoch 5/10000\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 3.0808 - accuracy: 0.6122 - val_loss: 2.5611 - val_accuracy: 0.6166\n",
      "Epoch 6/10000\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 2.8194 - accuracy: 0.6224 - val_loss: 2.3583 - val_accuracy: 0.6344\n",
      "Epoch 7/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 2.5952 - accuracy: 0.6347 - val_loss: 2.1691 - val_accuracy: 0.6495\n",
      "Epoch 8/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 2.3982 - accuracy: 0.6531 - val_loss: 2.0158 - val_accuracy: 0.6626\n",
      "Epoch 9/10000\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2.2199 - accuracy: 0.6694 - val_loss: 1.8742 - val_accuracy: 0.6801\n",
      "Epoch 10/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 2.0522 - accuracy: 0.6918 - val_loss: 1.7566 - val_accuracy: 0.6909\n",
      "Epoch 11/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 1.9099 - accuracy: 0.7102 - val_loss: 1.6449 - val_accuracy: 0.7046\n",
      "Epoch 12/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1.7772 - accuracy: 0.7143 - val_loss: 1.5343 - val_accuracy: 0.7185\n",
      "Epoch 13/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1.6509 - accuracy: 0.7388 - val_loss: 1.4215 - val_accuracy: 0.7385\n",
      "Epoch 14/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1.5546 - accuracy: 0.7469 - val_loss: 1.3411 - val_accuracy: 0.7504\n",
      "Epoch 15/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 1.4428 - accuracy: 0.7633 - val_loss: 1.2797 - val_accuracy: 0.7672\n",
      "Epoch 16/10000\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.3619 - accuracy: 0.7837 - val_loss: 1.2234 - val_accuracy: 0.7797\n",
      "Epoch 17/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 1.2921 - accuracy: 0.8061 - val_loss: 1.1711 - val_accuracy: 0.7907\n",
      "Epoch 18/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 1.2170 - accuracy: 0.8204 - val_loss: 1.1250 - val_accuracy: 0.8021\n",
      "Epoch 19/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 1.1526 - accuracy: 0.8327 - val_loss: 1.0843 - val_accuracy: 0.8086\n",
      "Epoch 20/10000\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0949 - accuracy: 0.8408 - val_loss: 1.0514 - val_accuracy: 0.8142\n",
      "Epoch 21/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 1.0439 - accuracy: 0.8510 - val_loss: 1.0142 - val_accuracy: 0.8224\n",
      "Epoch 22/10000\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.9989 - accuracy: 0.8592 - val_loss: 0.9834 - val_accuracy: 0.8285\n",
      "Epoch 23/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.9487 - accuracy: 0.8653 - val_loss: 0.9520 - val_accuracy: 0.8350\n",
      "Epoch 24/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.9133 - accuracy: 0.8816 - val_loss: 0.9276 - val_accuracy: 0.8387\n",
      "Epoch 25/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.8727 - accuracy: 0.8776 - val_loss: 0.9048 - val_accuracy: 0.8443\n",
      "Epoch 26/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.8383 - accuracy: 0.8837 - val_loss: 0.8831 - val_accuracy: 0.8477\n",
      "Epoch 27/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.8050 - accuracy: 0.8837 - val_loss: 0.8652 - val_accuracy: 0.8516\n",
      "Epoch 28/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.7736 - accuracy: 0.8898 - val_loss: 0.8422 - val_accuracy: 0.8585\n",
      "Epoch 29/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.7371 - accuracy: 0.8959 - val_loss: 0.8244 - val_accuracy: 0.8605\n",
      "Epoch 30/10000\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.7115 - accuracy: 0.8980 - val_loss: 0.8042 - val_accuracy: 0.8638\n",
      "Epoch 31/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.6863 - accuracy: 0.9041 - val_loss: 0.7890 - val_accuracy: 0.8660\n",
      "Epoch 32/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6616 - accuracy: 0.9041 - val_loss: 0.7719 - val_accuracy: 0.8695\n",
      "Epoch 33/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6365 - accuracy: 0.9041 - val_loss: 0.7512 - val_accuracy: 0.8728\n",
      "Epoch 34/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6093 - accuracy: 0.9082 - val_loss: 0.7387 - val_accuracy: 0.8739\n",
      "Epoch 35/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.5845 - accuracy: 0.9122 - val_loss: 0.7245 - val_accuracy: 0.8771\n",
      "Epoch 36/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.5655 - accuracy: 0.9163 - val_loss: 0.7077 - val_accuracy: 0.8802\n",
      "Epoch 37/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.5431 - accuracy: 0.9184 - val_loss: 0.6960 - val_accuracy: 0.8818\n",
      "Epoch 38/10000\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5260 - accuracy: 0.9204 - val_loss: 0.6885 - val_accuracy: 0.8809\n",
      "Epoch 39/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.5100 - accuracy: 0.9224 - val_loss: 0.6716 - val_accuracy: 0.8844\n",
      "Epoch 40/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.4888 - accuracy: 0.9265 - val_loss: 0.6602 - val_accuracy: 0.8874\n",
      "Epoch 41/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4696 - accuracy: 0.9245 - val_loss: 0.6505 - val_accuracy: 0.8878\n",
      "Epoch 42/10000\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4541 - accuracy: 0.9286 - val_loss: 0.6385 - val_accuracy: 0.8918\n",
      "Epoch 43/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4379 - accuracy: 0.9327 - val_loss: 0.6298 - val_accuracy: 0.8926\n",
      "Epoch 44/10000\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4216 - accuracy: 0.9367 - val_loss: 0.6202 - val_accuracy: 0.8925\n",
      "Epoch 45/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.4059 - accuracy: 0.9408 - val_loss: 0.6114 - val_accuracy: 0.8938\n",
      "Epoch 46/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.3914 - accuracy: 0.9429 - val_loss: 0.6028 - val_accuracy: 0.8955\n",
      "Epoch 47/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.3766 - accuracy: 0.9469 - val_loss: 0.5954 - val_accuracy: 0.8971\n",
      "Epoch 48/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3648 - accuracy: 0.9469 - val_loss: 0.5830 - val_accuracy: 0.9011\n",
      "Epoch 49/10000\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3528 - accuracy: 0.9429 - val_loss: 0.5792 - val_accuracy: 0.8996\n",
      "Epoch 50/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.3386 - accuracy: 0.9408 - val_loss: 0.5747 - val_accuracy: 0.9002\n",
      "Epoch 51/10000\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.3268 - accuracy: 0.9531 - val_loss: 0.5607 - val_accuracy: 0.9029\n",
      "Epoch 52/10000\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3128 - accuracy: 0.9551 - val_loss: 0.5559 - val_accuracy: 0.9024\n",
      "Epoch 53/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.3024 - accuracy: 0.9571 - val_loss: 0.5514 - val_accuracy: 0.9040\n",
      "Epoch 54/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2889 - accuracy: 0.9592 - val_loss: 0.5455 - val_accuracy: 0.9070\n",
      "Epoch 55/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2807 - accuracy: 0.9571 - val_loss: 0.5370 - val_accuracy: 0.9083\n",
      "Epoch 56/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2699 - accuracy: 0.9571 - val_loss: 0.5346 - val_accuracy: 0.9086\n",
      "Epoch 57/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2588 - accuracy: 0.9592 - val_loss: 0.5293 - val_accuracy: 0.9100\n",
      "Epoch 58/10000\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.2496 - accuracy: 0.9592 - val_loss: 0.5279 - val_accuracy: 0.9092\n",
      "Epoch 59/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2399 - accuracy: 0.9633 - val_loss: 0.5248 - val_accuracy: 0.9084\n",
      "Epoch 60/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.2330 - accuracy: 0.9673 - val_loss: 0.5217 - val_accuracy: 0.9092\n",
      "Epoch 61/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.2234 - accuracy: 0.9653 - val_loss: 0.5168 - val_accuracy: 0.9102\n",
      "Epoch 62/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.2140 - accuracy: 0.9653 - val_loss: 0.5132 - val_accuracy: 0.9109\n",
      "Epoch 63/10000\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.2033 - accuracy: 0.9633 - val_loss: 0.5102 - val_accuracy: 0.9113\n",
      "Epoch 64/10000\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.1963 - accuracy: 0.9633 - val_loss: 0.5051 - val_accuracy: 0.9128\n",
      "Epoch 65/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.1890 - accuracy: 0.9633 - val_loss: 0.5056 - val_accuracy: 0.9123\n",
      "Epoch 66/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.1789 - accuracy: 0.9653 - val_loss: 0.4994 - val_accuracy: 0.9134\n",
      "Epoch 67/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.1705 - accuracy: 0.9653 - val_loss: 0.4958 - val_accuracy: 0.9138\n",
      "Epoch 68/10000\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.1634 - accuracy: 0.9673 - val_loss: 0.4911 - val_accuracy: 0.9159\n",
      "Epoch 69/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.1556 - accuracy: 0.9653 - val_loss: 0.4898 - val_accuracy: 0.9156\n",
      "Epoch 70/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.1472 - accuracy: 0.9653 - val_loss: 0.4875 - val_accuracy: 0.9172\n",
      "Epoch 71/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.1392 - accuracy: 0.9653 - val_loss: 0.4853 - val_accuracy: 0.9172\n",
      "Epoch 72/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.1334 - accuracy: 0.9653 - val_loss: 0.4783 - val_accuracy: 0.9185\n",
      "Epoch 73/10000\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.1273 - accuracy: 0.9673 - val_loss: 0.4790 - val_accuracy: 0.9186\n",
      "Epoch 74/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1186 - accuracy: 0.9673 - val_loss: 0.4770 - val_accuracy: 0.9195\n",
      "Epoch 75/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1123 - accuracy: 0.9714 - val_loss: 0.4743 - val_accuracy: 0.9195\n",
      "Epoch 76/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1054 - accuracy: 0.9694 - val_loss: 0.4718 - val_accuracy: 0.9207\n",
      "Epoch 77/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.0994 - accuracy: 0.9694 - val_loss: 0.4658 - val_accuracy: 0.9217\n",
      "Epoch 78/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0929 - accuracy: 0.9735 - val_loss: 0.4644 - val_accuracy: 0.9224\n",
      "Epoch 79/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0883 - accuracy: 0.9776 - val_loss: 0.4647 - val_accuracy: 0.9222\n",
      "Epoch 80/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0820 - accuracy: 0.9776 - val_loss: 0.4606 - val_accuracy: 0.9236\n",
      "Epoch 81/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.0782 - accuracy: 0.9796 - val_loss: 0.4568 - val_accuracy: 0.9244\n",
      "Epoch 82/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0738 - accuracy: 0.9816 - val_loss: 0.4542 - val_accuracy: 0.9259\n",
      "Epoch 83/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0694 - accuracy: 0.9796 - val_loss: 0.4550 - val_accuracy: 0.9245\n",
      "Epoch 84/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0648 - accuracy: 0.9816 - val_loss: 0.4512 - val_accuracy: 0.9254\n",
      "Epoch 85/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0567 - accuracy: 0.9837 - val_loss: 0.4502 - val_accuracy: 0.9261\n",
      "Epoch 86/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.0558 - accuracy: 0.9837 - val_loss: 0.4485 - val_accuracy: 0.9265\n",
      "Epoch 87/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0503 - accuracy: 0.9837 - val_loss: 0.4450 - val_accuracy: 0.9271\n",
      "Epoch 88/10000\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.0468 - accuracy: 0.9837 - val_loss: 0.4417 - val_accuracy: 0.9279\n",
      "Epoch 89/10000\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.0451 - accuracy: 0.9837 - val_loss: 0.4424 - val_accuracy: 0.9273\n",
      "Epoch 90/10000\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.0419 - accuracy: 0.9878 - val_loss: 0.4406 - val_accuracy: 0.9278\n",
      "Epoch 91/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0403 - accuracy: 0.9918 - val_loss: 0.4377 - val_accuracy: 0.9282\n",
      "Epoch 92/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0375 - accuracy: 0.9918 - val_loss: 0.4355 - val_accuracy: 0.9287\n",
      "Epoch 93/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0360 - accuracy: 0.9939 - val_loss: 0.4330 - val_accuracy: 0.9291\n",
      "Epoch 94/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0347 - accuracy: 0.9918 - val_loss: 0.4322 - val_accuracy: 0.9301\n",
      "Epoch 95/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0326 - accuracy: 0.9939 - val_loss: 0.4304 - val_accuracy: 0.9299\n",
      "Epoch 96/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0313 - accuracy: 0.9918 - val_loss: 0.4292 - val_accuracy: 0.9305\n",
      "Epoch 97/10000\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.0304 - accuracy: 0.9918 - val_loss: 0.4280 - val_accuracy: 0.9307\n",
      "Epoch 98/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0292 - accuracy: 0.9939 - val_loss: 0.4268 - val_accuracy: 0.9312\n",
      "Epoch 99/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0278 - accuracy: 0.9939 - val_loss: 0.4246 - val_accuracy: 0.9319\n",
      "Epoch 100/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0267 - accuracy: 0.9959 - val_loss: 0.4268 - val_accuracy: 0.9317\n",
      "Epoch 101/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0264 - accuracy: 0.9980 - val_loss: 0.4236 - val_accuracy: 0.9323\n",
      "Epoch 102/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0248 - accuracy: 0.9959 - val_loss: 0.4204 - val_accuracy: 0.9329\n",
      "Epoch 103/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0241 - accuracy: 0.9959 - val_loss: 0.4191 - val_accuracy: 0.9339\n",
      "Epoch 104/10000\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.0234 - accuracy: 0.9959 - val_loss: 0.4176 - val_accuracy: 0.9343\n",
      "Epoch 105/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0223 - accuracy: 0.9959 - val_loss: 0.4166 - val_accuracy: 0.9344\n",
      "Epoch 106/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0216 - accuracy: 0.9959 - val_loss: 0.4155 - val_accuracy: 0.9348\n",
      "Epoch 107/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0210 - accuracy: 0.9959 - val_loss: 0.4140 - val_accuracy: 0.9349\n",
      "Epoch 108/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0203 - accuracy: 0.9980 - val_loss: 0.4134 - val_accuracy: 0.9355\n",
      "Epoch 109/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0198 - accuracy: 0.9959 - val_loss: 0.4114 - val_accuracy: 0.9360\n",
      "Epoch 110/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0191 - accuracy: 0.9980 - val_loss: 0.4110 - val_accuracy: 0.9360\n",
      "Epoch 111/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0185 - accuracy: 0.9980 - val_loss: 0.4108 - val_accuracy: 0.9365\n",
      "Epoch 112/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.0179 - accuracy: 0.9980 - val_loss: 0.4100 - val_accuracy: 0.9366\n",
      "Epoch 113/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0176 - accuracy: 1.0000 - val_loss: 0.4086 - val_accuracy: 0.9368\n",
      "Epoch 114/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0171 - accuracy: 1.0000 - val_loss: 0.4076 - val_accuracy: 0.9372\n",
      "Epoch 115/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0168 - accuracy: 1.0000 - val_loss: 0.4073 - val_accuracy: 0.9377\n",
      "Epoch 116/10000\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0163 - accuracy: 1.0000 - val_loss: 0.4064 - val_accuracy: 0.9380\n",
      "Epoch 117/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0160 - accuracy: 1.0000 - val_loss: 0.4063 - val_accuracy: 0.9376\n",
      "Epoch 118/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0154 - accuracy: 1.0000 - val_loss: 0.4051 - val_accuracy: 0.9376\n",
      "Epoch 119/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0151 - accuracy: 1.0000 - val_loss: 0.4045 - val_accuracy: 0.9379\n",
      "Epoch 120/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0147 - accuracy: 1.0000 - val_loss: 0.4037 - val_accuracy: 0.9383\n",
      "Epoch 121/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0143 - accuracy: 1.0000 - val_loss: 0.4030 - val_accuracy: 0.9385\n",
      "Epoch 122/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.0139 - accuracy: 1.0000 - val_loss: 0.4025 - val_accuracy: 0.9388\n",
      "Epoch 123/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0136 - accuracy: 1.0000 - val_loss: 0.4022 - val_accuracy: 0.9391\n",
      "Epoch 124/10000\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.0133 - accuracy: 1.0000 - val_loss: 0.4014 - val_accuracy: 0.9393\n",
      "Epoch 125/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0132 - accuracy: 1.0000 - val_loss: 0.4002 - val_accuracy: 0.9391\n",
      "Epoch 126/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0128 - accuracy: 1.0000 - val_loss: 0.3991 - val_accuracy: 0.9399\n",
      "Epoch 127/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0124 - accuracy: 1.0000 - val_loss: 0.3989 - val_accuracy: 0.9396\n",
      "Epoch 128/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0122 - accuracy: 1.0000 - val_loss: 0.3973 - val_accuracy: 0.9400\n",
      "Epoch 129/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0120 - accuracy: 1.0000 - val_loss: 0.3965 - val_accuracy: 0.9401\n",
      "Epoch 130/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 0.3961 - val_accuracy: 0.9403\n",
      "Epoch 131/10000\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0115 - accuracy: 1.0000 - val_loss: 0.3966 - val_accuracy: 0.9402\n",
      "Epoch 132/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.0112 - accuracy: 1.0000 - val_loss: 0.3957 - val_accuracy: 0.9405\n",
      "Epoch 133/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0111 - accuracy: 1.0000 - val_loss: 0.3947 - val_accuracy: 0.9404\n",
      "Epoch 134/10000\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.3939 - val_accuracy: 0.9408\n",
      "Epoch 135/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0106 - accuracy: 1.0000 - val_loss: 0.3937 - val_accuracy: 0.9412\n",
      "Epoch 136/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.0104 - accuracy: 1.0000 - val_loss: 0.3939 - val_accuracy: 0.9410\n",
      "Epoch 137/10000\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.3938 - val_accuracy: 0.9413\n",
      "Epoch 138/10000\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0100 - accuracy: 1.0000 - val_loss: 0.3927 - val_accuracy: 0.9414\n",
      "Epoch 139/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 0.3917 - val_accuracy: 0.9415\n",
      "Epoch 140/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.3914 - val_accuracy: 0.9418\n",
      "Epoch 141/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.3908 - val_accuracy: 0.9421\n",
      "Epoch 142/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.3903 - val_accuracy: 0.9421\n",
      "Epoch 143/10000\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.3899 - val_accuracy: 0.9421\n",
      "Epoch 144/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.3893 - val_accuracy: 0.9422\n",
      "Epoch 145/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.3890 - val_accuracy: 0.9422\n",
      "Epoch 146/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.3881 - val_accuracy: 0.9423\n",
      "Epoch 147/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.3870 - val_accuracy: 0.9425\n",
      "Epoch 148/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.3875 - val_accuracy: 0.9426\n",
      "Epoch 149/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.3870 - val_accuracy: 0.9428\n",
      "Epoch 150/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.3865 - val_accuracy: 0.9428\n",
      "Epoch 151/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.3860 - val_accuracy: 0.9430\n",
      "Epoch 152/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.3852 - val_accuracy: 0.9433\n",
      "Epoch 153/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.3851 - val_accuracy: 0.9430\n",
      "Epoch 154/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.3846 - val_accuracy: 0.9431\n",
      "Epoch 155/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.3840 - val_accuracy: 0.9435\n",
      "Epoch 156/10000\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.3850 - val_accuracy: 0.9431\n",
      "Epoch 157/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.3840 - val_accuracy: 0.9437\n",
      "Epoch 158/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.3830 - val_accuracy: 0.9437\n",
      "Epoch 159/10000\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.3823 - val_accuracy: 0.9440\n",
      "Epoch 160/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.3818 - val_accuracy: 0.9441\n",
      "Epoch 161/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.3818 - val_accuracy: 0.9442\n",
      "Epoch 162/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.3818 - val_accuracy: 0.9442\n",
      "Epoch 163/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.3808 - val_accuracy: 0.9443\n",
      "Epoch 164/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.3804 - val_accuracy: 0.9445\n",
      "Epoch 165/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.3801 - val_accuracy: 0.9447\n",
      "Epoch 166/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.3798 - val_accuracy: 0.9448\n",
      "Epoch 167/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.3794 - val_accuracy: 0.9452\n",
      "Epoch 168/10000\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.3791 - val_accuracy: 0.9452\n",
      "Epoch 169/10000\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.3787 - val_accuracy: 0.9456\n",
      "Epoch 170/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.3779 - val_accuracy: 0.9461\n",
      "Epoch 171/10000\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.3776 - val_accuracy: 0.9462\n",
      "Epoch 172/10000\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.3770 - val_accuracy: 0.9459\n",
      "Epoch 173/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.3764 - val_accuracy: 0.9461\n",
      "Epoch 174/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.3762 - val_accuracy: 0.9461\n",
      "Epoch 175/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.3759 - val_accuracy: 0.9461\n",
      "Epoch 176/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.3756 - val_accuracy: 0.9463\n",
      "Epoch 177/10000\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.3755 - val_accuracy: 0.9464\n",
      "Epoch 178/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.3751 - val_accuracy: 0.9465\n",
      "Epoch 179/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.3745 - val_accuracy: 0.9466\n",
      "Epoch 180/10000\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.3747 - val_accuracy: 0.9468\n",
      "Epoch 181/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.3742 - val_accuracy: 0.9469\n",
      "Epoch 182/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.3740 - val_accuracy: 0.9469\n",
      "Epoch 183/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.3734 - val_accuracy: 0.9468\n",
      "Epoch 184/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.3743 - val_accuracy: 0.9466\n",
      "Epoch 185/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.3743 - val_accuracy: 0.9465\n",
      "Epoch 186/10000\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.3737 - val_accuracy: 0.9468\n",
      "Epoch 187/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.3732 - val_accuracy: 0.9469\n",
      "Epoch 188/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.3727 - val_accuracy: 0.9469\n",
      "Epoch 189/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.3725 - val_accuracy: 0.9472\n",
      "Epoch 190/10000\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.3721 - val_accuracy: 0.9472\n",
      "Epoch 191/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.3716 - val_accuracy: 0.9473\n",
      "Epoch 192/10000\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.3715 - val_accuracy: 0.9474\n",
      "Epoch 193/10000\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.3713 - val_accuracy: 0.9474\n",
      "Epoch 194/10000\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.3707 - val_accuracy: 0.9474\n",
      "Epoch 195/10000\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.3707 - val_accuracy: 0.9476\n",
      "Epoch 196/10000\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.3703 - val_accuracy: 0.9476\n",
      "Epoch 197/10000\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.3703 - val_accuracy: 0.9476\n",
      "Epoch 198/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.3697 - val_accuracy: 0.9477\n",
      "Epoch 199/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.3693 - val_accuracy: 0.9476\n",
      "Epoch 200/10000\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.3692 - val_accuracy: 0.9478\n",
      "Epoch 201/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.3693 - val_accuracy: 0.9476\n",
      "Epoch 202/10000\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.3688 - val_accuracy: 0.9477\n",
      "Epoch 203/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.3686 - val_accuracy: 0.9477\n",
      "Epoch 204/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.3683 - val_accuracy: 0.9478\n",
      "Epoch 205/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.3679 - val_accuracy: 0.9480\n",
      "Epoch 206/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.3678 - val_accuracy: 0.9479\n",
      "Epoch 207/10000\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.3672 - val_accuracy: 0.9481\n",
      "Epoch 208/10000\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.3668 - val_accuracy: 0.9481\n",
      "Epoch 209/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.3665 - val_accuracy: 0.9481\n",
      "Epoch 210/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.3668 - val_accuracy: 0.9479\n",
      "Epoch 211/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.3665 - val_accuracy: 0.9481\n",
      "Epoch 212/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.3660 - val_accuracy: 0.9480\n",
      "Epoch 213/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.3657 - val_accuracy: 0.9480\n",
      "Epoch 214/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.3652 - val_accuracy: 0.9483\n",
      "Epoch 215/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.3654 - val_accuracy: 0.9483\n",
      "Epoch 216/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.3654 - val_accuracy: 0.9481\n",
      "Epoch 217/10000\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.3650 - val_accuracy: 0.9484\n",
      "Epoch 218/10000\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.3649 - val_accuracy: 0.9484\n",
      "Epoch 219/10000\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.3644 - val_accuracy: 0.9486\n",
      "Epoch 220/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.3645 - val_accuracy: 0.9483\n",
      "Epoch 221/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.3640 - val_accuracy: 0.9484\n",
      "Epoch 222/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.3639 - val_accuracy: 0.9484\n",
      "Epoch 223/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.3637 - val_accuracy: 0.9486\n",
      "Epoch 224/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.3638 - val_accuracy: 0.9484\n",
      "Epoch 225/10000\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.3636 - val_accuracy: 0.9486\n",
      "Epoch 226/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.3631 - val_accuracy: 0.9487\n",
      "Epoch 227/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.3628 - val_accuracy: 0.9487\n",
      "Epoch 228/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.3629 - val_accuracy: 0.9487\n",
      "Epoch 229/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.3623 - val_accuracy: 0.9492\n",
      "Epoch 230/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.3625 - val_accuracy: 0.9488\n",
      "Epoch 231/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.3619 - val_accuracy: 0.9489\n",
      "Epoch 232/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.3615 - val_accuracy: 0.9489\n",
      "Epoch 233/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.3618 - val_accuracy: 0.9490\n",
      "Epoch 234/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.3615 - val_accuracy: 0.9494\n",
      "Epoch 235/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.3615 - val_accuracy: 0.9494\n",
      "Epoch 236/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.3613 - val_accuracy: 0.9494\n",
      "Epoch 237/10000\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.3610 - val_accuracy: 0.9495\n",
      "Epoch 238/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.3613 - val_accuracy: 0.9491\n",
      "Epoch 239/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.3611 - val_accuracy: 0.9493\n",
      "Epoch 240/10000\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.3606 - val_accuracy: 0.9492\n",
      "Epoch 241/10000\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.3605 - val_accuracy: 0.9493\n",
      "Epoch 242/10000\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.3603 - val_accuracy: 0.9493\n",
      "Epoch 243/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.3600 - val_accuracy: 0.9495\n",
      "Epoch 244/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.3596 - val_accuracy: 0.9496\n",
      "Epoch 245/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.3593 - val_accuracy: 0.9495\n",
      "Epoch 246/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.3590 - val_accuracy: 0.9498\n",
      "Epoch 247/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.3589 - val_accuracy: 0.9499\n",
      "Epoch 248/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.3586 - val_accuracy: 0.9500\n",
      "Epoch 249/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.3594 - val_accuracy: 0.9497\n",
      "Epoch 250/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.3594 - val_accuracy: 0.9501\n",
      "Epoch 251/10000\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.3586 - val_accuracy: 0.9500\n",
      "Epoch 252/10000\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.3590 - val_accuracy: 0.9500\n",
      "Epoch 253/10000\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.3588 - val_accuracy: 0.9499\n",
      "Epoch 254/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.3584 - val_accuracy: 0.9498\n",
      "Epoch 255/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.3580 - val_accuracy: 0.9499\n",
      "Epoch 256/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.3578 - val_accuracy: 0.9502\n",
      "Epoch 257/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.3574 - val_accuracy: 0.9503\n",
      "Epoch 258/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.3574 - val_accuracy: 0.9503\n",
      "Epoch 259/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.3572 - val_accuracy: 0.9504\n",
      "Epoch 260/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.3569 - val_accuracy: 0.9505\n",
      "Epoch 261/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.3568 - val_accuracy: 0.9507\n",
      "Epoch 262/10000\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.3564 - val_accuracy: 0.9505\n",
      "Epoch 263/10000\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.3564 - val_accuracy: 0.9505\n",
      "Epoch 264/10000\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.3562 - val_accuracy: 0.9507\n",
      "Epoch 265/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.3561 - val_accuracy: 0.9509\n",
      "Epoch 266/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.3559 - val_accuracy: 0.9510\n",
      "Epoch 267/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.3552 - val_accuracy: 0.9513\n",
      "Epoch 268/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.3555 - val_accuracy: 0.9512\n",
      "Epoch 269/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.3553 - val_accuracy: 0.9513\n",
      "Epoch 270/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.3552 - val_accuracy: 0.9511\n",
      "Epoch 271/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.3553 - val_accuracy: 0.9510\n",
      "Epoch 272/10000\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.3553 - val_accuracy: 0.9512\n",
      "Epoch 273/10000\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.3548 - val_accuracy: 0.9514\n",
      "Epoch 274/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.3549 - val_accuracy: 0.9516\n",
      "Epoch 275/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.3547 - val_accuracy: 0.9516\n",
      "Epoch 276/10000\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.3546 - val_accuracy: 0.9518\n",
      "Epoch 277/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.3544 - val_accuracy: 0.9519\n",
      "Epoch 278/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.3539 - val_accuracy: 0.9516\n",
      "Epoch 279/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.3538 - val_accuracy: 0.9516\n",
      "Epoch 280/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.3539 - val_accuracy: 0.9518\n",
      "Epoch 281/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.3536 - val_accuracy: 0.9517\n",
      "Epoch 282/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.3535 - val_accuracy: 0.9517\n",
      "Epoch 283/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.3534 - val_accuracy: 0.9518\n",
      "Epoch 284/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.3539 - val_accuracy: 0.9519\n",
      "Epoch 285/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.3536 - val_accuracy: 0.9519\n",
      "Epoch 286/10000\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.3532 - val_accuracy: 0.9519\n",
      "Epoch 287/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.3527 - val_accuracy: 0.9519\n",
      "Epoch 288/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.3528 - val_accuracy: 0.9518\n",
      "Epoch 289/10000\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.3525 - val_accuracy: 0.9516\n",
      "Epoch 290/10000\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.3523 - val_accuracy: 0.9516\n",
      "Epoch 291/10000\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.3522 - val_accuracy: 0.9518\n",
      "Epoch 292/10000\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.3520 - val_accuracy: 0.9518\n",
      "Epoch 293/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.3518 - val_accuracy: 0.9519\n",
      "Epoch 294/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.3514 - val_accuracy: 0.9518\n",
      "Epoch 295/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.3513 - val_accuracy: 0.9519\n",
      "Epoch 296/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.3512 - val_accuracy: 0.9521\n",
      "Epoch 297/10000\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.3512 - val_accuracy: 0.9522\n",
      "Epoch 298/10000\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.3512 - val_accuracy: 0.9522\n",
      "Epoch 299/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.3507 - val_accuracy: 0.9521\n",
      "Epoch 300/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.3507 - val_accuracy: 0.9521\n",
      "Epoch 301/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.3503 - val_accuracy: 0.9522\n",
      "Epoch 302/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.3505 - val_accuracy: 0.9524\n",
      "Epoch 303/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.3501 - val_accuracy: 0.9522\n",
      "Epoch 304/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.3498 - val_accuracy: 0.9524\n",
      "Epoch 305/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.3496 - val_accuracy: 0.9524\n",
      "Epoch 306/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.3498 - val_accuracy: 0.9522\n",
      "Epoch 307/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.3496 - val_accuracy: 0.9523\n",
      "Epoch 308/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.3497 - val_accuracy: 0.9523\n",
      "Epoch 309/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.3496 - val_accuracy: 0.9524\n",
      "Epoch 310/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.3494 - val_accuracy: 0.9524\n",
      "Epoch 311/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.3493 - val_accuracy: 0.9526\n",
      "Epoch 312/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.3488 - val_accuracy: 0.9526\n",
      "Epoch 313/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.3487 - val_accuracy: 0.9526\n",
      "Epoch 314/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.3486 - val_accuracy: 0.9527\n",
      "Epoch 315/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.3484 - val_accuracy: 0.9527\n",
      "Epoch 316/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.3484 - val_accuracy: 0.9527\n",
      "Epoch 317/10000\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.3483 - val_accuracy: 0.9527\n",
      "Epoch 318/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.3483 - val_accuracy: 0.9527\n",
      "Epoch 319/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.3479 - val_accuracy: 0.9526\n",
      "Epoch 320/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.3479 - val_accuracy: 0.9526\n",
      "Epoch 321/10000\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.3478 - val_accuracy: 0.9526\n",
      "Epoch 322/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.3477 - val_accuracy: 0.9527\n",
      "Epoch 323/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.3474 - val_accuracy: 0.9527\n",
      "Epoch 324/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.3475 - val_accuracy: 0.9528\n",
      "Epoch 325/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.3476 - val_accuracy: 0.9528\n",
      "Epoch 326/10000\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.3474 - val_accuracy: 0.9528\n",
      "Epoch 327/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.3472 - val_accuracy: 0.9528\n",
      "Epoch 328/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.3471 - val_accuracy: 0.9528\n",
      "Epoch 329/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.3470 - val_accuracy: 0.9531\n",
      "Epoch 330/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.3470 - val_accuracy: 0.9529\n",
      "Epoch 331/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.3468 - val_accuracy: 0.9529\n",
      "Epoch 332/10000\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.3470 - val_accuracy: 0.9530\n",
      "Epoch 333/10000\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.3467 - val_accuracy: 0.9529\n",
      "Epoch 334/10000\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.3468 - val_accuracy: 0.9529\n",
      "Epoch 335/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.3467 - val_accuracy: 0.9530\n",
      "Epoch 336/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.3464 - val_accuracy: 0.9532\n",
      "Epoch 337/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.3465 - val_accuracy: 0.9531\n",
      "Epoch 338/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.3461 - val_accuracy: 0.9535\n",
      "Epoch 339/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.3460 - val_accuracy: 0.9534\n",
      "Epoch 340/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.3460 - val_accuracy: 0.9532\n",
      "Epoch 341/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.3458 - val_accuracy: 0.9534\n",
      "Epoch 342/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.3455 - val_accuracy: 0.9535\n",
      "Epoch 343/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.3453 - val_accuracy: 0.9536\n",
      "Epoch 344/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.3453 - val_accuracy: 0.9537\n",
      "Epoch 345/10000\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.3452 - val_accuracy: 0.9539\n",
      "Epoch 346/10000\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.3450 - val_accuracy: 0.9538\n",
      "Epoch 347/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.3448 - val_accuracy: 0.9540\n",
      "Epoch 348/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.3449 - val_accuracy: 0.9538\n",
      "Epoch 349/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.3447 - val_accuracy: 0.9541\n",
      "Epoch 350/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.3445 - val_accuracy: 0.9542\n",
      "Epoch 351/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.3443 - val_accuracy: 0.9541\n",
      "Epoch 352/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.3449 - val_accuracy: 0.9541\n",
      "Epoch 353/10000\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.3448 - val_accuracy: 0.9539\n",
      "Epoch 354/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 9.9946e-04 - accuracy: 1.0000 - val_loss: 0.3447 - val_accuracy: 0.9539\n",
      "Epoch 355/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 9.8577e-04 - accuracy: 1.0000 - val_loss: 0.3443 - val_accuracy: 0.9539\n",
      "Epoch 356/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 9.7782e-04 - accuracy: 1.0000 - val_loss: 0.3441 - val_accuracy: 0.9540\n",
      "Epoch 357/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 9.6856e-04 - accuracy: 1.0000 - val_loss: 0.3437 - val_accuracy: 0.9545\n",
      "Epoch 358/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 9.6195e-04 - accuracy: 1.0000 - val_loss: 0.3438 - val_accuracy: 0.9546\n",
      "Epoch 359/10000\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 9.5332e-04 - accuracy: 1.0000 - val_loss: 0.3436 - val_accuracy: 0.9547\n",
      "Epoch 360/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 9.5125e-04 - accuracy: 1.0000 - val_loss: 0.3437 - val_accuracy: 0.9545\n",
      "Epoch 361/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 9.4280e-04 - accuracy: 1.0000 - val_loss: 0.3437 - val_accuracy: 0.9546\n",
      "Epoch 362/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 9.3141e-04 - accuracy: 1.0000 - val_loss: 0.3434 - val_accuracy: 0.9546\n",
      "Epoch 363/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 9.2329e-04 - accuracy: 1.0000 - val_loss: 0.3433 - val_accuracy: 0.9549\n",
      "Epoch 364/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 9.1764e-04 - accuracy: 1.0000 - val_loss: 0.3433 - val_accuracy: 0.9551\n",
      "Epoch 365/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 9.1193e-04 - accuracy: 1.0000 - val_loss: 0.3432 - val_accuracy: 0.9550\n",
      "Epoch 366/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 9.0283e-04 - accuracy: 1.0000 - val_loss: 0.3430 - val_accuracy: 0.9551\n",
      "Epoch 367/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 9.1114e-04 - accuracy: 1.0000 - val_loss: 0.3425 - val_accuracy: 0.9554\n",
      "Epoch 368/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 8.8892e-04 - accuracy: 1.0000 - val_loss: 0.3425 - val_accuracy: 0.9554\n",
      "Epoch 369/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 8.8132e-04 - accuracy: 1.0000 - val_loss: 0.3426 - val_accuracy: 0.9553\n",
      "Epoch 370/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 8.7545e-04 - accuracy: 1.0000 - val_loss: 0.3426 - val_accuracy: 0.9552\n",
      "Epoch 371/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 8.6840e-04 - accuracy: 1.0000 - val_loss: 0.3432 - val_accuracy: 0.9554\n",
      "Epoch 372/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 8.6352e-04 - accuracy: 1.0000 - val_loss: 0.3429 - val_accuracy: 0.9553\n",
      "Epoch 373/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 8.5365e-04 - accuracy: 1.0000 - val_loss: 0.3433 - val_accuracy: 0.9553\n",
      "Epoch 374/10000\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 8.5078e-04 - accuracy: 1.0000 - val_loss: 0.3432 - val_accuracy: 0.9553\n",
      "Epoch 375/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 8.4048e-04 - accuracy: 1.0000 - val_loss: 0.3426 - val_accuracy: 0.9553\n",
      "Epoch 376/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 8.3157e-04 - accuracy: 1.0000 - val_loss: 0.3425 - val_accuracy: 0.9552\n",
      "Epoch 377/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 8.2560e-04 - accuracy: 1.0000 - val_loss: 0.3422 - val_accuracy: 0.9552\n",
      "Epoch 378/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 8.1777e-04 - accuracy: 1.0000 - val_loss: 0.3421 - val_accuracy: 0.9553\n",
      "Epoch 379/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 8.1572e-04 - accuracy: 1.0000 - val_loss: 0.3421 - val_accuracy: 0.9554\n",
      "Epoch 380/10000\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 8.0409e-04 - accuracy: 1.0000 - val_loss: 0.3420 - val_accuracy: 0.9554\n",
      "Epoch 381/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 7.9972e-04 - accuracy: 1.0000 - val_loss: 0.3420 - val_accuracy: 0.9555\n",
      "Epoch 382/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 7.9225e-04 - accuracy: 1.0000 - val_loss: 0.3419 - val_accuracy: 0.9554\n",
      "Epoch 383/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 7.8929e-04 - accuracy: 1.0000 - val_loss: 0.3417 - val_accuracy: 0.9555\n",
      "Epoch 384/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 7.8098e-04 - accuracy: 1.0000 - val_loss: 0.3415 - val_accuracy: 0.9554\n",
      "Epoch 385/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 7.7595e-04 - accuracy: 1.0000 - val_loss: 0.3417 - val_accuracy: 0.9554\n",
      "Epoch 386/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 7.6723e-04 - accuracy: 1.0000 - val_loss: 0.3417 - val_accuracy: 0.9553\n",
      "Epoch 387/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 7.6566e-04 - accuracy: 1.0000 - val_loss: 0.3416 - val_accuracy: 0.9556\n",
      "Epoch 388/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 7.5886e-04 - accuracy: 1.0000 - val_loss: 0.3414 - val_accuracy: 0.9554\n",
      "Epoch 389/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 7.5215e-04 - accuracy: 1.0000 - val_loss: 0.3413 - val_accuracy: 0.9556\n",
      "Epoch 390/10000\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 7.4468e-04 - accuracy: 1.0000 - val_loss: 0.3414 - val_accuracy: 0.9556\n",
      "Epoch 391/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 7.3900e-04 - accuracy: 1.0000 - val_loss: 0.3412 - val_accuracy: 0.9554\n",
      "Epoch 392/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 7.3616e-04 - accuracy: 1.0000 - val_loss: 0.3413 - val_accuracy: 0.9556\n",
      "Epoch 393/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 7.2800e-04 - accuracy: 1.0000 - val_loss: 0.3411 - val_accuracy: 0.9556\n",
      "Epoch 394/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 7.2282e-04 - accuracy: 1.0000 - val_loss: 0.3412 - val_accuracy: 0.9556\n",
      "Epoch 395/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 7.1702e-04 - accuracy: 1.0000 - val_loss: 0.3410 - val_accuracy: 0.9556\n",
      "Epoch 396/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 7.1056e-04 - accuracy: 1.0000 - val_loss: 0.3407 - val_accuracy: 0.9556\n",
      "Epoch 397/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 7.0567e-04 - accuracy: 1.0000 - val_loss: 0.3402 - val_accuracy: 0.9559\n",
      "Epoch 398/10000\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 7.0188e-04 - accuracy: 1.0000 - val_loss: 0.3402 - val_accuracy: 0.9556\n",
      "Epoch 399/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 6.9676e-04 - accuracy: 1.0000 - val_loss: 0.3402 - val_accuracy: 0.9555\n",
      "Epoch 400/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 6.9132e-04 - accuracy: 1.0000 - val_loss: 0.3405 - val_accuracy: 0.9556\n",
      "Epoch 401/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 6.8334e-04 - accuracy: 1.0000 - val_loss: 0.3404 - val_accuracy: 0.9556\n",
      "Epoch 402/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 6.8306e-04 - accuracy: 1.0000 - val_loss: 0.3402 - val_accuracy: 0.9557\n",
      "Epoch 403/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 6.7110e-04 - accuracy: 1.0000 - val_loss: 0.3399 - val_accuracy: 0.9557\n",
      "Epoch 404/10000\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 6.6510e-04 - accuracy: 1.0000 - val_loss: 0.3400 - val_accuracy: 0.9557\n",
      "Epoch 405/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 6.6315e-04 - accuracy: 1.0000 - val_loss: 0.3401 - val_accuracy: 0.9559\n",
      "Epoch 406/10000\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 6.6031e-04 - accuracy: 1.0000 - val_loss: 0.3399 - val_accuracy: 0.9557\n",
      "Epoch 407/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 6.5143e-04 - accuracy: 1.0000 - val_loss: 0.3397 - val_accuracy: 0.9556\n",
      "Epoch 408/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 6.4569e-04 - accuracy: 1.0000 - val_loss: 0.3396 - val_accuracy: 0.9556\n",
      "Epoch 409/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 6.4043e-04 - accuracy: 1.0000 - val_loss: 0.3395 - val_accuracy: 0.9556\n",
      "Epoch 410/10000\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 6.3907e-04 - accuracy: 1.0000 - val_loss: 0.3395 - val_accuracy: 0.9557\n",
      "Epoch 411/10000\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 6.3020e-04 - accuracy: 1.0000 - val_loss: 0.3397 - val_accuracy: 0.9560\n",
      "Epoch 412/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 6.3095e-04 - accuracy: 1.0000 - val_loss: 0.3394 - val_accuracy: 0.9560\n",
      "Epoch 413/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 6.2483e-04 - accuracy: 1.0000 - val_loss: 0.3395 - val_accuracy: 0.9561\n",
      "Epoch 414/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 6.2046e-04 - accuracy: 1.0000 - val_loss: 0.3390 - val_accuracy: 0.9559\n",
      "Epoch 415/10000\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 6.1060e-04 - accuracy: 1.0000 - val_loss: 0.3391 - val_accuracy: 0.9560\n",
      "Epoch 416/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 6.1015e-04 - accuracy: 1.0000 - val_loss: 0.3392 - val_accuracy: 0.9561\n",
      "Epoch 417/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 6.0208e-04 - accuracy: 1.0000 - val_loss: 0.3390 - val_accuracy: 0.9561\n",
      "Epoch 418/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.9895e-04 - accuracy: 1.0000 - val_loss: 0.3392 - val_accuracy: 0.9561\n",
      "Epoch 419/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.9248e-04 - accuracy: 1.0000 - val_loss: 0.3392 - val_accuracy: 0.9563\n",
      "Epoch 420/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 5.8771e-04 - accuracy: 1.0000 - val_loss: 0.3392 - val_accuracy: 0.9560\n",
      "Epoch 421/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.8259e-04 - accuracy: 1.0000 - val_loss: 0.3391 - val_accuracy: 0.9562\n",
      "Epoch 422/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.7762e-04 - accuracy: 1.0000 - val_loss: 0.3389 - val_accuracy: 0.9563\n",
      "Epoch 423/10000\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 5.7304e-04 - accuracy: 1.0000 - val_loss: 0.3387 - val_accuracy: 0.9563\n",
      "Epoch 424/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 5.7015e-04 - accuracy: 1.0000 - val_loss: 0.3384 - val_accuracy: 0.9563\n",
      "Epoch 425/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.6513e-04 - accuracy: 1.0000 - val_loss: 0.3385 - val_accuracy: 0.9564\n",
      "Epoch 426/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 5.6062e-04 - accuracy: 1.0000 - val_loss: 0.3388 - val_accuracy: 0.9564\n",
      "Epoch 427/10000\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 5.5671e-04 - accuracy: 1.0000 - val_loss: 0.3385 - val_accuracy: 0.9564\n",
      "Epoch 428/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.5193e-04 - accuracy: 1.0000 - val_loss: 0.3385 - val_accuracy: 0.9566\n",
      "Epoch 429/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.4939e-04 - accuracy: 1.0000 - val_loss: 0.3382 - val_accuracy: 0.9564\n",
      "Epoch 430/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.4631e-04 - accuracy: 1.0000 - val_loss: 0.3385 - val_accuracy: 0.9563\n",
      "Epoch 431/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.3780e-04 - accuracy: 1.0000 - val_loss: 0.3384 - val_accuracy: 0.9564\n",
      "Epoch 432/10000\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 5.3410e-04 - accuracy: 1.0000 - val_loss: 0.3383 - val_accuracy: 0.9565\n",
      "Epoch 433/10000\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 5.3169e-04 - accuracy: 1.0000 - val_loss: 0.3383 - val_accuracy: 0.9567\n",
      "Epoch 434/10000\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 5.2700e-04 - accuracy: 1.0000 - val_loss: 0.3382 - val_accuracy: 0.9567\n",
      "Epoch 435/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 5.2408e-04 - accuracy: 1.0000 - val_loss: 0.3381 - val_accuracy: 0.9567\n",
      "Epoch 436/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 5.2281e-04 - accuracy: 1.0000 - val_loss: 0.3381 - val_accuracy: 0.9568\n",
      "Epoch 437/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 5.1590e-04 - accuracy: 1.0000 - val_loss: 0.3380 - val_accuracy: 0.9568\n",
      "Epoch 438/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.1138e-04 - accuracy: 1.0000 - val_loss: 0.3380 - val_accuracy: 0.9568\n",
      "Epoch 439/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.0998e-04 - accuracy: 1.0000 - val_loss: 0.3376 - val_accuracy: 0.9568\n",
      "Epoch 440/10000\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 5.0433e-04 - accuracy: 1.0000 - val_loss: 0.3376 - val_accuracy: 0.9568\n",
      "Epoch 441/10000\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 5.0261e-04 - accuracy: 1.0000 - val_loss: 0.3377 - val_accuracy: 0.9567\n",
      "Epoch 442/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 5.0113e-04 - accuracy: 1.0000 - val_loss: 0.3374 - val_accuracy: 0.9567\n",
      "Epoch 443/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 4.9151e-04 - accuracy: 1.0000 - val_loss: 0.3374 - val_accuracy: 0.9569\n",
      "Epoch 444/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.8831e-04 - accuracy: 1.0000 - val_loss: 0.3375 - val_accuracy: 0.9568\n",
      "Epoch 445/10000\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 4.8525e-04 - accuracy: 1.0000 - val_loss: 0.3377 - val_accuracy: 0.9570\n",
      "Epoch 446/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 4.8179e-04 - accuracy: 1.0000 - val_loss: 0.3376 - val_accuracy: 0.9569\n",
      "Epoch 447/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 4.7886e-04 - accuracy: 1.0000 - val_loss: 0.3377 - val_accuracy: 0.9571\n",
      "Epoch 448/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 4.7437e-04 - accuracy: 1.0000 - val_loss: 0.3377 - val_accuracy: 0.9571\n",
      "Epoch 449/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 4.7060e-04 - accuracy: 1.0000 - val_loss: 0.3377 - val_accuracy: 0.9573\n",
      "Epoch 450/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 4.6736e-04 - accuracy: 1.0000 - val_loss: 0.3374 - val_accuracy: 0.9573\n",
      "Epoch 451/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 4.6425e-04 - accuracy: 1.0000 - val_loss: 0.3373 - val_accuracy: 0.9573\n",
      "Epoch 452/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 4.5984e-04 - accuracy: 1.0000 - val_loss: 0.3370 - val_accuracy: 0.9574\n",
      "Epoch 453/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.5643e-04 - accuracy: 1.0000 - val_loss: 0.3372 - val_accuracy: 0.9574\n",
      "Epoch 454/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.5470e-04 - accuracy: 1.0000 - val_loss: 0.3371 - val_accuracy: 0.9573\n",
      "Epoch 455/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.4907e-04 - accuracy: 1.0000 - val_loss: 0.3371 - val_accuracy: 0.9574\n",
      "Epoch 456/10000\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 4.4786e-04 - accuracy: 1.0000 - val_loss: 0.3369 - val_accuracy: 0.9573\n",
      "Epoch 457/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 4.4299e-04 - accuracy: 1.0000 - val_loss: 0.3368 - val_accuracy: 0.9575\n",
      "Epoch 458/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.4200e-04 - accuracy: 1.0000 - val_loss: 0.3368 - val_accuracy: 0.9576\n",
      "Epoch 459/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 4.3575e-04 - accuracy: 1.0000 - val_loss: 0.3366 - val_accuracy: 0.9576\n",
      "Epoch 460/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 4.3462e-04 - accuracy: 1.0000 - val_loss: 0.3366 - val_accuracy: 0.9576\n",
      "Epoch 461/10000\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 4.2985e-04 - accuracy: 1.0000 - val_loss: 0.3365 - val_accuracy: 0.9576\n",
      "Epoch 462/10000\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 4.2524e-04 - accuracy: 1.0000 - val_loss: 0.3363 - val_accuracy: 0.9579\n",
      "Epoch 463/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.2251e-04 - accuracy: 1.0000 - val_loss: 0.3364 - val_accuracy: 0.9579\n",
      "Epoch 464/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.2072e-04 - accuracy: 1.0000 - val_loss: 0.3363 - val_accuracy: 0.9578\n",
      "Epoch 465/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 4.1851e-04 - accuracy: 1.0000 - val_loss: 0.3362 - val_accuracy: 0.9578\n",
      "Epoch 466/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 4.1493e-04 - accuracy: 1.0000 - val_loss: 0.3361 - val_accuracy: 0.9579\n",
      "Epoch 467/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 4.1044e-04 - accuracy: 1.0000 - val_loss: 0.3363 - val_accuracy: 0.9580\n",
      "Epoch 468/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.0706e-04 - accuracy: 1.0000 - val_loss: 0.3362 - val_accuracy: 0.9581\n",
      "Epoch 469/10000\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4.0511e-04 - accuracy: 1.0000 - val_loss: 0.3362 - val_accuracy: 0.9580\n",
      "Epoch 470/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.0595e-04 - accuracy: 1.0000 - val_loss: 0.3362 - val_accuracy: 0.9582\n",
      "Epoch 471/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.0004e-04 - accuracy: 1.0000 - val_loss: 0.3364 - val_accuracy: 0.9582\n",
      "Epoch 472/10000\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 3.9668e-04 - accuracy: 1.0000 - val_loss: 0.3362 - val_accuracy: 0.9582\n",
      "Epoch 473/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 3.9296e-04 - accuracy: 1.0000 - val_loss: 0.3357 - val_accuracy: 0.9582\n",
      "Epoch 474/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3.8954e-04 - accuracy: 1.0000 - val_loss: 0.3357 - val_accuracy: 0.9580\n",
      "Epoch 475/10000\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 3.8844e-04 - accuracy: 1.0000 - val_loss: 0.3359 - val_accuracy: 0.9581\n",
      "Epoch 476/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 3.8486e-04 - accuracy: 1.0000 - val_loss: 0.3359 - val_accuracy: 0.9580\n",
      "Epoch 477/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 3.8541e-04 - accuracy: 1.0000 - val_loss: 0.3365 - val_accuracy: 0.9585\n",
      "Epoch 478/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 3.7968e-04 - accuracy: 1.0000 - val_loss: 0.3363 - val_accuracy: 0.9586\n",
      "Epoch 479/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 3.7696e-04 - accuracy: 1.0000 - val_loss: 0.3361 - val_accuracy: 0.9581\n",
      "Epoch 480/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3.7236e-04 - accuracy: 1.0000 - val_loss: 0.3358 - val_accuracy: 0.9584\n",
      "Epoch 481/10000\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 3.6885e-04 - accuracy: 1.0000 - val_loss: 0.3356 - val_accuracy: 0.9585\n",
      "Epoch 482/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3.6729e-04 - accuracy: 1.0000 - val_loss: 0.3356 - val_accuracy: 0.9585\n",
      "Epoch 483/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3.6397e-04 - accuracy: 1.0000 - val_loss: 0.3356 - val_accuracy: 0.9585\n",
      "Epoch 484/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 3.5991e-04 - accuracy: 1.0000 - val_loss: 0.3357 - val_accuracy: 0.9585\n",
      "Epoch 485/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3.5792e-04 - accuracy: 1.0000 - val_loss: 0.3359 - val_accuracy: 0.9586\n",
      "Epoch 486/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3.5450e-04 - accuracy: 1.0000 - val_loss: 0.3358 - val_accuracy: 0.9585\n",
      "Epoch 487/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3.5341e-04 - accuracy: 1.0000 - val_loss: 0.3355 - val_accuracy: 0.9586\n",
      "Epoch 488/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 3.4970e-04 - accuracy: 1.0000 - val_loss: 0.3356 - val_accuracy: 0.9587\n",
      "Epoch 489/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 3.4741e-04 - accuracy: 1.0000 - val_loss: 0.3355 - val_accuracy: 0.9588\n",
      "Epoch 490/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 3.4500e-04 - accuracy: 1.0000 - val_loss: 0.3353 - val_accuracy: 0.9589\n",
      "Epoch 491/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3.4267e-04 - accuracy: 1.0000 - val_loss: 0.3353 - val_accuracy: 0.9587\n",
      "Epoch 492/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3.3887e-04 - accuracy: 1.0000 - val_loss: 0.3350 - val_accuracy: 0.9588\n",
      "Epoch 493/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 3.3772e-04 - accuracy: 1.0000 - val_loss: 0.3350 - val_accuracy: 0.9588\n",
      "Epoch 494/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 3.3488e-04 - accuracy: 1.0000 - val_loss: 0.3348 - val_accuracy: 0.9588\n",
      "Epoch 495/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 3.3244e-04 - accuracy: 1.0000 - val_loss: 0.3347 - val_accuracy: 0.9588\n",
      "Epoch 496/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 3.2924e-04 - accuracy: 1.0000 - val_loss: 0.3345 - val_accuracy: 0.9588\n",
      "Epoch 497/10000\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 3.2797e-04 - accuracy: 1.0000 - val_loss: 0.3344 - val_accuracy: 0.9588\n",
      "Epoch 498/10000\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 3.2627e-04 - accuracy: 1.0000 - val_loss: 0.3344 - val_accuracy: 0.9590\n",
      "Epoch 499/10000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 3.2529e-04 - accuracy: 1.0000 - val_loss: 0.3346 - val_accuracy: 0.9590\n",
      "Epoch 500/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3.1949e-04 - accuracy: 1.0000 - val_loss: 0.3346 - val_accuracy: 0.9588\n",
      "Epoch 501/10000\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 3.1902e-04 - accuracy: 1.0000 - val_loss: 0.3347 - val_accuracy: 0.9589\n",
      "Epoch 502/10000\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 3.1615e-04 - accuracy: 1.0000 - val_loss: 0.3345 - val_accuracy: 0.9588\n",
      "Epoch 503/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3.1308e-04 - accuracy: 1.0000 - val_loss: 0.3345 - val_accuracy: 0.9588\n",
      "Epoch 504/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3.1120e-04 - accuracy: 1.0000 - val_loss: 0.3345 - val_accuracy: 0.9589\n",
      "Epoch 505/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 3.0825e-04 - accuracy: 1.0000 - val_loss: 0.3343 - val_accuracy: 0.9591\n",
      "Epoch 506/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3.0634e-04 - accuracy: 1.0000 - val_loss: 0.3343 - val_accuracy: 0.9590\n",
      "Epoch 507/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3.0444e-04 - accuracy: 1.0000 - val_loss: 0.3341 - val_accuracy: 0.9595\n",
      "Epoch 508/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3.0226e-04 - accuracy: 1.0000 - val_loss: 0.3342 - val_accuracy: 0.9591\n",
      "Epoch 509/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 2.9880e-04 - accuracy: 1.0000 - val_loss: 0.3340 - val_accuracy: 0.9593\n",
      "Epoch 510/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 2.9665e-04 - accuracy: 1.0000 - val_loss: 0.3340 - val_accuracy: 0.9591\n",
      "Epoch 511/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 2.9509e-04 - accuracy: 1.0000 - val_loss: 0.3340 - val_accuracy: 0.9591\n",
      "Epoch 512/10000\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.9227e-04 - accuracy: 1.0000 - val_loss: 0.3340 - val_accuracy: 0.9593\n",
      "Epoch 513/10000\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 2.9203e-04 - accuracy: 1.0000 - val_loss: 0.3341 - val_accuracy: 0.9595\n",
      "Epoch 514/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 2.8846e-04 - accuracy: 1.0000 - val_loss: 0.3341 - val_accuracy: 0.9594\n",
      "Epoch 515/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 2.8561e-04 - accuracy: 1.0000 - val_loss: 0.3341 - val_accuracy: 0.9593\n",
      "Epoch 516/10000\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 2.8552e-04 - accuracy: 1.0000 - val_loss: 0.3343 - val_accuracy: 0.9593\n",
      "Epoch 517/10000\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 2.8195e-04 - accuracy: 1.0000 - val_loss: 0.3342 - val_accuracy: 0.9594\n",
      "Epoch 518/10000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 2.8057e-04 - accuracy: 1.0000 - val_loss: 0.3342 - val_accuracy: 0.9593\n",
      "Epoch 519/10000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 2.7706e-04 - accuracy: 1.0000 - val_loss: 0.3340 - val_accuracy: 0.9594\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f37031ffbe0>"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.fit(X_train_small, y_train_small,validation_data=(X_val, y_val), epochs=10000, batch_size=32, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練時間：60.53 秒\n"
     ]
    }
   ],
   "source": [
    "# 記錄結束時間\n",
    "end_time = time.time()\n",
    "\n",
    "# 計算訓練時間（秒）\n",
    "training_time = end_time - start_time\n",
    "print(f\"訓練時間：{training_time:.2f} 秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/301 [..............................] - ETA: 2s - loss: 0.1109 - accuracy: 0.9375"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301/301 [==============================] - 0s 453us/step - loss: 0.3812 - accuracy: 0.9592\n",
      "Evaluation on 90% unused data - Loss: 0.3812, Accuracy: 0.9592\n"
     ]
    }
   ],
   "source": [
    "# 使用剩餘的 90% 測試資料進行模型評估\n",
    "loss, accuracy = base_model.evaluate(X_test, y_test)\n",
    "print(f\"Evaluation on 90% unused data - Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### 預測\n",
    "# base_model.evaluate(X_test_scaled, y_test_numeric)  # 確保模型在測試模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "616/616 [==============================] - 0s 460us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1-3',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-3',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-3',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-3',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-3',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-3',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-3',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-3',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-1',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '3-11',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '3-11',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '3-11',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '3-11',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-11',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '3-11',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '3-11',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '3-11',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-7',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-11',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-10',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " '1-11',\n",
       " ...]"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 預測測試資料\n",
    "y_test_pred_numeric = base_model.predict(X_test_scaled)\n",
    "y_pred_classes = np.argmax(y_test_pred_numeric, axis=1)\n",
    "\n",
    "# 轉換為原本的 Label\n",
    "y_test_pred_labels = [label_mapping[str(num + 1)] for num in y_pred_classes]  # 補回 +1\n",
    "y_test_pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>level_1</th>\n",
       "      <th>AP1_Distance (mm)</th>\n",
       "      <th>AP2_Distance (mm)</th>\n",
       "      <th>AP3_Distance (mm)</th>\n",
       "      <th>AP4_Distance (mm)</th>\n",
       "      <th>AP1_Rssi</th>\n",
       "      <th>AP2_Rssi</th>\n",
       "      <th>AP3_Rssi</th>\n",
       "      <th>AP4_Rssi</th>\n",
       "      <th>AP1_StdDev (mm)</th>\n",
       "      <th>AP2_StdDev (mm)</th>\n",
       "      <th>AP3_StdDev (mm)</th>\n",
       "      <th>AP4_StdDev (mm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1-1</td>\n",
       "      <td>0</td>\n",
       "      <td>4038.0</td>\n",
       "      <td>7358.0</td>\n",
       "      <td>13018.0</td>\n",
       "      <td>-1855.0</td>\n",
       "      <td>-65.0</td>\n",
       "      <td>-68.0</td>\n",
       "      <td>-64.0</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>965.0</td>\n",
       "      <td>877.0</td>\n",
       "      <td>314.0</td>\n",
       "      <td>2003.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1-1</td>\n",
       "      <td>1</td>\n",
       "      <td>3960.0</td>\n",
       "      <td>7241.0</td>\n",
       "      <td>13252.0</td>\n",
       "      <td>-342.0</td>\n",
       "      <td>-65.0</td>\n",
       "      <td>-68.0</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>-54.0</td>\n",
       "      <td>1056.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>483.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1-1</td>\n",
       "      <td>2</td>\n",
       "      <td>3843.0</td>\n",
       "      <td>7202.0</td>\n",
       "      <td>13838.0</td>\n",
       "      <td>-303.0</td>\n",
       "      <td>-65.0</td>\n",
       "      <td>-68.0</td>\n",
       "      <td>-64.0</td>\n",
       "      <td>-54.0</td>\n",
       "      <td>1053.0</td>\n",
       "      <td>876.0</td>\n",
       "      <td>714.0</td>\n",
       "      <td>583.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1-1</td>\n",
       "      <td>3</td>\n",
       "      <td>3960.0</td>\n",
       "      <td>7397.0</td>\n",
       "      <td>14307.0</td>\n",
       "      <td>-342.0</td>\n",
       "      <td>-65.0</td>\n",
       "      <td>-68.0</td>\n",
       "      <td>-64.0</td>\n",
       "      <td>-54.0</td>\n",
       "      <td>938.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>265.0</td>\n",
       "      <td>522.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1-1</td>\n",
       "      <td>4</td>\n",
       "      <td>3999.0</td>\n",
       "      <td>7075.0</td>\n",
       "      <td>13174.0</td>\n",
       "      <td>-460.0</td>\n",
       "      <td>-65.0</td>\n",
       "      <td>-68.0</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>-54.0</td>\n",
       "      <td>959.0</td>\n",
       "      <td>303.0</td>\n",
       "      <td>341.0</td>\n",
       "      <td>637.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19693</th>\n",
       "      <td>9-11</td>\n",
       "      <td>19693</td>\n",
       "      <td>1850.0</td>\n",
       "      <td>5464.0</td>\n",
       "      <td>2666.0</td>\n",
       "      <td>4852.0</td>\n",
       "      <td>-55.0</td>\n",
       "      <td>-65.0</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>-67.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>959.0</td>\n",
       "      <td>409.0</td>\n",
       "      <td>497.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19694</th>\n",
       "      <td>9-11</td>\n",
       "      <td>19694</td>\n",
       "      <td>1967.0</td>\n",
       "      <td>5112.0</td>\n",
       "      <td>2353.0</td>\n",
       "      <td>4881.0</td>\n",
       "      <td>-55.0</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>-67.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>1158.0</td>\n",
       "      <td>709.0</td>\n",
       "      <td>577.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19695</th>\n",
       "      <td>9-11</td>\n",
       "      <td>19695</td>\n",
       "      <td>1968.0</td>\n",
       "      <td>5024.0</td>\n",
       "      <td>2366.0</td>\n",
       "      <td>4705.0</td>\n",
       "      <td>-55.0</td>\n",
       "      <td>-67.0</td>\n",
       "      <td>-48.0</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>1166.0</td>\n",
       "      <td>671.0</td>\n",
       "      <td>491.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19696</th>\n",
       "      <td>9-11</td>\n",
       "      <td>19696</td>\n",
       "      <td>1772.0</td>\n",
       "      <td>5434.0</td>\n",
       "      <td>2666.0</td>\n",
       "      <td>5008.0</td>\n",
       "      <td>-55.0</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>-52.0</td>\n",
       "      <td>-67.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>861.0</td>\n",
       "      <td>519.0</td>\n",
       "      <td>483.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19697</th>\n",
       "      <td>9-11</td>\n",
       "      <td>19697</td>\n",
       "      <td>1850.0</td>\n",
       "      <td>5229.0</td>\n",
       "      <td>2627.0</td>\n",
       "      <td>5789.0</td>\n",
       "      <td>-55.0</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>-51.0</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>1062.0</td>\n",
       "      <td>572.0</td>\n",
       "      <td>2298.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19698 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Label  level_1  AP1_Distance (mm)  AP2_Distance (mm)  AP3_Distance (mm)  \\\n",
       "0       1-1        0             4038.0             7358.0            13018.0   \n",
       "1       1-1        1             3960.0             7241.0            13252.0   \n",
       "2       1-1        2             3843.0             7202.0            13838.0   \n",
       "3       1-1        3             3960.0             7397.0            14307.0   \n",
       "4       1-1        4             3999.0             7075.0            13174.0   \n",
       "...     ...      ...                ...                ...                ...   \n",
       "19693  9-11    19693             1850.0             5464.0             2666.0   \n",
       "19694  9-11    19694             1967.0             5112.0             2353.0   \n",
       "19695  9-11    19695             1968.0             5024.0             2366.0   \n",
       "19696  9-11    19696             1772.0             5434.0             2666.0   \n",
       "19697  9-11    19697             1850.0             5229.0             2627.0   \n",
       "\n",
       "       AP4_Distance (mm)  AP1_Rssi  AP2_Rssi  AP3_Rssi  AP4_Rssi  \\\n",
       "0                -1855.0     -65.0     -68.0     -64.0     -50.0   \n",
       "1                 -342.0     -65.0     -68.0     -66.0     -54.0   \n",
       "2                 -303.0     -65.0     -68.0     -64.0     -54.0   \n",
       "3                 -342.0     -65.0     -68.0     -64.0     -54.0   \n",
       "4                 -460.0     -65.0     -68.0     -66.0     -54.0   \n",
       "...                  ...       ...       ...       ...       ...   \n",
       "19693             4852.0     -55.0     -65.0     -50.0     -67.0   \n",
       "19694             4881.0     -55.0     -66.0     -50.0     -67.0   \n",
       "19695             4705.0     -55.0     -67.0     -48.0     -66.0   \n",
       "19696             5008.0     -55.0     -66.0     -52.0     -67.0   \n",
       "19697             5789.0     -55.0     -66.0     -51.0     -66.0   \n",
       "\n",
       "       AP1_StdDev (mm)  AP2_StdDev (mm)  AP3_StdDev (mm)  AP4_StdDev (mm)  \n",
       "0                965.0            877.0            314.0           2003.0  \n",
       "1               1056.0            880.0            160.0            483.0  \n",
       "2               1053.0            876.0            714.0            583.0  \n",
       "3                938.0            105.0            265.0            522.0  \n",
       "4                959.0            303.0            341.0            637.0  \n",
       "...                ...              ...              ...              ...  \n",
       "19693            215.0            959.0            409.0            497.0  \n",
       "19694            141.0           1158.0            709.0            577.0  \n",
       "19695            175.0           1166.0            671.0            491.0  \n",
       "19696            151.0            861.0            519.0            483.0  \n",
       "19697            197.0           1062.0            572.0           2298.0  \n",
       "\n",
       "[19698 rows x 14 columns]"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 讀取測試資料的實際 Label\n",
    "y_test_actual = test_data_imputed[target_column]\n",
    "test_data_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data MDE report saved to: DNN 4 mcAPs BEST_2025_01_10.json\n",
      "\n",
      "Test Data Mean MDE: 0.0668 meters\n"
     ]
    }
   ],
   "source": [
    "# 取得預測與實際座標\n",
    "y_test_pred_coordinates = np.array([label_to_coordinates[label] for label in y_test_pred_labels])\n",
    "y_test_actual_coordinates = np.array([label_to_coordinates[label] for label in y_test_actual])\n",
    "\n",
    "# 計算 MDE (Mean Distance Error)\n",
    "distances = np.linalg.norm(y_test_pred_coordinates - y_test_actual_coordinates, axis=1)\n",
    "mean_mde = np.mean(distances)\n",
    "\n",
    "# 記錄每個 RP 的 MDE\n",
    "mde_report_test = {}\n",
    "for true_label, distance in zip(y_test_actual, distances):\n",
    "    if true_label not in mde_report_test:\n",
    "        mde_report_test[true_label] = []\n",
    "    mde_report_test[true_label].append(distance)\n",
    "\n",
    "# 計算測試資料的 MDE 平均值\n",
    "mde_report_test_avg = {label: {\"mde\": np.mean(dists), \"count\": len(dists)} for label, dists in mde_report_test.items()}\n",
    "\n",
    "# 儲存 MDE 結果到 JSON 檔案\n",
    "test_file_path = f\"{modelname}_{date_test}.json\"\n",
    "with open(test_file_path, \"w\") as f:\n",
    "    json.dump(mde_report_test_avg, f, indent=4)\n",
    "\n",
    "print(f\"Test Data MDE report saved to: {test_file_path}\")\n",
    "print(f\"\\nTest Data Mean MDE: {mean_mde:.4f} meters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.save(f\"DNN_best_model4mcAP_1week_to_2week_20dataPerRP.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TTM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
