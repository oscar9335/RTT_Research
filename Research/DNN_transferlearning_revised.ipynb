{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 載入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_coordinates = {\n",
    "    \"1-1\": (0, 0), \"1-2\": (0.6, 0), \"1-3\": (1.2, 0), \"1-4\": (1.8, 0), \"1-5\": (2.4, 0), \"1-6\": (3.0, 0),\"1-7\": (3.6, 0), \"1-8\": (4.2, 0), \"1-9\": (4.8, 0), \"1-10\": (5.4, 0), \"1-11\": (6.0, 0),\n",
    "    \"2-1\": (0, 0.6), \"2-11\": (6.0, 0.6),\n",
    "    \"3-1\": (0, 1.2), \"3-11\": (6.0, 1.2),\n",
    "    \"4-1\": (0, 1.8), \"4-11\": (6.0, 1.8),\n",
    "    \"5-1\": (0, 2.4), \"5-11\": (6.0, 2.4),\n",
    "    \"6-1\": (0, 3.0), \"6-2\": (0.6, 3.0), \"6-3\": (1.2, 3.0), \"6-4\": (1.8, 3.0), \"6-5\": (2.4, 3.0),\"6-6\": (3.0, 3.0), \"6-7\": (3.6, 3.0), \"6-8\": (4.2, 3.0), \"6-9\": (4.8, 3.0), \"6-10\": (5.4, 3.0), \"6-11\": (6.0, 3.0),\n",
    "    \"7-1\": (0, 3.6), \"7-11\": (6.0, 3.6),\n",
    "    \"8-1\": (0, 4.2), \"8-11\": (6.0, 4.2),\n",
    "    \"9-1\": (0, 4.8), \"9-11\": (6.0, 4.8),\n",
    "    \"10-1\": (0, 5.4), \"10-11\": (6.0, 5.4),\n",
    "    \"11-1\": (0, 6.0), \"11-2\": (0.6, 6.0), \"11-3\": (1.2, 6.0), \"11-4\": (1.8, 6.0), \"11-5\": (2.4, 6.0),\"11-6\": (3.0, 6.0), \"11-7\": (3.6, 6.0), \"11-8\": (4.2, 6.0), \"11-9\": (4.8, 6.0), \"11-10\": (5.4, 6.0), \"11-11\": (6.0, 6.0)\n",
    "}\n",
    "label_mapping = {\n",
    "    '11': '1-1','10': '1-2','9': '1-3','8': '1-4','7': '1-5','6': '1-6','5': '1-7','4': '1-8','3': '1-9','2': '1-10','1': '1-11',\n",
    "    '12': '2-1','30': '2-11',\n",
    "    '13': '3-1','29': '3-11',\n",
    "    '14': '4-1','28': '4-11',\n",
    "    '15': '5-1','27': '5-11',\n",
    "    '16': '6-1','17': '6-2','18': '6-3','19': '6-4','20': '6-5','21': '6-6','22': '6-7','23': '6-8','24': '6-9','25': '6-10','26': '6-11',\n",
    "    '49': '7-1','31': '7-11',\n",
    "    '48': '8-1','32': '8-11',\n",
    "    '47': '9-1','33': '9-11',\n",
    "    '46': '10-1','34': '10-11',\n",
    "    '45': '11-1','44': '11-2','43': '11-3','42': '11-4','41': '11-5','40': '11-6','39': '11-7','38': '11-8','37': '11-9','36': '11-10','35': '11-11'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import joblib  # 用於保存模型\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = ['Label',\n",
    "                        'AP2_Distance (mm)','AP4_Distance (mm)',\n",
    "                        'AP2_StdDev (mm)','AP4_StdDev (mm)',\n",
    "                                'AP1_Rssi','AP2_Rssi','AP3_Rssi','AP4_Rssi']  \n",
    "\n",
    "X_testing_selected_columns = [\n",
    "'AP2_Distance (mm)','AP4_Distance (mm)',\n",
    "                        'AP2_StdDev (mm)','AP4_StdDev (mm)',\n",
    "                                'AP1_Rssi','AP2_Rssi','AP3_Rssi','AP4_Rssi']  \n",
    "\n",
    "label_column = 'Label'\n",
    "target_column = 'Label'\n",
    "\n",
    "\n",
    "#  'AP1_Rssi','AP2_Rssi','AP3_Rssi','AP4_Rssi'\n",
    "# 'AP1_Distance (mm)','AP2_Distance (mm)','AP3_Distance (mm)','AP4_Distance (mm)',\n",
    "# 'AP1_StdDev (mm)','AP2_StdDev (mm)','AP3_StdDev (mm)','AP4_StdDev (mm)',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap = '2mcAP_Worst'\n",
    "root = '20 data per RP transfer revised'\n",
    "\n",
    "dataamount = 20\n",
    "N_val = 4\n",
    "\n",
    "N_train = dataamount # 訓練集每個類別至少要有 N_train 筆資料\n",
    "test_val_ratio = 1  # 剩餘資料中，50% 作為驗證集，50% 作為測試集\n",
    "\n",
    "weekrepresent = [['0','0'],['0','1'],['1','2'],['2','3'],['3','4'],['4','5']]\n",
    "alldatadate = ['2024_12_21','2024_12_27','2025_01_03','2025_01_10','2025_02_28']\n",
    "number_of_week = len(alldatadate)\n",
    "\n",
    "ALL_trainingtime = []\n",
    "ALL_loss = []\n",
    "ALL_accuracy = []\n",
    "ALL_mean_mde = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_104 (Dense)           (None, 64)                320       \n",
      "                                                                 \n",
      " dense_105 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_106 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_107 (Dense)           (None, 49)                3185      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20081 (78.44 KB)\n",
      "Trainable params: 3185 (12.44 KB)\n",
      "Non-trainable params: 16896 (66.00 KB)\n",
      "_________________________________________________________________\n",
      "   AP1_Rssi  AP2_Rssi  AP3_Rssi  AP4_Rssi\n",
      "0     -63.0     -74.0     -65.0     -49.0\n",
      "1     -63.0     -74.0     -67.0     -52.0\n",
      "2     -64.0     -75.0     -69.0     -52.0\n",
      "3     -63.0     -74.0     -63.0     -48.0\n",
      "4     -58.0     -73.0     -63.0     -54.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3268413/3821025327.py:45: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  test_processed_data = pd.concat([test_processed_data, group], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Set</th>\n",
       "      <th>Validation Set</th>\n",
       "      <th>Test Set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Training Set  Validation Set  Test Set\n",
       "0             32               8       325\n",
       "1             32               8       325\n",
       "2             32               8       325\n",
       "3             32               8       325\n",
       "4             32               8       325\n",
       "5             32               8       325\n",
       "6             32               8       325\n",
       "7             32               8       325\n",
       "8             32               8       325\n",
       "9             32               8       325\n",
       "10            32               8       325\n",
       "11            32               8       325\n",
       "12            32               8       325\n",
       "13            32               8       325\n",
       "14            32               8       325\n",
       "15            32               8       325\n",
       "16            32               8       325\n",
       "17            32               8       325\n",
       "18            32               8       325\n",
       "19            32               8       325\n",
       "20            32               8       325\n",
       "21            32               8       325\n",
       "22            32               8       325\n",
       "23            32               8       325\n",
       "24            32               8       325\n",
       "25            32               8       325\n",
       "26            32               8       325\n",
       "27            32               8       325\n",
       "28            32               8       325\n",
       "29            32               8       325\n",
       "30            32               8       325\n",
       "31            32               8       325\n",
       "32            32               8       325\n",
       "33            32               8       325\n",
       "34            32               8       325\n",
       "35            32               8       325\n",
       "36            32               8       325\n",
       "37            32               8       325\n",
       "38            32               8       325\n",
       "39            32               8       325\n",
       "40            32               8       325\n",
       "41            32               8       325\n",
       "42            32               8       325\n",
       "43            32               8       325\n",
       "44            32               8       325\n",
       "45            32               8       325\n",
       "46            32               8       325\n",
       "47            32               8       325\n",
       "48            32               8       325"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "Epoch 1/10000\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 21.1419 - accuracy: 0.1767 - val_loss: 19.6214 - val_accuracy: 0.2143\n",
      "Epoch 2/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 18.5534 - accuracy: 0.2239 - val_loss: 17.5148 - val_accuracy: 0.2602\n",
      "Epoch 3/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 16.4431 - accuracy: 0.2679 - val_loss: 15.7944 - val_accuracy: 0.2832\n",
      "Epoch 4/10000\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 14.7246 - accuracy: 0.3093 - val_loss: 14.3742 - val_accuracy: 0.3189\n",
      "Epoch 5/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 13.3398 - accuracy: 0.3431 - val_loss: 13.1807 - val_accuracy: 0.3495\n",
      "Epoch 6/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 12.1507 - accuracy: 0.3769 - val_loss: 12.1896 - val_accuracy: 0.3878\n",
      "Epoch 7/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 11.1420 - accuracy: 0.4094 - val_loss: 11.3366 - val_accuracy: 0.4209\n",
      "Epoch 8/10000\n",
      "49/49 [==============================] - 0s 850us/step - loss: 10.2577 - accuracy: 0.4388 - val_loss: 10.6022 - val_accuracy: 0.4413\n",
      "Epoch 9/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 9.4746 - accuracy: 0.4554 - val_loss: 9.9373 - val_accuracy: 0.4643\n",
      "Epoch 10/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 8.7891 - accuracy: 0.4783 - val_loss: 9.3531 - val_accuracy: 0.4821\n",
      "Epoch 11/10000\n",
      "49/49 [==============================] - 0s 853us/step - loss: 8.1423 - accuracy: 0.4936 - val_loss: 8.8366 - val_accuracy: 0.5026\n",
      "Epoch 12/10000\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 7.5622 - accuracy: 0.5102 - val_loss: 8.3562 - val_accuracy: 0.5179\n",
      "Epoch 13/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 7.0304 - accuracy: 0.5325 - val_loss: 7.9275 - val_accuracy: 0.5306\n",
      "Epoch 14/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 6.5480 - accuracy: 0.5446 - val_loss: 7.5372 - val_accuracy: 0.5357\n",
      "Epoch 15/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 6.0932 - accuracy: 0.5536 - val_loss: 7.1611 - val_accuracy: 0.5434\n",
      "Epoch 16/10000\n",
      "49/49 [==============================] - 0s 901us/step - loss: 5.6746 - accuracy: 0.5676 - val_loss: 6.7949 - val_accuracy: 0.5536\n",
      "Epoch 17/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 5.2921 - accuracy: 0.5842 - val_loss: 6.4409 - val_accuracy: 0.5485\n",
      "Epoch 18/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 4.9287 - accuracy: 0.5861 - val_loss: 6.1322 - val_accuracy: 0.5510\n",
      "Epoch 19/10000\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 4.5925 - accuracy: 0.5969 - val_loss: 5.8320 - val_accuracy: 0.5689\n",
      "Epoch 20/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 4.2876 - accuracy: 0.6110 - val_loss: 5.5554 - val_accuracy: 0.5714\n",
      "Epoch 21/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 4.0044 - accuracy: 0.6186 - val_loss: 5.2887 - val_accuracy: 0.5689\n",
      "Epoch 22/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 3.7406 - accuracy: 0.6288 - val_loss: 5.0484 - val_accuracy: 0.5842\n",
      "Epoch 23/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 3.4958 - accuracy: 0.6416 - val_loss: 4.8237 - val_accuracy: 0.5944\n",
      "Epoch 24/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 3.2742 - accuracy: 0.6499 - val_loss: 4.6261 - val_accuracy: 0.6046\n",
      "Epoch 25/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 3.0916 - accuracy: 0.6671 - val_loss: 4.4434 - val_accuracy: 0.6199\n",
      "Epoch 26/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 2.9242 - accuracy: 0.6818 - val_loss: 4.2813 - val_accuracy: 0.6224\n",
      "Epoch 27/10000\n",
      "49/49 [==============================] - 0s 895us/step - loss: 2.7683 - accuracy: 0.6818 - val_loss: 4.1351 - val_accuracy: 0.6327\n",
      "Epoch 28/10000\n",
      "49/49 [==============================] - 0s 846us/step - loss: 2.6264 - accuracy: 0.6971 - val_loss: 3.9835 - val_accuracy: 0.6429\n",
      "Epoch 29/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 2.4999 - accuracy: 0.7047 - val_loss: 3.8721 - val_accuracy: 0.6505\n",
      "Epoch 30/10000\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 2.3820 - accuracy: 0.7117 - val_loss: 3.7515 - val_accuracy: 0.6607\n",
      "Epoch 31/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 2.2758 - accuracy: 0.7226 - val_loss: 3.6491 - val_accuracy: 0.6607\n",
      "Epoch 32/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 2.1838 - accuracy: 0.7360 - val_loss: 3.5607 - val_accuracy: 0.6607\n",
      "Epoch 33/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 2.0886 - accuracy: 0.7430 - val_loss: 3.4728 - val_accuracy: 0.6607\n",
      "Epoch 34/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 2.0084 - accuracy: 0.7551 - val_loss: 3.3901 - val_accuracy: 0.6684\n",
      "Epoch 35/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.9310 - accuracy: 0.7653 - val_loss: 3.3163 - val_accuracy: 0.6786\n",
      "Epoch 36/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.8638 - accuracy: 0.7691 - val_loss: 3.2445 - val_accuracy: 0.6811\n",
      "Epoch 37/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.7937 - accuracy: 0.7787 - val_loss: 3.1765 - val_accuracy: 0.6888\n",
      "Epoch 38/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.7378 - accuracy: 0.7787 - val_loss: 3.1207 - val_accuracy: 0.6811\n",
      "Epoch 39/10000\n",
      "49/49 [==============================] - 0s 749us/step - loss: 1.6759 - accuracy: 0.7832 - val_loss: 3.0546 - val_accuracy: 0.6990\n",
      "Epoch 40/10000\n",
      "49/49 [==============================] - 0s 680us/step - loss: 1.6266 - accuracy: 0.7876 - val_loss: 3.0051 - val_accuracy: 0.6990\n",
      "Epoch 41/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.5727 - accuracy: 0.7902 - val_loss: 2.9494 - val_accuracy: 0.7117\n",
      "Epoch 42/10000\n",
      "49/49 [==============================] - 0s 856us/step - loss: 1.5256 - accuracy: 0.7978 - val_loss: 2.8986 - val_accuracy: 0.7041\n",
      "Epoch 43/10000\n",
      "49/49 [==============================] - 0s 915us/step - loss: 1.4807 - accuracy: 0.8042 - val_loss: 2.8486 - val_accuracy: 0.7143\n",
      "Epoch 44/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.4446 - accuracy: 0.8061 - val_loss: 2.8034 - val_accuracy: 0.7117\n",
      "Epoch 45/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.4077 - accuracy: 0.8138 - val_loss: 2.7637 - val_accuracy: 0.7143\n",
      "Epoch 46/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.3684 - accuracy: 0.8195 - val_loss: 2.7214 - val_accuracy: 0.7270\n",
      "Epoch 47/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.3316 - accuracy: 0.8240 - val_loss: 2.6861 - val_accuracy: 0.7296\n",
      "Epoch 48/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.2997 - accuracy: 0.8246 - val_loss: 2.6537 - val_accuracy: 0.7296\n",
      "Epoch 49/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.2734 - accuracy: 0.8284 - val_loss: 2.6193 - val_accuracy: 0.7270\n",
      "Epoch 50/10000\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 1.2402 - accuracy: 0.8272 - val_loss: 2.5915 - val_accuracy: 0.7245\n",
      "Epoch 51/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.2119 - accuracy: 0.8348 - val_loss: 2.5566 - val_accuracy: 0.7321\n",
      "Epoch 52/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.1870 - accuracy: 0.8361 - val_loss: 2.5286 - val_accuracy: 0.7398\n",
      "Epoch 53/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.1598 - accuracy: 0.8386 - val_loss: 2.5059 - val_accuracy: 0.7372\n",
      "Epoch 54/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.1351 - accuracy: 0.8386 - val_loss: 2.4723 - val_accuracy: 0.7423\n",
      "Epoch 55/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.1115 - accuracy: 0.8399 - val_loss: 2.4545 - val_accuracy: 0.7398\n",
      "Epoch 56/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.0857 - accuracy: 0.8425 - val_loss: 2.4263 - val_accuracy: 0.7423\n",
      "Epoch 57/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.0618 - accuracy: 0.8444 - val_loss: 2.4054 - val_accuracy: 0.7423\n",
      "Epoch 58/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.0414 - accuracy: 0.8450 - val_loss: 2.3764 - val_accuracy: 0.7474\n",
      "Epoch 59/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.0196 - accuracy: 0.8495 - val_loss: 2.3627 - val_accuracy: 0.7449\n",
      "Epoch 60/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.9970 - accuracy: 0.8508 - val_loss: 2.3351 - val_accuracy: 0.7449\n",
      "Epoch 61/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.9753 - accuracy: 0.8533 - val_loss: 2.3160 - val_accuracy: 0.7526\n",
      "Epoch 62/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.9600 - accuracy: 0.8527 - val_loss: 2.2925 - val_accuracy: 0.7551\n",
      "Epoch 63/10000\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.9408 - accuracy: 0.8591 - val_loss: 2.2803 - val_accuracy: 0.7500\n",
      "Epoch 64/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.9194 - accuracy: 0.8565 - val_loss: 2.2589 - val_accuracy: 0.7474\n",
      "Epoch 65/10000\n",
      "49/49 [==============================] - 0s 738us/step - loss: 0.9059 - accuracy: 0.8597 - val_loss: 2.2445 - val_accuracy: 0.7577\n",
      "Epoch 66/10000\n",
      "49/49 [==============================] - 0s 751us/step - loss: 0.8860 - accuracy: 0.8591 - val_loss: 2.2243 - val_accuracy: 0.7526\n",
      "Epoch 67/10000\n",
      "49/49 [==============================] - 0s 691us/step - loss: 0.8699 - accuracy: 0.8610 - val_loss: 2.2083 - val_accuracy: 0.7628\n",
      "Epoch 68/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.8536 - accuracy: 0.8667 - val_loss: 2.1900 - val_accuracy: 0.7628\n",
      "Epoch 69/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.8363 - accuracy: 0.8686 - val_loss: 2.1716 - val_accuracy: 0.7679\n",
      "Epoch 70/10000\n",
      "49/49 [==============================] - 0s 940us/step - loss: 0.8217 - accuracy: 0.8680 - val_loss: 2.1594 - val_accuracy: 0.7755\n",
      "Epoch 71/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.8050 - accuracy: 0.8705 - val_loss: 2.1486 - val_accuracy: 0.7755\n",
      "Epoch 72/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.7948 - accuracy: 0.8699 - val_loss: 2.1334 - val_accuracy: 0.7806\n",
      "Epoch 73/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.7797 - accuracy: 0.8763 - val_loss: 2.1092 - val_accuracy: 0.7781\n",
      "Epoch 74/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.7641 - accuracy: 0.8731 - val_loss: 2.1062 - val_accuracy: 0.7934\n",
      "Epoch 75/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.7501 - accuracy: 0.8807 - val_loss: 2.0923 - val_accuracy: 0.7857\n",
      "Epoch 76/10000\n",
      "49/49 [==============================] - 0s 924us/step - loss: 0.7340 - accuracy: 0.8750 - val_loss: 2.0918 - val_accuracy: 0.7883\n",
      "Epoch 77/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.7227 - accuracy: 0.8782 - val_loss: 2.0761 - val_accuracy: 0.7857\n",
      "Epoch 78/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.7097 - accuracy: 0.8807 - val_loss: 2.0625 - val_accuracy: 0.7959\n",
      "Epoch 79/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.6984 - accuracy: 0.8820 - val_loss: 2.0542 - val_accuracy: 0.7857\n",
      "Epoch 80/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.6835 - accuracy: 0.8839 - val_loss: 2.0387 - val_accuracy: 0.7959\n",
      "Epoch 81/10000\n",
      "49/49 [==============================] - 0s 882us/step - loss: 0.6750 - accuracy: 0.8884 - val_loss: 2.0326 - val_accuracy: 0.7934\n",
      "Epoch 82/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.6612 - accuracy: 0.8865 - val_loss: 2.0241 - val_accuracy: 0.7883\n",
      "Epoch 83/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.6511 - accuracy: 0.8852 - val_loss: 2.0095 - val_accuracy: 0.7908\n",
      "Epoch 84/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.6397 - accuracy: 0.8846 - val_loss: 2.0052 - val_accuracy: 0.7959\n",
      "Epoch 85/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.6283 - accuracy: 0.8852 - val_loss: 1.9976 - val_accuracy: 0.7985\n",
      "Epoch 86/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.6181 - accuracy: 0.8858 - val_loss: 1.9881 - val_accuracy: 0.8112\n",
      "Epoch 87/10000\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.6066 - accuracy: 0.8909 - val_loss: 1.9819 - val_accuracy: 0.8010\n",
      "Epoch 88/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.5950 - accuracy: 0.8941 - val_loss: 1.9758 - val_accuracy: 0.8061\n",
      "Epoch 89/10000\n",
      "49/49 [==============================] - 0s 907us/step - loss: 0.5839 - accuracy: 0.8935 - val_loss: 1.9736 - val_accuracy: 0.8036\n",
      "Epoch 90/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.5793 - accuracy: 0.8897 - val_loss: 1.9647 - val_accuracy: 0.8061\n",
      "Epoch 91/10000\n",
      "49/49 [==============================] - 0s 893us/step - loss: 0.5656 - accuracy: 0.8922 - val_loss: 1.9637 - val_accuracy: 0.8010\n",
      "Epoch 92/10000\n",
      "49/49 [==============================] - 0s 985us/step - loss: 0.5559 - accuracy: 0.8929 - val_loss: 1.9463 - val_accuracy: 0.7985\n",
      "Epoch 93/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.5478 - accuracy: 0.8960 - val_loss: 1.9412 - val_accuracy: 0.8036\n",
      "Epoch 94/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.5374 - accuracy: 0.8973 - val_loss: 1.9375 - val_accuracy: 0.8061\n",
      "Epoch 95/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.5276 - accuracy: 0.8999 - val_loss: 1.9204 - val_accuracy: 0.8036\n",
      "Epoch 96/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.5199 - accuracy: 0.8992 - val_loss: 1.9197 - val_accuracy: 0.8061\n",
      "Epoch 97/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.5073 - accuracy: 0.8992 - val_loss: 1.9180 - val_accuracy: 0.8061\n",
      "Epoch 98/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.4989 - accuracy: 0.8992 - val_loss: 1.9147 - val_accuracy: 0.8036\n",
      "Epoch 99/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.4917 - accuracy: 0.8999 - val_loss: 1.9051 - val_accuracy: 0.8138\n",
      "Epoch 100/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.4852 - accuracy: 0.9037 - val_loss: 1.9014 - val_accuracy: 0.8087\n",
      "Epoch 101/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.4768 - accuracy: 0.9056 - val_loss: 1.8984 - val_accuracy: 0.8087\n",
      "Epoch 102/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.4662 - accuracy: 0.9037 - val_loss: 1.8913 - val_accuracy: 0.8112\n",
      "Epoch 103/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.4599 - accuracy: 0.9062 - val_loss: 1.8838 - val_accuracy: 0.8087\n",
      "Epoch 104/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.4525 - accuracy: 0.9062 - val_loss: 1.8778 - val_accuracy: 0.8112\n",
      "Epoch 105/10000\n",
      "49/49 [==============================] - 0s 777us/step - loss: 0.4428 - accuracy: 0.9069 - val_loss: 1.8780 - val_accuracy: 0.8163\n",
      "Epoch 106/10000\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.4358 - accuracy: 0.9062 - val_loss: 1.8711 - val_accuracy: 0.8112\n",
      "Epoch 107/10000\n",
      "49/49 [==============================] - 0s 987us/step - loss: 0.4277 - accuracy: 0.9114 - val_loss: 1.8712 - val_accuracy: 0.8138\n",
      "Epoch 108/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.4201 - accuracy: 0.9101 - val_loss: 1.8675 - val_accuracy: 0.8138\n",
      "Epoch 109/10000\n",
      "49/49 [==============================] - 0s 965us/step - loss: 0.4121 - accuracy: 0.9139 - val_loss: 1.8515 - val_accuracy: 0.8240\n",
      "Epoch 110/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.4057 - accuracy: 0.9114 - val_loss: 1.8561 - val_accuracy: 0.8214\n",
      "Epoch 111/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3987 - accuracy: 0.9126 - val_loss: 1.8497 - val_accuracy: 0.8189\n",
      "Epoch 112/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3922 - accuracy: 0.9126 - val_loss: 1.8426 - val_accuracy: 0.8163\n",
      "Epoch 113/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.3876 - accuracy: 0.9114 - val_loss: 1.8439 - val_accuracy: 0.8163\n",
      "Epoch 114/10000\n",
      "49/49 [==============================] - 0s 960us/step - loss: 0.3800 - accuracy: 0.9126 - val_loss: 1.8419 - val_accuracy: 0.8189\n",
      "Epoch 115/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.3743 - accuracy: 0.9171 - val_loss: 1.8297 - val_accuracy: 0.8163\n",
      "Epoch 116/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.3684 - accuracy: 0.9158 - val_loss: 1.8402 - val_accuracy: 0.8214\n",
      "Epoch 117/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3620 - accuracy: 0.9184 - val_loss: 1.8268 - val_accuracy: 0.8240\n",
      "Epoch 118/10000\n",
      "49/49 [==============================] - 0s 807us/step - loss: 0.3591 - accuracy: 0.9184 - val_loss: 1.8324 - val_accuracy: 0.8214\n",
      "Epoch 119/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.3530 - accuracy: 0.9216 - val_loss: 1.8278 - val_accuracy: 0.8214\n",
      "Epoch 120/10000\n",
      "49/49 [==============================] - 0s 913us/step - loss: 0.3467 - accuracy: 0.9177 - val_loss: 1.8255 - val_accuracy: 0.8265\n",
      "Epoch 121/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3416 - accuracy: 0.9196 - val_loss: 1.8159 - val_accuracy: 0.8214\n",
      "Epoch 122/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3360 - accuracy: 0.9241 - val_loss: 1.8197 - val_accuracy: 0.8291\n",
      "Epoch 123/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3304 - accuracy: 0.9228 - val_loss: 1.8128 - val_accuracy: 0.8265\n",
      "Epoch 124/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3283 - accuracy: 0.9247 - val_loss: 1.8121 - val_accuracy: 0.8291\n",
      "Epoch 125/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3217 - accuracy: 0.9273 - val_loss: 1.8109 - val_accuracy: 0.8265\n",
      "Epoch 126/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3159 - accuracy: 0.9260 - val_loss: 1.8074 - val_accuracy: 0.8265\n",
      "Epoch 127/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.3134 - accuracy: 0.9254 - val_loss: 1.8090 - val_accuracy: 0.8316\n",
      "Epoch 128/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3074 - accuracy: 0.9260 - val_loss: 1.8079 - val_accuracy: 0.8291\n",
      "Epoch 129/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.3045 - accuracy: 0.9279 - val_loss: 1.8022 - val_accuracy: 0.8316\n",
      "Epoch 130/10000\n",
      "49/49 [==============================] - 0s 796us/step - loss: 0.3007 - accuracy: 0.9298 - val_loss: 1.8043 - val_accuracy: 0.8316\n",
      "Epoch 131/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2960 - accuracy: 0.9298 - val_loss: 1.8001 - val_accuracy: 0.8342\n",
      "Epoch 132/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.2924 - accuracy: 0.9298 - val_loss: 1.8006 - val_accuracy: 0.8342\n",
      "Epoch 133/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2863 - accuracy: 0.9298 - val_loss: 1.7986 - val_accuracy: 0.8291\n",
      "Epoch 134/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2848 - accuracy: 0.9362 - val_loss: 1.7969 - val_accuracy: 0.8291\n",
      "Epoch 135/10000\n",
      "49/49 [==============================] - 0s 893us/step - loss: 0.2810 - accuracy: 0.9318 - val_loss: 1.7956 - val_accuracy: 0.8342\n",
      "Epoch 136/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.2771 - accuracy: 0.9298 - val_loss: 1.7953 - val_accuracy: 0.8316\n",
      "Epoch 137/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2719 - accuracy: 0.9362 - val_loss: 1.7919 - val_accuracy: 0.8367\n",
      "Epoch 138/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2704 - accuracy: 0.9337 - val_loss: 1.7965 - val_accuracy: 0.8342\n",
      "Epoch 139/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.2674 - accuracy: 0.9343 - val_loss: 1.7913 - val_accuracy: 0.8444\n",
      "Epoch 140/10000\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.2621 - accuracy: 0.9349 - val_loss: 1.7951 - val_accuracy: 0.8342\n",
      "Epoch 141/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.2587 - accuracy: 0.9362 - val_loss: 1.7832 - val_accuracy: 0.8393\n",
      "Epoch 142/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2558 - accuracy: 0.9362 - val_loss: 1.7877 - val_accuracy: 0.8418\n",
      "Epoch 143/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2529 - accuracy: 0.9369 - val_loss: 1.7851 - val_accuracy: 0.8444\n",
      "Epoch 144/10000\n",
      "49/49 [==============================] - 0s 914us/step - loss: 0.2502 - accuracy: 0.9388 - val_loss: 1.7867 - val_accuracy: 0.8393\n",
      "Epoch 145/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.2450 - accuracy: 0.9388 - val_loss: 1.7775 - val_accuracy: 0.8393\n",
      "Epoch 146/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2420 - accuracy: 0.9407 - val_loss: 1.7831 - val_accuracy: 0.8418\n",
      "Epoch 147/10000\n",
      "49/49 [==============================] - 0s 989us/step - loss: 0.2423 - accuracy: 0.9413 - val_loss: 1.7847 - val_accuracy: 0.8444\n",
      "Epoch 148/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2388 - accuracy: 0.9394 - val_loss: 1.7802 - val_accuracy: 0.8418\n",
      "Epoch 149/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2355 - accuracy: 0.9426 - val_loss: 1.7767 - val_accuracy: 0.8418\n",
      "Epoch 150/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2334 - accuracy: 0.9407 - val_loss: 1.7774 - val_accuracy: 0.8444\n",
      "Epoch 151/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2301 - accuracy: 0.9394 - val_loss: 1.7715 - val_accuracy: 0.8444\n",
      "Epoch 152/10000\n",
      "49/49 [==============================] - 0s 913us/step - loss: 0.2265 - accuracy: 0.9432 - val_loss: 1.7745 - val_accuracy: 0.8444\n",
      "Epoch 153/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2247 - accuracy: 0.9458 - val_loss: 1.7795 - val_accuracy: 0.8418\n",
      "Epoch 154/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2203 - accuracy: 0.9452 - val_loss: 1.7720 - val_accuracy: 0.8418\n",
      "Epoch 155/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2184 - accuracy: 0.9439 - val_loss: 1.7717 - val_accuracy: 0.8444\n",
      "Epoch 156/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2162 - accuracy: 0.9445 - val_loss: 1.7745 - val_accuracy: 0.8469\n",
      "Epoch 157/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2116 - accuracy: 0.9439 - val_loss: 1.7727 - val_accuracy: 0.8495\n",
      "Epoch 158/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2106 - accuracy: 0.9509 - val_loss: 1.7697 - val_accuracy: 0.8469\n",
      "Epoch 159/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.2082 - accuracy: 0.9477 - val_loss: 1.7739 - val_accuracy: 0.8418\n",
      "Epoch 160/10000\n",
      "49/49 [==============================] - 0s 836us/step - loss: 0.2072 - accuracy: 0.9452 - val_loss: 1.7702 - val_accuracy: 0.8444\n",
      "Epoch 161/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.2043 - accuracy: 0.9483 - val_loss: 1.7687 - val_accuracy: 0.8418\n",
      "Epoch 162/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.2010 - accuracy: 0.9483 - val_loss: 1.7631 - val_accuracy: 0.8495\n",
      "Epoch 163/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1996 - accuracy: 0.9509 - val_loss: 1.7661 - val_accuracy: 0.8418\n",
      "Epoch 164/10000\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.1974 - accuracy: 0.9509 - val_loss: 1.7613 - val_accuracy: 0.8393\n",
      "Epoch 165/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1940 - accuracy: 0.9522 - val_loss: 1.7633 - val_accuracy: 0.8393\n",
      "Epoch 166/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1922 - accuracy: 0.9509 - val_loss: 1.7636 - val_accuracy: 0.8469\n",
      "Epoch 167/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1915 - accuracy: 0.9503 - val_loss: 1.7571 - val_accuracy: 0.8418\n",
      "Epoch 168/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1891 - accuracy: 0.9509 - val_loss: 1.7644 - val_accuracy: 0.8418\n",
      "Epoch 169/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1889 - accuracy: 0.9522 - val_loss: 1.7597 - val_accuracy: 0.8418\n",
      "Epoch 170/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1832 - accuracy: 0.9496 - val_loss: 1.7608 - val_accuracy: 0.8469\n",
      "Epoch 171/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1827 - accuracy: 0.9534 - val_loss: 1.7642 - val_accuracy: 0.8393\n",
      "Epoch 172/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1813 - accuracy: 0.9522 - val_loss: 1.7617 - val_accuracy: 0.8393\n",
      "Epoch 173/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1776 - accuracy: 0.9554 - val_loss: 1.7599 - val_accuracy: 0.8393\n",
      "Epoch 174/10000\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.1758 - accuracy: 0.9522 - val_loss: 1.7630 - val_accuracy: 0.8469\n",
      "Epoch 175/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.1739 - accuracy: 0.9541 - val_loss: 1.7669 - val_accuracy: 0.8342\n",
      "Epoch 176/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1711 - accuracy: 0.9534 - val_loss: 1.7651 - val_accuracy: 0.8367\n",
      "Epoch 177/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.1691 - accuracy: 0.9547 - val_loss: 1.7614 - val_accuracy: 0.8418\n",
      "訓練時間：12.42 秒\n",
      "498/498 [==============================] - 0s 750us/step - loss: 1.1431 - accuracy: 0.8502\n",
      "Evaluation on 90% unused data - Loss: 1.1431, Accuracy: 0.8502\n",
      "559/559 [==============================] - 0s 809us/step\n",
      "Test Data MDE report saved to: 40 data per RP transfer revised/DNN 0mcAPs BEST_40data_2024_12_21.json\n",
      "\n",
      "Test Data Mean MDE: 0.4092 meters\n",
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_104 (Dense)           (None, 64)                320       \n",
      "                                                                 \n",
      " dense_105 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_106 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_107 (Dense)           (None, 49)                3185      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20081 (78.44 KB)\n",
      "Trainable params: 3185 (12.44 KB)\n",
      "Non-trainable params: 16896 (66.00 KB)\n",
      "_________________________________________________________________\n",
      "   AP1_Rssi  AP2_Rssi  AP3_Rssi  AP4_Rssi\n",
      "0     -54.0     -72.0     -62.0     -45.0\n",
      "1     -62.0     -72.0     -64.0     -43.0\n",
      "2     -57.0     -72.0     -62.0     -45.0\n",
      "3     -62.0     -71.0     -62.0     -47.0\n",
      "4     -62.0     -72.0     -64.0     -47.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcslab/anaconda3/envs/yang_cuda3/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n",
      "/tmp/ipykernel_3268413/3821025327.py:45: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  test_processed_data = pd.concat([test_processed_data, group], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Set</th>\n",
       "      <th>Validation Set</th>\n",
       "      <th>Test Set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Training Set  Validation Set  Test Set\n",
       "0             32               8       376\n",
       "1             32               8       376\n",
       "2             32               8       376\n",
       "3             32               8       376\n",
       "4             32               8       376\n",
       "5             32               8       376\n",
       "6             32               8       376\n",
       "7             32               8       376\n",
       "8             32               8       376\n",
       "9             32               8       376\n",
       "10            32               8       376\n",
       "11            32               8       376\n",
       "12            32               8       376\n",
       "13            32               8       376\n",
       "14            32               8       376\n",
       "15            32               8       376\n",
       "16            32               8       376\n",
       "17            32               8       376\n",
       "18            32               8       376\n",
       "19            32               8       376\n",
       "20            32               8       376\n",
       "21            32               8       376\n",
       "22            32               8       376\n",
       "23            32               8       376\n",
       "24            32               8       376\n",
       "25            32               8       376\n",
       "26            32               8       376\n",
       "27            32               8       376\n",
       "28            32               8       376\n",
       "29            32               8       376\n",
       "30            32               8       376\n",
       "31            32               8       376\n",
       "32            32               8       376\n",
       "33            32               8       376\n",
       "34            32               8       376\n",
       "35            32               8       376\n",
       "36            32               8       376\n",
       "37            32               8       376\n",
       "38            32               8       376\n",
       "39            32               8       376\n",
       "40            32               8       376\n",
       "41            32               8       376\n",
       "42            32               8       376\n",
       "43            32               8       376\n",
       "44            32               8       376\n",
       "45            32               8       376\n",
       "46            32               8       376\n",
       "47            32               8       376\n",
       "48            32               8       376"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "Epoch 1/10000\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 27.1683 - accuracy: 0.1269 - val_loss: 23.7027 - val_accuracy: 0.1684\n",
      "Epoch 2/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 24.0210 - accuracy: 0.1492 - val_loss: 21.0218 - val_accuracy: 0.1990\n",
      "Epoch 3/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 21.4049 - accuracy: 0.1837 - val_loss: 18.7489 - val_accuracy: 0.2219\n",
      "Epoch 4/10000\n",
      "49/49 [==============================] - 0s 799us/step - loss: 19.1983 - accuracy: 0.2117 - val_loss: 16.8351 - val_accuracy: 0.2577\n",
      "Epoch 5/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 17.3389 - accuracy: 0.2506 - val_loss: 15.1719 - val_accuracy: 0.2806\n",
      "Epoch 6/10000\n",
      "49/49 [==============================] - 0s 898us/step - loss: 15.7395 - accuracy: 0.2832 - val_loss: 13.7364 - val_accuracy: 0.3240\n",
      "Epoch 7/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 14.3282 - accuracy: 0.3182 - val_loss: 12.5106 - val_accuracy: 0.3622\n",
      "Epoch 8/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 13.1141 - accuracy: 0.3610 - val_loss: 11.4390 - val_accuracy: 0.4133\n",
      "Epoch 9/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 12.0470 - accuracy: 0.3897 - val_loss: 10.5206 - val_accuracy: 0.4260\n",
      "Epoch 10/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 11.0986 - accuracy: 0.4126 - val_loss: 9.6543 - val_accuracy: 0.4413\n",
      "Epoch 11/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 10.2400 - accuracy: 0.4318 - val_loss: 8.8606 - val_accuracy: 0.4566\n",
      "Epoch 12/10000\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 9.4331 - accuracy: 0.4534 - val_loss: 8.1530 - val_accuracy: 0.4872\n",
      "Epoch 13/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 8.7013 - accuracy: 0.4739 - val_loss: 7.5509 - val_accuracy: 0.5153\n",
      "Epoch 14/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 8.0562 - accuracy: 0.4974 - val_loss: 6.9906 - val_accuracy: 0.5561\n",
      "Epoch 15/10000\n",
      "49/49 [==============================] - 0s 706us/step - loss: 7.4554 - accuracy: 0.5217 - val_loss: 6.5054 - val_accuracy: 0.5765\n",
      "Epoch 16/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 6.9211 - accuracy: 0.5421 - val_loss: 6.0573 - val_accuracy: 0.6020\n",
      "Epoch 17/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 6.4322 - accuracy: 0.5561 - val_loss: 5.6325 - val_accuracy: 0.6173\n",
      "Epoch 18/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 5.9711 - accuracy: 0.5708 - val_loss: 5.2494 - val_accuracy: 0.6250\n",
      "Epoch 19/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 5.5339 - accuracy: 0.5855 - val_loss: 4.8793 - val_accuracy: 0.6352\n",
      "Epoch 20/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 5.1354 - accuracy: 0.5976 - val_loss: 4.5563 - val_accuracy: 0.6454\n",
      "Epoch 21/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 4.7621 - accuracy: 0.6071 - val_loss: 4.2486 - val_accuracy: 0.6633\n",
      "Epoch 22/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 4.4319 - accuracy: 0.6237 - val_loss: 3.9958 - val_accuracy: 0.6684\n",
      "Epoch 23/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 4.1484 - accuracy: 0.6397 - val_loss: 3.7737 - val_accuracy: 0.6888\n",
      "Epoch 24/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 3.9041 - accuracy: 0.6582 - val_loss: 3.5964 - val_accuracy: 0.6939\n",
      "Epoch 25/10000\n",
      "49/49 [==============================] - 0s 765us/step - loss: 3.6927 - accuracy: 0.6754 - val_loss: 3.4462 - val_accuracy: 0.7066\n",
      "Epoch 26/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 3.5152 - accuracy: 0.6907 - val_loss: 3.3158 - val_accuracy: 0.7245\n",
      "Epoch 27/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 3.3590 - accuracy: 0.7034 - val_loss: 3.2074 - val_accuracy: 0.7296\n",
      "Epoch 28/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 3.2290 - accuracy: 0.7111 - val_loss: 3.1178 - val_accuracy: 0.7372\n",
      "Epoch 29/10000\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 3.1042 - accuracy: 0.7226 - val_loss: 3.0214 - val_accuracy: 0.7398\n",
      "Epoch 30/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 2.9957 - accuracy: 0.7315 - val_loss: 2.9542 - val_accuracy: 0.7474\n",
      "Epoch 31/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 2.8938 - accuracy: 0.7404 - val_loss: 2.8885 - val_accuracy: 0.7551\n",
      "Epoch 32/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 2.7970 - accuracy: 0.7487 - val_loss: 2.8252 - val_accuracy: 0.7628\n",
      "Epoch 33/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 2.7106 - accuracy: 0.7455 - val_loss: 2.7734 - val_accuracy: 0.7653\n",
      "Epoch 34/10000\n",
      "49/49 [==============================] - 0s 912us/step - loss: 2.6344 - accuracy: 0.7462 - val_loss: 2.7137 - val_accuracy: 0.7704\n",
      "Epoch 35/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 2.5420 - accuracy: 0.7577 - val_loss: 2.6594 - val_accuracy: 0.7730\n",
      "Epoch 36/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 2.4686 - accuracy: 0.7583 - val_loss: 2.6108 - val_accuracy: 0.7857\n",
      "Epoch 37/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 2.3959 - accuracy: 0.7691 - val_loss: 2.5750 - val_accuracy: 0.7832\n",
      "Epoch 38/10000\n",
      "49/49 [==============================] - 0s 803us/step - loss: 2.3303 - accuracy: 0.7723 - val_loss: 2.5335 - val_accuracy: 0.7857\n",
      "Epoch 39/10000\n",
      "49/49 [==============================] - 0s 721us/step - loss: 2.2584 - accuracy: 0.7838 - val_loss: 2.4881 - val_accuracy: 0.7832\n",
      "Epoch 40/10000\n",
      "49/49 [==============================] - 0s 690us/step - loss: 2.1963 - accuracy: 0.7787 - val_loss: 2.4487 - val_accuracy: 0.7883\n",
      "Epoch 41/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 2.1362 - accuracy: 0.7883 - val_loss: 2.4004 - val_accuracy: 0.7908\n",
      "Epoch 42/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 2.0774 - accuracy: 0.7870 - val_loss: 2.3741 - val_accuracy: 0.7908\n",
      "Epoch 43/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 2.0246 - accuracy: 0.7902 - val_loss: 2.3406 - val_accuracy: 0.7857\n",
      "Epoch 44/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.9679 - accuracy: 0.7966 - val_loss: 2.3059 - val_accuracy: 0.7908\n",
      "Epoch 45/10000\n",
      "49/49 [==============================] - 0s 715us/step - loss: 1.9199 - accuracy: 0.8029 - val_loss: 2.2639 - val_accuracy: 0.7857\n",
      "Epoch 46/10000\n",
      "49/49 [==============================] - 0s 756us/step - loss: 1.8694 - accuracy: 0.7966 - val_loss: 2.2431 - val_accuracy: 0.7908\n",
      "Epoch 47/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.8245 - accuracy: 0.7985 - val_loss: 2.2021 - val_accuracy: 0.7908\n",
      "Epoch 48/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.7818 - accuracy: 0.8080 - val_loss: 2.1731 - val_accuracy: 0.7883\n",
      "Epoch 49/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.7332 - accuracy: 0.8055 - val_loss: 2.1404 - val_accuracy: 0.7908\n",
      "Epoch 50/10000\n",
      "49/49 [==============================] - 0s 923us/step - loss: 1.6921 - accuracy: 0.8106 - val_loss: 2.1128 - val_accuracy: 0.7908\n",
      "Epoch 51/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.6515 - accuracy: 0.8061 - val_loss: 2.0883 - val_accuracy: 0.7985\n",
      "Epoch 52/10000\n",
      "49/49 [==============================] - 0s 888us/step - loss: 1.6065 - accuracy: 0.8144 - val_loss: 2.0614 - val_accuracy: 0.7832\n",
      "Epoch 53/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.5706 - accuracy: 0.8144 - val_loss: 2.0219 - val_accuracy: 0.7959\n",
      "Epoch 54/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.5342 - accuracy: 0.8208 - val_loss: 2.0033 - val_accuracy: 0.7908\n",
      "Epoch 55/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.4995 - accuracy: 0.8138 - val_loss: 1.9789 - val_accuracy: 0.7832\n",
      "Epoch 56/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.4636 - accuracy: 0.8246 - val_loss: 1.9595 - val_accuracy: 0.7908\n",
      "Epoch 57/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.4259 - accuracy: 0.8214 - val_loss: 1.9280 - val_accuracy: 0.7959\n",
      "Epoch 58/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.3941 - accuracy: 0.8246 - val_loss: 1.9114 - val_accuracy: 0.7857\n",
      "Epoch 59/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.3614 - accuracy: 0.8348 - val_loss: 1.8901 - val_accuracy: 0.7883\n",
      "Epoch 60/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.3321 - accuracy: 0.8323 - val_loss: 1.8535 - val_accuracy: 0.7934\n",
      "Epoch 61/10000\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 1.3066 - accuracy: 0.8335 - val_loss: 1.8470 - val_accuracy: 0.7985\n",
      "Epoch 62/10000\n",
      "49/49 [==============================] - 0s 891us/step - loss: 1.2680 - accuracy: 0.8380 - val_loss: 1.8154 - val_accuracy: 0.7985\n",
      "Epoch 63/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.2475 - accuracy: 0.8367 - val_loss: 1.8107 - val_accuracy: 0.8010\n",
      "Epoch 64/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.2201 - accuracy: 0.8444 - val_loss: 1.7822 - val_accuracy: 0.8087\n",
      "Epoch 65/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.1908 - accuracy: 0.8418 - val_loss: 1.7616 - val_accuracy: 0.8010\n",
      "Epoch 66/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.1604 - accuracy: 0.8399 - val_loss: 1.7380 - val_accuracy: 0.8036\n",
      "Epoch 67/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.1341 - accuracy: 0.8406 - val_loss: 1.7282 - val_accuracy: 0.8010\n",
      "Epoch 68/10000\n",
      "49/49 [==============================] - 0s 975us/step - loss: 1.1080 - accuracy: 0.8457 - val_loss: 1.7096 - val_accuracy: 0.8087\n",
      "Epoch 69/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.0823 - accuracy: 0.8476 - val_loss: 1.6999 - val_accuracy: 0.8087\n",
      "Epoch 70/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.0542 - accuracy: 0.8514 - val_loss: 1.6857 - val_accuracy: 0.8061\n",
      "Epoch 71/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.0327 - accuracy: 0.8476 - val_loss: 1.6654 - val_accuracy: 0.8010\n",
      "Epoch 72/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.0108 - accuracy: 0.8457 - val_loss: 1.6414 - val_accuracy: 0.8061\n",
      "Epoch 73/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.9846 - accuracy: 0.8546 - val_loss: 1.6305 - val_accuracy: 0.8087\n",
      "Epoch 74/10000\n",
      "49/49 [==============================] - 0s 851us/step - loss: 0.9627 - accuracy: 0.8571 - val_loss: 1.6167 - val_accuracy: 0.8087\n",
      "Epoch 75/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.9388 - accuracy: 0.8584 - val_loss: 1.5966 - val_accuracy: 0.8163\n",
      "Epoch 76/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.9151 - accuracy: 0.8610 - val_loss: 1.5897 - val_accuracy: 0.8112\n",
      "Epoch 77/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.8941 - accuracy: 0.8616 - val_loss: 1.5715 - val_accuracy: 0.8112\n",
      "Epoch 78/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.8742 - accuracy: 0.8635 - val_loss: 1.5660 - val_accuracy: 0.8112\n",
      "Epoch 79/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.8597 - accuracy: 0.8584 - val_loss: 1.5473 - val_accuracy: 0.8138\n",
      "Epoch 80/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.8337 - accuracy: 0.8699 - val_loss: 1.5405 - val_accuracy: 0.8163\n",
      "Epoch 81/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.8198 - accuracy: 0.8731 - val_loss: 1.5300 - val_accuracy: 0.8189\n",
      "Epoch 82/10000\n",
      "49/49 [==============================] - 0s 709us/step - loss: 0.7967 - accuracy: 0.8705 - val_loss: 1.5136 - val_accuracy: 0.8163\n",
      "Epoch 83/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.7806 - accuracy: 0.8756 - val_loss: 1.5017 - val_accuracy: 0.8138\n",
      "Epoch 84/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.7657 - accuracy: 0.8737 - val_loss: 1.5049 - val_accuracy: 0.8163\n",
      "Epoch 85/10000\n",
      "49/49 [==============================] - 0s 742us/step - loss: 0.7501 - accuracy: 0.8756 - val_loss: 1.4889 - val_accuracy: 0.8189\n",
      "Epoch 86/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.7342 - accuracy: 0.8750 - val_loss: 1.4810 - val_accuracy: 0.8163\n",
      "Epoch 87/10000\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.7171 - accuracy: 0.8756 - val_loss: 1.4664 - val_accuracy: 0.8189\n",
      "Epoch 88/10000\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.7107 - accuracy: 0.8763 - val_loss: 1.4568 - val_accuracy: 0.8189\n",
      "Epoch 89/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.6904 - accuracy: 0.8801 - val_loss: 1.4491 - val_accuracy: 0.8163\n",
      "Epoch 90/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.6738 - accuracy: 0.8820 - val_loss: 1.4321 - val_accuracy: 0.8138\n",
      "Epoch 91/10000\n",
      "49/49 [==============================] - 0s 765us/step - loss: 0.6607 - accuracy: 0.8807 - val_loss: 1.4299 - val_accuracy: 0.8189\n",
      "Epoch 92/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.6478 - accuracy: 0.8795 - val_loss: 1.4181 - val_accuracy: 0.8214\n",
      "Epoch 93/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.6338 - accuracy: 0.8833 - val_loss: 1.4138 - val_accuracy: 0.8189\n",
      "Epoch 94/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.6197 - accuracy: 0.8884 - val_loss: 1.3987 - val_accuracy: 0.8189\n",
      "Epoch 95/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.6077 - accuracy: 0.8884 - val_loss: 1.3867 - val_accuracy: 0.8240\n",
      "Epoch 96/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.5971 - accuracy: 0.8846 - val_loss: 1.3918 - val_accuracy: 0.8189\n",
      "Epoch 97/10000\n",
      "49/49 [==============================] - 0s 787us/step - loss: 0.5826 - accuracy: 0.8967 - val_loss: 1.3754 - val_accuracy: 0.8214\n",
      "Epoch 98/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.5736 - accuracy: 0.8973 - val_loss: 1.3721 - val_accuracy: 0.8189\n",
      "Epoch 99/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.5607 - accuracy: 0.8980 - val_loss: 1.3614 - val_accuracy: 0.8214\n",
      "Epoch 100/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.5492 - accuracy: 0.9011 - val_loss: 1.3580 - val_accuracy: 0.8214\n",
      "Epoch 101/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.5370 - accuracy: 0.8967 - val_loss: 1.3369 - val_accuracy: 0.8138\n",
      "Epoch 102/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.5288 - accuracy: 0.8973 - val_loss: 1.3288 - val_accuracy: 0.8189\n",
      "Epoch 103/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.5170 - accuracy: 0.8999 - val_loss: 1.3360 - val_accuracy: 0.8189\n",
      "Epoch 104/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.5099 - accuracy: 0.8980 - val_loss: 1.3197 - val_accuracy: 0.8240\n",
      "Epoch 105/10000\n",
      "49/49 [==============================] - 0s 894us/step - loss: 0.4997 - accuracy: 0.8986 - val_loss: 1.3174 - val_accuracy: 0.8214\n",
      "Epoch 106/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.4913 - accuracy: 0.9018 - val_loss: 1.3114 - val_accuracy: 0.8240\n",
      "Epoch 107/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.4805 - accuracy: 0.9024 - val_loss: 1.3054 - val_accuracy: 0.8189\n",
      "Epoch 108/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.4700 - accuracy: 0.9050 - val_loss: 1.2962 - val_accuracy: 0.8291\n",
      "Epoch 109/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.4624 - accuracy: 0.9062 - val_loss: 1.2861 - val_accuracy: 0.8240\n",
      "Epoch 110/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.4524 - accuracy: 0.9050 - val_loss: 1.2797 - val_accuracy: 0.8240\n",
      "Epoch 111/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.4410 - accuracy: 0.9056 - val_loss: 1.2671 - val_accuracy: 0.8214\n",
      "Epoch 112/10000\n",
      "49/49 [==============================] - 0s 849us/step - loss: 0.4361 - accuracy: 0.9069 - val_loss: 1.2687 - val_accuracy: 0.8240\n",
      "Epoch 113/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.4259 - accuracy: 0.9075 - val_loss: 1.2522 - val_accuracy: 0.8265\n",
      "Epoch 114/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.4187 - accuracy: 0.9088 - val_loss: 1.2501 - val_accuracy: 0.8240\n",
      "Epoch 115/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.4111 - accuracy: 0.9101 - val_loss: 1.2416 - val_accuracy: 0.8316\n",
      "Epoch 116/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.4036 - accuracy: 0.9126 - val_loss: 1.2402 - val_accuracy: 0.8291\n",
      "Epoch 117/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3948 - accuracy: 0.9171 - val_loss: 1.2203 - val_accuracy: 0.8265\n",
      "Epoch 118/10000\n",
      "49/49 [==============================] - 0s 967us/step - loss: 0.3870 - accuracy: 0.9152 - val_loss: 1.2258 - val_accuracy: 0.8265\n",
      "Epoch 119/10000\n",
      "49/49 [==============================] - 0s 935us/step - loss: 0.3848 - accuracy: 0.9145 - val_loss: 1.2122 - val_accuracy: 0.8342\n",
      "Epoch 120/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.3723 - accuracy: 0.9190 - val_loss: 1.2101 - val_accuracy: 0.8367\n",
      "Epoch 121/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3687 - accuracy: 0.9177 - val_loss: 1.2091 - val_accuracy: 0.8316\n",
      "Epoch 122/10000\n",
      "49/49 [==============================] - 0s 947us/step - loss: 0.3650 - accuracy: 0.9139 - val_loss: 1.2038 - val_accuracy: 0.8367\n",
      "Epoch 123/10000\n",
      "49/49 [==============================] - 0s 680us/step - loss: 0.3548 - accuracy: 0.9216 - val_loss: 1.1933 - val_accuracy: 0.8393\n",
      "Epoch 124/10000\n",
      "49/49 [==============================] - 0s 854us/step - loss: 0.3501 - accuracy: 0.9177 - val_loss: 1.1840 - val_accuracy: 0.8393\n",
      "Epoch 125/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.3437 - accuracy: 0.9209 - val_loss: 1.1810 - val_accuracy: 0.8418\n",
      "Epoch 126/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.3403 - accuracy: 0.9241 - val_loss: 1.1727 - val_accuracy: 0.8444\n",
      "Epoch 127/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3325 - accuracy: 0.9254 - val_loss: 1.1659 - val_accuracy: 0.8444\n",
      "Epoch 128/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.3246 - accuracy: 0.9241 - val_loss: 1.1606 - val_accuracy: 0.8418\n",
      "Epoch 129/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3192 - accuracy: 0.9241 - val_loss: 1.1556 - val_accuracy: 0.8469\n",
      "Epoch 130/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3161 - accuracy: 0.9292 - val_loss: 1.1495 - val_accuracy: 0.8444\n",
      "Epoch 131/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3135 - accuracy: 0.9298 - val_loss: 1.1397 - val_accuracy: 0.8418\n",
      "Epoch 132/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3036 - accuracy: 0.9305 - val_loss: 1.1365 - val_accuracy: 0.8469\n",
      "Epoch 133/10000\n",
      "49/49 [==============================] - 0s 959us/step - loss: 0.2989 - accuracy: 0.9298 - val_loss: 1.1408 - val_accuracy: 0.8495\n",
      "Epoch 134/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2971 - accuracy: 0.9260 - val_loss: 1.1312 - val_accuracy: 0.8469\n",
      "Epoch 135/10000\n",
      "49/49 [==============================] - 0s 715us/step - loss: 0.2899 - accuracy: 0.9324 - val_loss: 1.1224 - val_accuracy: 0.8469\n",
      "Epoch 136/10000\n",
      "49/49 [==============================] - 0s 815us/step - loss: 0.2893 - accuracy: 0.9286 - val_loss: 1.1227 - val_accuracy: 0.8469\n",
      "Epoch 137/10000\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.2848 - accuracy: 0.9311 - val_loss: 1.1202 - val_accuracy: 0.8393\n",
      "Epoch 138/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.2778 - accuracy: 0.9356 - val_loss: 1.1103 - val_accuracy: 0.8495\n",
      "Epoch 139/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.2715 - accuracy: 0.9369 - val_loss: 1.0974 - val_accuracy: 0.8469\n",
      "Epoch 140/10000\n",
      "49/49 [==============================] - 0s 964us/step - loss: 0.2705 - accuracy: 0.9369 - val_loss: 1.0965 - val_accuracy: 0.8444\n",
      "Epoch 141/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2654 - accuracy: 0.9388 - val_loss: 1.0990 - val_accuracy: 0.8444\n",
      "Epoch 142/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.2617 - accuracy: 0.9369 - val_loss: 1.0952 - val_accuracy: 0.8469\n",
      "Epoch 143/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2590 - accuracy: 0.9349 - val_loss: 1.0842 - val_accuracy: 0.8495\n",
      "Epoch 144/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.2559 - accuracy: 0.9330 - val_loss: 1.0807 - val_accuracy: 0.8444\n",
      "Epoch 145/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.2498 - accuracy: 0.9381 - val_loss: 1.0809 - val_accuracy: 0.8469\n",
      "Epoch 146/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2459 - accuracy: 0.9407 - val_loss: 1.0739 - val_accuracy: 0.8444\n",
      "Epoch 147/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.2424 - accuracy: 0.9413 - val_loss: 1.0648 - val_accuracy: 0.8520\n",
      "Epoch 148/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.2402 - accuracy: 0.9394 - val_loss: 1.0572 - val_accuracy: 0.8520\n",
      "Epoch 149/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2363 - accuracy: 0.9375 - val_loss: 1.0595 - val_accuracy: 0.8495\n",
      "Epoch 150/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2349 - accuracy: 0.9445 - val_loss: 1.0559 - val_accuracy: 0.8469\n",
      "Epoch 151/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2297 - accuracy: 0.9445 - val_loss: 1.0590 - val_accuracy: 0.8495\n",
      "Epoch 152/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2272 - accuracy: 0.9413 - val_loss: 1.0443 - val_accuracy: 0.8520\n",
      "Epoch 153/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2236 - accuracy: 0.9401 - val_loss: 1.0452 - val_accuracy: 0.8546\n",
      "Epoch 154/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2204 - accuracy: 0.9452 - val_loss: 1.0395 - val_accuracy: 0.8520\n",
      "Epoch 155/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.2175 - accuracy: 0.9432 - val_loss: 1.0335 - val_accuracy: 0.8495\n",
      "Epoch 156/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2163 - accuracy: 0.9432 - val_loss: 1.0381 - val_accuracy: 0.8520\n",
      "Epoch 157/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.2125 - accuracy: 0.9445 - val_loss: 1.0368 - val_accuracy: 0.8520\n",
      "Epoch 158/10000\n",
      "49/49 [==============================] - 0s 885us/step - loss: 0.2088 - accuracy: 0.9477 - val_loss: 1.0245 - val_accuracy: 0.8546\n",
      "Epoch 159/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2089 - accuracy: 0.9445 - val_loss: 1.0292 - val_accuracy: 0.8546\n",
      "Epoch 160/10000\n",
      "49/49 [==============================] - 0s 854us/step - loss: 0.2038 - accuracy: 0.9483 - val_loss: 1.0273 - val_accuracy: 0.8520\n",
      "Epoch 161/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2040 - accuracy: 0.9464 - val_loss: 1.0203 - val_accuracy: 0.8571\n",
      "Epoch 162/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1993 - accuracy: 0.9426 - val_loss: 1.0156 - val_accuracy: 0.8546\n",
      "Epoch 163/10000\n",
      "49/49 [==============================] - 0s 991us/step - loss: 0.1983 - accuracy: 0.9471 - val_loss: 1.0191 - val_accuracy: 0.8546\n",
      "Epoch 164/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.1950 - accuracy: 0.9464 - val_loss: 1.0096 - val_accuracy: 0.8520\n",
      "Epoch 165/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.1928 - accuracy: 0.9477 - val_loss: 1.0146 - val_accuracy: 0.8546\n",
      "Epoch 166/10000\n",
      "49/49 [==============================] - 0s 1000us/step - loss: 0.1887 - accuracy: 0.9509 - val_loss: 0.9950 - val_accuracy: 0.8546\n",
      "Epoch 167/10000\n",
      "49/49 [==============================] - 0s 696us/step - loss: 0.1852 - accuracy: 0.9503 - val_loss: 1.0062 - val_accuracy: 0.8520\n",
      "Epoch 168/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.1829 - accuracy: 0.9490 - val_loss: 1.0021 - val_accuracy: 0.8622\n",
      "Epoch 169/10000\n",
      "49/49 [==============================] - 0s 941us/step - loss: 0.1826 - accuracy: 0.9528 - val_loss: 0.9969 - val_accuracy: 0.8597\n",
      "Epoch 170/10000\n",
      "49/49 [==============================] - 0s 748us/step - loss: 0.1788 - accuracy: 0.9515 - val_loss: 1.0002 - val_accuracy: 0.8546\n",
      "Epoch 171/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1767 - accuracy: 0.9522 - val_loss: 0.9933 - val_accuracy: 0.8648\n",
      "Epoch 172/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.1759 - accuracy: 0.9509 - val_loss: 0.9931 - val_accuracy: 0.8520\n",
      "Epoch 173/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1776 - accuracy: 0.9509 - val_loss: 1.0016 - val_accuracy: 0.8546\n",
      "Epoch 174/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1733 - accuracy: 0.9534 - val_loss: 0.9982 - val_accuracy: 0.8546\n",
      "Epoch 175/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1674 - accuracy: 0.9522 - val_loss: 0.9865 - val_accuracy: 0.8597\n",
      "Epoch 176/10000\n",
      "49/49 [==============================] - 0s 976us/step - loss: 0.1676 - accuracy: 0.9534 - val_loss: 0.9930 - val_accuracy: 0.8546\n",
      "Epoch 177/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.1650 - accuracy: 0.9547 - val_loss: 0.9810 - val_accuracy: 0.8622\n",
      "Epoch 178/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1609 - accuracy: 0.9554 - val_loss: 0.9877 - val_accuracy: 0.8571\n",
      "Epoch 179/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1618 - accuracy: 0.9509 - val_loss: 0.9807 - val_accuracy: 0.8546\n",
      "Epoch 180/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.1623 - accuracy: 0.9541 - val_loss: 0.9798 - val_accuracy: 0.8648\n",
      "Epoch 181/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.1569 - accuracy: 0.9490 - val_loss: 0.9786 - val_accuracy: 0.8546\n",
      "Epoch 182/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.1541 - accuracy: 0.9554 - val_loss: 0.9759 - val_accuracy: 0.8546\n",
      "Epoch 183/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1519 - accuracy: 0.9560 - val_loss: 0.9752 - val_accuracy: 0.8597\n",
      "Epoch 184/10000\n",
      "49/49 [==============================] - 0s 776us/step - loss: 0.1521 - accuracy: 0.9547 - val_loss: 0.9764 - val_accuracy: 0.8546\n",
      "Epoch 185/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.1497 - accuracy: 0.9554 - val_loss: 0.9843 - val_accuracy: 0.8495\n",
      "Epoch 186/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.1516 - accuracy: 0.9547 - val_loss: 0.9735 - val_accuracy: 0.8597\n",
      "Epoch 187/10000\n",
      "49/49 [==============================] - 0s 738us/step - loss: 0.1494 - accuracy: 0.9579 - val_loss: 0.9778 - val_accuracy: 0.8597\n",
      "Epoch 188/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1445 - accuracy: 0.9534 - val_loss: 0.9751 - val_accuracy: 0.8571\n",
      "Epoch 189/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.1426 - accuracy: 0.9560 - val_loss: 0.9769 - val_accuracy: 0.8622\n",
      "Epoch 190/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1420 - accuracy: 0.9554 - val_loss: 0.9662 - val_accuracy: 0.8622\n",
      "Epoch 191/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1402 - accuracy: 0.9598 - val_loss: 0.9706 - val_accuracy: 0.8520\n",
      "Epoch 192/10000\n",
      "49/49 [==============================] - 0s 832us/step - loss: 0.1406 - accuracy: 0.9579 - val_loss: 0.9760 - val_accuracy: 0.8571\n",
      "Epoch 193/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1401 - accuracy: 0.9579 - val_loss: 0.9646 - val_accuracy: 0.8571\n",
      "Epoch 194/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1339 - accuracy: 0.9579 - val_loss: 0.9602 - val_accuracy: 0.8546\n",
      "Epoch 195/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1347 - accuracy: 0.9605 - val_loss: 0.9670 - val_accuracy: 0.8622\n",
      "Epoch 196/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1317 - accuracy: 0.9592 - val_loss: 0.9642 - val_accuracy: 0.8597\n",
      "Epoch 197/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.1322 - accuracy: 0.9585 - val_loss: 0.9669 - val_accuracy: 0.8597\n",
      "Epoch 198/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1295 - accuracy: 0.9611 - val_loss: 0.9612 - val_accuracy: 0.8622\n",
      "Epoch 199/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.1268 - accuracy: 0.9592 - val_loss: 0.9630 - val_accuracy: 0.8597\n",
      "Epoch 200/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1302 - accuracy: 0.9611 - val_loss: 0.9678 - val_accuracy: 0.8571\n",
      "Epoch 201/10000\n",
      "49/49 [==============================] - 0s 981us/step - loss: 0.1275 - accuracy: 0.9605 - val_loss: 0.9633 - val_accuracy: 0.8597\n",
      "Epoch 202/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1246 - accuracy: 0.9617 - val_loss: 0.9574 - val_accuracy: 0.8622\n",
      "Epoch 203/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.1249 - accuracy: 0.9624 - val_loss: 0.9606 - val_accuracy: 0.8648\n",
      "Epoch 204/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1223 - accuracy: 0.9598 - val_loss: 0.9579 - val_accuracy: 0.8622\n",
      "Epoch 205/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1203 - accuracy: 0.9649 - val_loss: 0.9669 - val_accuracy: 0.8622\n",
      "Epoch 206/10000\n",
      "49/49 [==============================] - 0s 984us/step - loss: 0.1180 - accuracy: 0.9630 - val_loss: 0.9628 - val_accuracy: 0.8597\n",
      "Epoch 207/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1170 - accuracy: 0.9630 - val_loss: 0.9639 - val_accuracy: 0.8597\n",
      "Epoch 208/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.1173 - accuracy: 0.9617 - val_loss: 0.9541 - val_accuracy: 0.8648\n",
      "Epoch 209/10000\n",
      "49/49 [==============================] - 0s 877us/step - loss: 0.1171 - accuracy: 0.9630 - val_loss: 0.9560 - val_accuracy: 0.8648\n",
      "Epoch 210/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.1150 - accuracy: 0.9624 - val_loss: 0.9448 - val_accuracy: 0.8597\n",
      "Epoch 211/10000\n",
      "49/49 [==============================] - 0s 821us/step - loss: 0.1136 - accuracy: 0.9681 - val_loss: 0.9548 - val_accuracy: 0.8724\n",
      "Epoch 212/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1120 - accuracy: 0.9668 - val_loss: 0.9457 - val_accuracy: 0.8673\n",
      "Epoch 213/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1124 - accuracy: 0.9643 - val_loss: 0.9486 - val_accuracy: 0.8622\n",
      "Epoch 214/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1105 - accuracy: 0.9643 - val_loss: 0.9449 - val_accuracy: 0.8648\n",
      "Epoch 215/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.1070 - accuracy: 0.9662 - val_loss: 0.9523 - val_accuracy: 0.8622\n",
      "Epoch 216/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1085 - accuracy: 0.9675 - val_loss: 0.9416 - val_accuracy: 0.8622\n",
      "Epoch 217/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1096 - accuracy: 0.9624 - val_loss: 0.9502 - val_accuracy: 0.8622\n",
      "Epoch 218/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1067 - accuracy: 0.9668 - val_loss: 0.9430 - val_accuracy: 0.8648\n",
      "Epoch 219/10000\n",
      "49/49 [==============================] - 0s 924us/step - loss: 0.1051 - accuracy: 0.9681 - val_loss: 0.9430 - val_accuracy: 0.8622\n",
      "Epoch 220/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.1020 - accuracy: 0.9649 - val_loss: 0.9418 - val_accuracy: 0.8622\n",
      "Epoch 221/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1042 - accuracy: 0.9643 - val_loss: 0.9386 - val_accuracy: 0.8673\n",
      "Epoch 222/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.1022 - accuracy: 0.9675 - val_loss: 0.9400 - val_accuracy: 0.8648\n",
      "Epoch 223/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0995 - accuracy: 0.9662 - val_loss: 0.9380 - val_accuracy: 0.8648\n",
      "Epoch 224/10000\n",
      "49/49 [==============================] - 0s 904us/step - loss: 0.1006 - accuracy: 0.9668 - val_loss: 0.9373 - val_accuracy: 0.8699\n",
      "Epoch 225/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0973 - accuracy: 0.9675 - val_loss: 0.9352 - val_accuracy: 0.8724\n",
      "Epoch 226/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0966 - accuracy: 0.9656 - val_loss: 0.9478 - val_accuracy: 0.8673\n",
      "Epoch 227/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0974 - accuracy: 0.9675 - val_loss: 0.9332 - val_accuracy: 0.8699\n",
      "Epoch 228/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0962 - accuracy: 0.9700 - val_loss: 0.9326 - val_accuracy: 0.8648\n",
      "Epoch 229/10000\n",
      "49/49 [==============================] - 0s 830us/step - loss: 0.0952 - accuracy: 0.9662 - val_loss: 0.9396 - val_accuracy: 0.8648\n",
      "Epoch 230/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0959 - accuracy: 0.9662 - val_loss: 0.9396 - val_accuracy: 0.8648\n",
      "Epoch 231/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0929 - accuracy: 0.9732 - val_loss: 0.9341 - val_accuracy: 0.8724\n",
      "Epoch 232/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0946 - accuracy: 0.9681 - val_loss: 0.9358 - val_accuracy: 0.8673\n",
      "Epoch 233/10000\n",
      "49/49 [==============================] - 0s 994us/step - loss: 0.0952 - accuracy: 0.9688 - val_loss: 0.9359 - val_accuracy: 0.8699\n",
      "Epoch 234/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0913 - accuracy: 0.9694 - val_loss: 0.9368 - val_accuracy: 0.8648\n",
      "Epoch 235/10000\n",
      "49/49 [==============================] - 0s 831us/step - loss: 0.0923 - accuracy: 0.9675 - val_loss: 0.9324 - val_accuracy: 0.8673\n",
      "Epoch 236/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0899 - accuracy: 0.9688 - val_loss: 0.9200 - val_accuracy: 0.8699\n",
      "Epoch 237/10000\n",
      "49/49 [==============================] - 0s 832us/step - loss: 0.0890 - accuracy: 0.9707 - val_loss: 0.9229 - val_accuracy: 0.8699\n",
      "Epoch 238/10000\n",
      "49/49 [==============================] - 0s 711us/step - loss: 0.0889 - accuracy: 0.9700 - val_loss: 0.9242 - val_accuracy: 0.8699\n",
      "Epoch 239/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0866 - accuracy: 0.9707 - val_loss: 0.9274 - val_accuracy: 0.8699\n",
      "Epoch 240/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0876 - accuracy: 0.9694 - val_loss: 0.9304 - val_accuracy: 0.8724\n",
      "Epoch 241/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0863 - accuracy: 0.9726 - val_loss: 0.9262 - val_accuracy: 0.8699\n",
      "Epoch 242/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0873 - accuracy: 0.9739 - val_loss: 0.9215 - val_accuracy: 0.8699\n",
      "Epoch 243/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0836 - accuracy: 0.9688 - val_loss: 0.9336 - val_accuracy: 0.8673\n",
      "Epoch 244/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0861 - accuracy: 0.9726 - val_loss: 0.9288 - val_accuracy: 0.8699\n",
      "Epoch 245/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0848 - accuracy: 0.9713 - val_loss: 0.9146 - val_accuracy: 0.8724\n",
      "Epoch 246/10000\n",
      "49/49 [==============================] - 0s 814us/step - loss: 0.0833 - accuracy: 0.9700 - val_loss: 0.9267 - val_accuracy: 0.8673\n",
      "Epoch 247/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0824 - accuracy: 0.9726 - val_loss: 0.9255 - val_accuracy: 0.8699\n",
      "Epoch 248/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0819 - accuracy: 0.9739 - val_loss: 0.9263 - val_accuracy: 0.8673\n",
      "Epoch 249/10000\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0817 - accuracy: 0.9739 - val_loss: 0.9316 - val_accuracy: 0.8622\n",
      "Epoch 250/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0800 - accuracy: 0.9758 - val_loss: 0.9153 - val_accuracy: 0.8699\n",
      "Epoch 251/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0817 - accuracy: 0.9732 - val_loss: 0.9256 - val_accuracy: 0.8673\n",
      "Epoch 252/10000\n",
      "49/49 [==============================] - 0s 731us/step - loss: 0.0801 - accuracy: 0.9732 - val_loss: 0.9235 - val_accuracy: 0.8673\n",
      "Epoch 253/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0779 - accuracy: 0.9739 - val_loss: 0.9150 - val_accuracy: 0.8673\n",
      "Epoch 254/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0784 - accuracy: 0.9732 - val_loss: 0.9223 - val_accuracy: 0.8673\n",
      "Epoch 255/10000\n",
      "49/49 [==============================] - 0s 851us/step - loss: 0.0779 - accuracy: 0.9758 - val_loss: 0.9232 - val_accuracy: 0.8648\n",
      "訓練時間：17.53 秒\n",
      "576/576 [==============================] - 1s 1ms/step - loss: 1.1532 - accuracy: 0.8705\n",
      "Evaluation on 90% unused data - Loss: 1.1532, Accuracy: 0.8705\n",
      "637/637 [==============================] - 1s 757us/step\n",
      "Test Data MDE report saved to: 40 data per RP transfer revised/DNN 0mcAPs BEST_40data_2024_12_27.json\n",
      "\n",
      "Test Data Mean MDE: 0.3465 meters\n",
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_104 (Dense)           (None, 64)                320       \n",
      "                                                                 \n",
      " dense_105 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_106 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_107 (Dense)           (None, 49)                3185      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20081 (78.44 KB)\n",
      "Trainable params: 3185 (12.44 KB)\n",
      "Non-trainable params: 16896 (66.00 KB)\n",
      "_________________________________________________________________\n",
      "   AP1_Rssi  AP2_Rssi  AP3_Rssi  AP4_Rssi\n",
      "0     -53.0     -71.0     -60.0     -47.0\n",
      "1     -61.0     -71.0     -64.0     -47.0\n",
      "2     -60.0     -71.0     -64.0     -45.0\n",
      "3     -61.0     -71.0     -65.0     -47.0\n",
      "4     -61.0     -71.0     -64.0     -47.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcslab/anaconda3/envs/yang_cuda3/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n",
      "/tmp/ipykernel_3268413/3821025327.py:45: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  test_processed_data = pd.concat([test_processed_data, group], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Set</th>\n",
       "      <th>Validation Set</th>\n",
       "      <th>Test Set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Training Set  Validation Set  Test Set\n",
       "0             32               8       358\n",
       "1             32               8       358\n",
       "2             32               8       358\n",
       "3             32               8       358\n",
       "4             32               8       358\n",
       "5             32               8       358\n",
       "6             32               8       358\n",
       "7             32               8       358\n",
       "8             32               8       358\n",
       "9             32               8       358\n",
       "10            32               8       358\n",
       "11            32               8       358\n",
       "12            32               8       358\n",
       "13            32               8       358\n",
       "14            32               8       358\n",
       "15            32               8       358\n",
       "16            32               8       358\n",
       "17            32               8       358\n",
       "18            32               8       358\n",
       "19            32               8       358\n",
       "20            32               8       358\n",
       "21            32               8       358\n",
       "22            32               8       358\n",
       "23            32               8       358\n",
       "24            32               8       358\n",
       "25            32               8       358\n",
       "26            32               8       358\n",
       "27            32               8       358\n",
       "28            32               8       358\n",
       "29            32               8       358\n",
       "30            32               8       358\n",
       "31            32               8       358\n",
       "32            32               8       358\n",
       "33            32               8       358\n",
       "34            32               8       358\n",
       "35            32               8       358\n",
       "36            32               8       358\n",
       "37            32               8       358\n",
       "38            32               8       358\n",
       "39            32               8       358\n",
       "40            32               8       358\n",
       "41            32               8       358\n",
       "42            32               8       358\n",
       "43            32               8       358\n",
       "44            32               8       358\n",
       "45            32               8       358\n",
       "46            32               8       358\n",
       "47            32               8       358\n",
       "48            32               8       358"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "Epoch 1/10000\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 19.3454 - accuracy: 0.2009 - val_loss: 17.3466 - val_accuracy: 0.2347\n",
      "Epoch 2/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 16.3219 - accuracy: 0.2519 - val_loss: 14.8664 - val_accuracy: 0.2832\n",
      "Epoch 3/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 14.1740 - accuracy: 0.2876 - val_loss: 12.9375 - val_accuracy: 0.3087\n",
      "Epoch 4/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 12.4358 - accuracy: 0.3112 - val_loss: 11.2460 - val_accuracy: 0.3240\n",
      "Epoch 5/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 10.9231 - accuracy: 0.3399 - val_loss: 9.8503 - val_accuracy: 0.3648\n",
      "Epoch 6/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 9.6386 - accuracy: 0.3712 - val_loss: 8.6485 - val_accuracy: 0.4056\n",
      "Epoch 7/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 8.5458 - accuracy: 0.4107 - val_loss: 7.6372 - val_accuracy: 0.4235\n",
      "Epoch 8/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 7.6185 - accuracy: 0.4311 - val_loss: 6.7396 - val_accuracy: 0.4515\n",
      "Epoch 9/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 6.7967 - accuracy: 0.4522 - val_loss: 6.0107 - val_accuracy: 0.4770\n",
      "Epoch 10/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 6.0717 - accuracy: 0.4675 - val_loss: 5.3550 - val_accuracy: 0.5077\n",
      "Epoch 11/10000\n",
      "49/49 [==============================] - 0s 845us/step - loss: 5.4492 - accuracy: 0.5006 - val_loss: 4.7716 - val_accuracy: 0.5383\n",
      "Epoch 12/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 4.9047 - accuracy: 0.5191 - val_loss: 4.3169 - val_accuracy: 0.5663\n",
      "Epoch 13/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 4.4456 - accuracy: 0.5523 - val_loss: 3.9595 - val_accuracy: 0.6071\n",
      "Epoch 14/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 4.0766 - accuracy: 0.5867 - val_loss: 3.6323 - val_accuracy: 0.6301\n",
      "Epoch 15/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 3.7618 - accuracy: 0.6167 - val_loss: 3.3751 - val_accuracy: 0.6352\n",
      "Epoch 16/10000\n",
      "49/49 [==============================] - 0s 727us/step - loss: 3.5025 - accuracy: 0.6314 - val_loss: 3.1576 - val_accuracy: 0.6505\n",
      "Epoch 17/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 3.2708 - accuracy: 0.6460 - val_loss: 2.9841 - val_accuracy: 0.6837\n",
      "Epoch 18/10000\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 3.0689 - accuracy: 0.6716 - val_loss: 2.8144 - val_accuracy: 0.6964\n",
      "Epoch 19/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 2.8905 - accuracy: 0.6894 - val_loss: 2.6621 - val_accuracy: 0.7015\n",
      "Epoch 20/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 2.7256 - accuracy: 0.7015 - val_loss: 2.5210 - val_accuracy: 0.7092\n",
      "Epoch 21/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 2.5866 - accuracy: 0.7124 - val_loss: 2.3966 - val_accuracy: 0.7092\n",
      "Epoch 22/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 2.4547 - accuracy: 0.7251 - val_loss: 2.2845 - val_accuracy: 0.7194\n",
      "Epoch 23/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 2.3401 - accuracy: 0.7277 - val_loss: 2.1838 - val_accuracy: 0.7270\n",
      "Epoch 24/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 2.2289 - accuracy: 0.7398 - val_loss: 2.0778 - val_accuracy: 0.7398\n",
      "Epoch 25/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 2.1269 - accuracy: 0.7513 - val_loss: 2.0027 - val_accuracy: 0.7423\n",
      "Epoch 26/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 2.0276 - accuracy: 0.7602 - val_loss: 1.9219 - val_accuracy: 0.7526\n",
      "Epoch 27/10000\n",
      "49/49 [==============================] - 0s 831us/step - loss: 1.9400 - accuracy: 0.7647 - val_loss: 1.8527 - val_accuracy: 0.7500\n",
      "Epoch 28/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.8627 - accuracy: 0.7710 - val_loss: 1.7903 - val_accuracy: 0.7577\n",
      "Epoch 29/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.7853 - accuracy: 0.7781 - val_loss: 1.7221 - val_accuracy: 0.7653\n",
      "Epoch 30/10000\n",
      "49/49 [==============================] - 0s 961us/step - loss: 1.7125 - accuracy: 0.7851 - val_loss: 1.6758 - val_accuracy: 0.7730\n",
      "Epoch 31/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.6494 - accuracy: 0.7895 - val_loss: 1.6288 - val_accuracy: 0.7730\n",
      "Epoch 32/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.5854 - accuracy: 0.7972 - val_loss: 1.5892 - val_accuracy: 0.7806\n",
      "Epoch 33/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.5290 - accuracy: 0.7991 - val_loss: 1.5468 - val_accuracy: 0.7857\n",
      "Epoch 34/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.4737 - accuracy: 0.8036 - val_loss: 1.5119 - val_accuracy: 0.7908\n",
      "Epoch 35/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.4254 - accuracy: 0.8023 - val_loss: 1.4824 - val_accuracy: 0.7934\n",
      "Epoch 36/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.3762 - accuracy: 0.8119 - val_loss: 1.4576 - val_accuracy: 0.7908\n",
      "Epoch 37/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.3342 - accuracy: 0.8106 - val_loss: 1.4270 - val_accuracy: 0.8036\n",
      "Epoch 38/10000\n",
      "49/49 [==============================] - 0s 745us/step - loss: 1.2897 - accuracy: 0.8170 - val_loss: 1.4066 - val_accuracy: 0.8010\n",
      "Epoch 39/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.2503 - accuracy: 0.8227 - val_loss: 1.3851 - val_accuracy: 0.8061\n",
      "Epoch 40/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.2068 - accuracy: 0.8272 - val_loss: 1.3620 - val_accuracy: 0.8087\n",
      "Epoch 41/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.1722 - accuracy: 0.8246 - val_loss: 1.3375 - val_accuracy: 0.8138\n",
      "Epoch 42/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.1412 - accuracy: 0.8304 - val_loss: 1.3184 - val_accuracy: 0.8112\n",
      "Epoch 43/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.1072 - accuracy: 0.8348 - val_loss: 1.3063 - val_accuracy: 0.8189\n",
      "Epoch 44/10000\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 1.0790 - accuracy: 0.8348 - val_loss: 1.2951 - val_accuracy: 0.8189\n",
      "Epoch 45/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.0482 - accuracy: 0.8361 - val_loss: 1.2769 - val_accuracy: 0.8163\n",
      "Epoch 46/10000\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 1.0193 - accuracy: 0.8463 - val_loss: 1.2633 - val_accuracy: 0.8240\n",
      "Epoch 47/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.9940 - accuracy: 0.8438 - val_loss: 1.2478 - val_accuracy: 0.8240\n",
      "Epoch 48/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.9687 - accuracy: 0.8489 - val_loss: 1.2342 - val_accuracy: 0.8265\n",
      "Epoch 49/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.9444 - accuracy: 0.8463 - val_loss: 1.2262 - val_accuracy: 0.8291\n",
      "Epoch 50/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.9234 - accuracy: 0.8469 - val_loss: 1.2146 - val_accuracy: 0.8291\n",
      "Epoch 51/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.9025 - accuracy: 0.8520 - val_loss: 1.2039 - val_accuracy: 0.8265\n",
      "Epoch 52/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.8770 - accuracy: 0.8552 - val_loss: 1.1911 - val_accuracy: 0.8316\n",
      "Epoch 53/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.8571 - accuracy: 0.8559 - val_loss: 1.1851 - val_accuracy: 0.8265\n",
      "Epoch 54/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.8371 - accuracy: 0.8591 - val_loss: 1.1712 - val_accuracy: 0.8291\n",
      "Epoch 55/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.8234 - accuracy: 0.8591 - val_loss: 1.1580 - val_accuracy: 0.8316\n",
      "Epoch 56/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.8003 - accuracy: 0.8603 - val_loss: 1.1522 - val_accuracy: 0.8342\n",
      "Epoch 57/10000\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.7850 - accuracy: 0.8597 - val_loss: 1.1463 - val_accuracy: 0.8291\n",
      "Epoch 58/10000\n",
      "49/49 [==============================] - 0s 957us/step - loss: 0.7672 - accuracy: 0.8584 - val_loss: 1.1387 - val_accuracy: 0.8342\n",
      "Epoch 59/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.7480 - accuracy: 0.8648 - val_loss: 1.1294 - val_accuracy: 0.8316\n",
      "Epoch 60/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.7314 - accuracy: 0.8667 - val_loss: 1.1272 - val_accuracy: 0.8316\n",
      "Epoch 61/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.7145 - accuracy: 0.8705 - val_loss: 1.1123 - val_accuracy: 0.8367\n",
      "Epoch 62/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.6996 - accuracy: 0.8693 - val_loss: 1.1143 - val_accuracy: 0.8520\n",
      "Epoch 63/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.6850 - accuracy: 0.8699 - val_loss: 1.1002 - val_accuracy: 0.8444\n",
      "Epoch 64/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.6729 - accuracy: 0.8776 - val_loss: 1.0981 - val_accuracy: 0.8418\n",
      "Epoch 65/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.6590 - accuracy: 0.8648 - val_loss: 1.0852 - val_accuracy: 0.8444\n",
      "Epoch 66/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.6451 - accuracy: 0.8744 - val_loss: 1.0835 - val_accuracy: 0.8571\n",
      "Epoch 67/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.6310 - accuracy: 0.8763 - val_loss: 1.0801 - val_accuracy: 0.8520\n",
      "Epoch 68/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.6156 - accuracy: 0.8820 - val_loss: 1.0720 - val_accuracy: 0.8571\n",
      "Epoch 69/10000\n",
      "49/49 [==============================] - 0s 919us/step - loss: 0.6060 - accuracy: 0.8814 - val_loss: 1.0641 - val_accuracy: 0.8571\n",
      "Epoch 70/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.5939 - accuracy: 0.8820 - val_loss: 1.0525 - val_accuracy: 0.8622\n",
      "Epoch 71/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.5807 - accuracy: 0.8871 - val_loss: 1.0550 - val_accuracy: 0.8597\n",
      "Epoch 72/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.5681 - accuracy: 0.8852 - val_loss: 1.0525 - val_accuracy: 0.8648\n",
      "Epoch 73/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.5584 - accuracy: 0.8916 - val_loss: 1.0447 - val_accuracy: 0.8597\n",
      "Epoch 74/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.5489 - accuracy: 0.8884 - val_loss: 1.0358 - val_accuracy: 0.8648\n",
      "Epoch 75/10000\n",
      "49/49 [==============================] - 0s 723us/step - loss: 0.5382 - accuracy: 0.8897 - val_loss: 1.0314 - val_accuracy: 0.8673\n",
      "Epoch 76/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.5255 - accuracy: 0.8916 - val_loss: 1.0287 - val_accuracy: 0.8673\n",
      "Epoch 77/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.5150 - accuracy: 0.8967 - val_loss: 1.0208 - val_accuracy: 0.8673\n",
      "Epoch 78/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.5077 - accuracy: 0.8999 - val_loss: 1.0231 - val_accuracy: 0.8622\n",
      "Epoch 79/10000\n",
      "49/49 [==============================] - 0s 894us/step - loss: 0.4972 - accuracy: 0.8954 - val_loss: 1.0184 - val_accuracy: 0.8622\n",
      "Epoch 80/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.4877 - accuracy: 0.8992 - val_loss: 1.0051 - val_accuracy: 0.8648\n",
      "Epoch 81/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.4789 - accuracy: 0.8967 - val_loss: 1.0070 - val_accuracy: 0.8622\n",
      "Epoch 82/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.4719 - accuracy: 0.8992 - val_loss: 1.0019 - val_accuracy: 0.8648\n",
      "Epoch 83/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.4602 - accuracy: 0.8999 - val_loss: 0.9993 - val_accuracy: 0.8597\n",
      "Epoch 84/10000\n",
      "49/49 [==============================] - 0s 742us/step - loss: 0.4536 - accuracy: 0.9037 - val_loss: 0.9928 - val_accuracy: 0.8648\n",
      "Epoch 85/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.4442 - accuracy: 0.9031 - val_loss: 0.9831 - val_accuracy: 0.8597\n",
      "Epoch 86/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.4391 - accuracy: 0.9037 - val_loss: 0.9820 - val_accuracy: 0.8622\n",
      "Epoch 87/10000\n",
      "49/49 [==============================] - 0s 906us/step - loss: 0.4278 - accuracy: 0.9069 - val_loss: 0.9797 - val_accuracy: 0.8622\n",
      "Epoch 88/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.4225 - accuracy: 0.9082 - val_loss: 0.9745 - val_accuracy: 0.8597\n",
      "Epoch 89/10000\n",
      "49/49 [==============================] - 0s 906us/step - loss: 0.4152 - accuracy: 0.9120 - val_loss: 0.9674 - val_accuracy: 0.8648\n",
      "Epoch 90/10000\n",
      "49/49 [==============================] - 0s 900us/step - loss: 0.4075 - accuracy: 0.9114 - val_loss: 0.9659 - val_accuracy: 0.8648\n",
      "Epoch 91/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.3998 - accuracy: 0.9177 - val_loss: 0.9607 - val_accuracy: 0.8673\n",
      "Epoch 92/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3952 - accuracy: 0.9114 - val_loss: 0.9541 - val_accuracy: 0.8699\n",
      "Epoch 93/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.3897 - accuracy: 0.9133 - val_loss: 0.9498 - val_accuracy: 0.8673\n",
      "Epoch 94/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3835 - accuracy: 0.9101 - val_loss: 0.9451 - val_accuracy: 0.8699\n",
      "Epoch 95/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.3766 - accuracy: 0.9158 - val_loss: 0.9409 - val_accuracy: 0.8699\n",
      "Epoch 96/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3686 - accuracy: 0.9196 - val_loss: 0.9388 - val_accuracy: 0.8724\n",
      "Epoch 97/10000\n",
      "49/49 [==============================] - 0s 996us/step - loss: 0.3636 - accuracy: 0.9190 - val_loss: 0.9379 - val_accuracy: 0.8597\n",
      "Epoch 98/10000\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.3613 - accuracy: 0.9165 - val_loss: 0.9267 - val_accuracy: 0.8673\n",
      "Epoch 99/10000\n",
      "49/49 [==============================] - 0s 999us/step - loss: 0.3537 - accuracy: 0.9190 - val_loss: 0.9165 - val_accuracy: 0.8673\n",
      "Epoch 100/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3488 - accuracy: 0.9196 - val_loss: 0.9176 - val_accuracy: 0.8724\n",
      "Epoch 101/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3428 - accuracy: 0.9222 - val_loss: 0.9119 - val_accuracy: 0.8699\n",
      "Epoch 102/10000\n",
      "49/49 [==============================] - 0s 933us/step - loss: 0.3384 - accuracy: 0.9203 - val_loss: 0.9148 - val_accuracy: 0.8622\n",
      "Epoch 103/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.3321 - accuracy: 0.9241 - val_loss: 0.9084 - val_accuracy: 0.8699\n",
      "Epoch 104/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.3270 - accuracy: 0.9241 - val_loss: 0.9031 - val_accuracy: 0.8699\n",
      "Epoch 105/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3242 - accuracy: 0.9279 - val_loss: 0.9032 - val_accuracy: 0.8648\n",
      "Epoch 106/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3170 - accuracy: 0.9273 - val_loss: 0.8936 - val_accuracy: 0.8648\n",
      "Epoch 107/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.3123 - accuracy: 0.9311 - val_loss: 0.8948 - val_accuracy: 0.8724\n",
      "Epoch 108/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3120 - accuracy: 0.9228 - val_loss: 0.8876 - val_accuracy: 0.8776\n",
      "Epoch 109/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.3046 - accuracy: 0.9267 - val_loss: 0.8843 - val_accuracy: 0.8724\n",
      "Epoch 110/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3004 - accuracy: 0.9298 - val_loss: 0.8786 - val_accuracy: 0.8724\n",
      "Epoch 111/10000\n",
      "49/49 [==============================] - 0s 929us/step - loss: 0.2971 - accuracy: 0.9324 - val_loss: 0.8810 - val_accuracy: 0.8724\n",
      "Epoch 112/10000\n",
      "49/49 [==============================] - 0s 698us/step - loss: 0.2919 - accuracy: 0.9292 - val_loss: 0.8781 - val_accuracy: 0.8750\n",
      "Epoch 113/10000\n",
      "49/49 [==============================] - 0s 681us/step - loss: 0.2865 - accuracy: 0.9324 - val_loss: 0.8713 - val_accuracy: 0.8699\n",
      "Epoch 114/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2837 - accuracy: 0.9311 - val_loss: 0.8674 - val_accuracy: 0.8750\n",
      "Epoch 115/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2832 - accuracy: 0.9305 - val_loss: 0.8743 - val_accuracy: 0.8750\n",
      "Epoch 116/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2770 - accuracy: 0.9330 - val_loss: 0.8740 - val_accuracy: 0.8699\n",
      "Epoch 117/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2749 - accuracy: 0.9318 - val_loss: 0.8640 - val_accuracy: 0.8699\n",
      "Epoch 118/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2702 - accuracy: 0.9349 - val_loss: 0.8572 - val_accuracy: 0.8750\n",
      "Epoch 119/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.2665 - accuracy: 0.9356 - val_loss: 0.8544 - val_accuracy: 0.8699\n",
      "Epoch 120/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2644 - accuracy: 0.9369 - val_loss: 0.8592 - val_accuracy: 0.8724\n",
      "Epoch 121/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.2629 - accuracy: 0.9324 - val_loss: 0.8568 - val_accuracy: 0.8724\n",
      "Epoch 122/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.2553 - accuracy: 0.9401 - val_loss: 0.8456 - val_accuracy: 0.8699\n",
      "Epoch 123/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2530 - accuracy: 0.9381 - val_loss: 0.8511 - val_accuracy: 0.8699\n",
      "Epoch 124/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.2511 - accuracy: 0.9356 - val_loss: 0.8509 - val_accuracy: 0.8699\n",
      "Epoch 125/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2468 - accuracy: 0.9458 - val_loss: 0.8524 - val_accuracy: 0.8699\n",
      "Epoch 126/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2425 - accuracy: 0.9375 - val_loss: 0.8419 - val_accuracy: 0.8673\n",
      "Epoch 127/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2422 - accuracy: 0.9420 - val_loss: 0.8450 - val_accuracy: 0.8724\n",
      "Epoch 128/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.2383 - accuracy: 0.9420 - val_loss: 0.8425 - val_accuracy: 0.8699\n",
      "Epoch 129/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2363 - accuracy: 0.9375 - val_loss: 0.8416 - val_accuracy: 0.8699\n",
      "Epoch 130/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2350 - accuracy: 0.9407 - val_loss: 0.8418 - val_accuracy: 0.8699\n",
      "Epoch 131/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.2293 - accuracy: 0.9432 - val_loss: 0.8351 - val_accuracy: 0.8699\n",
      "Epoch 132/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.2265 - accuracy: 0.9445 - val_loss: 0.8441 - val_accuracy: 0.8673\n",
      "Epoch 133/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.2236 - accuracy: 0.9381 - val_loss: 0.8388 - val_accuracy: 0.8724\n",
      "Epoch 134/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.2209 - accuracy: 0.9452 - val_loss: 0.8333 - val_accuracy: 0.8776\n",
      "Epoch 135/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2182 - accuracy: 0.9432 - val_loss: 0.8297 - val_accuracy: 0.8776\n",
      "Epoch 136/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.2178 - accuracy: 0.9490 - val_loss: 0.8376 - val_accuracy: 0.8699\n",
      "Epoch 137/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2158 - accuracy: 0.9432 - val_loss: 0.8315 - val_accuracy: 0.8724\n",
      "Epoch 138/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2113 - accuracy: 0.9445 - val_loss: 0.8306 - val_accuracy: 0.8724\n",
      "Epoch 139/10000\n",
      "49/49 [==============================] - 0s 996us/step - loss: 0.2085 - accuracy: 0.9464 - val_loss: 0.8392 - val_accuracy: 0.8724\n",
      "Epoch 140/10000\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.2054 - accuracy: 0.9477 - val_loss: 0.8300 - val_accuracy: 0.8724\n",
      "Epoch 141/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2044 - accuracy: 0.9503 - val_loss: 0.8308 - val_accuracy: 0.8699\n",
      "Epoch 142/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2023 - accuracy: 0.9490 - val_loss: 0.8364 - val_accuracy: 0.8776\n",
      "Epoch 143/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2022 - accuracy: 0.9483 - val_loss: 0.8304 - val_accuracy: 0.8699\n",
      "Epoch 144/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.1972 - accuracy: 0.9471 - val_loss: 0.8256 - val_accuracy: 0.8776\n",
      "Epoch 145/10000\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.1932 - accuracy: 0.9483 - val_loss: 0.8276 - val_accuracy: 0.8801\n",
      "Epoch 146/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1930 - accuracy: 0.9534 - val_loss: 0.8310 - val_accuracy: 0.8776\n",
      "Epoch 147/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.1908 - accuracy: 0.9541 - val_loss: 0.8294 - val_accuracy: 0.8699\n",
      "Epoch 148/10000\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.1888 - accuracy: 0.9541 - val_loss: 0.8359 - val_accuracy: 0.8801\n",
      "Epoch 149/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1874 - accuracy: 0.9534 - val_loss: 0.8300 - val_accuracy: 0.8750\n",
      "Epoch 150/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1852 - accuracy: 0.9490 - val_loss: 0.8234 - val_accuracy: 0.8750\n",
      "Epoch 151/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1840 - accuracy: 0.9503 - val_loss: 0.8281 - val_accuracy: 0.8750\n",
      "Epoch 152/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1784 - accuracy: 0.9490 - val_loss: 0.8253 - val_accuracy: 0.8724\n",
      "Epoch 153/10000\n",
      "49/49 [==============================] - 0s 997us/step - loss: 0.1803 - accuracy: 0.9522 - val_loss: 0.8301 - val_accuracy: 0.8750\n",
      "Epoch 154/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1791 - accuracy: 0.9483 - val_loss: 0.8293 - val_accuracy: 0.8699\n",
      "Epoch 155/10000\n",
      "49/49 [==============================] - 0s 938us/step - loss: 0.1762 - accuracy: 0.9490 - val_loss: 0.8257 - val_accuracy: 0.8776\n",
      "Epoch 156/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.1747 - accuracy: 0.9554 - val_loss: 0.8238 - val_accuracy: 0.8827\n",
      "Epoch 157/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1701 - accuracy: 0.9554 - val_loss: 0.8249 - val_accuracy: 0.8724\n",
      "Epoch 158/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1705 - accuracy: 0.9554 - val_loss: 0.8254 - val_accuracy: 0.8724\n",
      "Epoch 159/10000\n",
      "49/49 [==============================] - 0s 871us/step - loss: 0.1674 - accuracy: 0.9515 - val_loss: 0.8230 - val_accuracy: 0.8776\n",
      "Epoch 160/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1664 - accuracy: 0.9541 - val_loss: 0.8257 - val_accuracy: 0.8776\n",
      "Epoch 161/10000\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.1650 - accuracy: 0.9566 - val_loss: 0.8235 - val_accuracy: 0.8724\n",
      "Epoch 162/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1653 - accuracy: 0.9534 - val_loss: 0.8258 - val_accuracy: 0.8750\n",
      "Epoch 163/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1611 - accuracy: 0.9573 - val_loss: 0.8265 - val_accuracy: 0.8724\n",
      "Epoch 164/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1610 - accuracy: 0.9541 - val_loss: 0.8246 - val_accuracy: 0.8750\n",
      "Epoch 165/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1580 - accuracy: 0.9560 - val_loss: 0.8169 - val_accuracy: 0.8776\n",
      "Epoch 166/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1581 - accuracy: 0.9547 - val_loss: 0.8178 - val_accuracy: 0.8801\n",
      "Epoch 167/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1572 - accuracy: 0.9528 - val_loss: 0.8301 - val_accuracy: 0.8750\n",
      "Epoch 168/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1552 - accuracy: 0.9579 - val_loss: 0.8265 - val_accuracy: 0.8750\n",
      "Epoch 169/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1533 - accuracy: 0.9605 - val_loss: 0.8258 - val_accuracy: 0.8699\n",
      "Epoch 170/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1520 - accuracy: 0.9579 - val_loss: 0.8218 - val_accuracy: 0.8776\n",
      "Epoch 171/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.1534 - accuracy: 0.9573 - val_loss: 0.8238 - val_accuracy: 0.8776\n",
      "Epoch 172/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1509 - accuracy: 0.9598 - val_loss: 0.8183 - val_accuracy: 0.8827\n",
      "Epoch 173/10000\n",
      "49/49 [==============================] - 0s 815us/step - loss: 0.1471 - accuracy: 0.9592 - val_loss: 0.8223 - val_accuracy: 0.8750\n",
      "Epoch 174/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1478 - accuracy: 0.9566 - val_loss: 0.8253 - val_accuracy: 0.8827\n",
      "Epoch 175/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1451 - accuracy: 0.9624 - val_loss: 0.8177 - val_accuracy: 0.8827\n",
      "訓練時間：12.48 秒\n",
      "549/549 [==============================] - 0s 668us/step - loss: 0.9214 - accuracy: 0.8691\n",
      "Evaluation on 90% unused data - Loss: 0.9214, Accuracy: 0.8691\n",
      "610/610 [==============================] - 0s 777us/step\n",
      "Test Data MDE report saved to: 40 data per RP transfer revised/DNN 0mcAPs BEST_40data_2025_01_03.json\n",
      "\n",
      "Test Data Mean MDE: 0.2971 meters\n",
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_104 (Dense)           (None, 64)                320       \n",
      "                                                                 \n",
      " dense_105 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_106 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_107 (Dense)           (None, 49)                3185      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20081 (78.44 KB)\n",
      "Trainable params: 3185 (12.44 KB)\n",
      "Non-trainable params: 16896 (66.00 KB)\n",
      "_________________________________________________________________\n",
      "   AP1_Rssi  AP2_Rssi  AP3_Rssi  AP4_Rssi\n",
      "0     -65.0     -68.0     -64.0     -50.0\n",
      "1     -65.0     -68.0     -66.0     -54.0\n",
      "2     -65.0     -68.0     -64.0     -54.0\n",
      "3     -65.0     -68.0     -64.0     -54.0\n",
      "4     -65.0     -68.0     -66.0     -54.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcslab/anaconda3/envs/yang_cuda3/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n",
      "/tmp/ipykernel_3268413/3821025327.py:45: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  test_processed_data = pd.concat([test_processed_data, group], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Set</th>\n",
       "      <th>Validation Set</th>\n",
       "      <th>Test Set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Training Set  Validation Set  Test Set\n",
       "0             32               8       362\n",
       "1             32               8       362\n",
       "2             32               8       362\n",
       "3             32               8       362\n",
       "4             32               8       362\n",
       "5             32               8       362\n",
       "6             32               8       362\n",
       "7             32               8       362\n",
       "8             32               8       362\n",
       "9             32               8       362\n",
       "10            32               8       362\n",
       "11            32               8       362\n",
       "12            32               8       362\n",
       "13            32               8       362\n",
       "14            32               8       362\n",
       "15            32               8       362\n",
       "16            32               8       362\n",
       "17            32               8       362\n",
       "18            32               8       362\n",
       "19            32               8       362\n",
       "20            32               8       362\n",
       "21            32               8       362\n",
       "22            32               8       362\n",
       "23            32               8       362\n",
       "24            32               8       362\n",
       "25            32               8       362\n",
       "26            32               8       362\n",
       "27            32               8       362\n",
       "28            32               8       362\n",
       "29            32               8       362\n",
       "30            32               8       362\n",
       "31            32               8       362\n",
       "32            32               8       362\n",
       "33            32               8       362\n",
       "34            32               8       362\n",
       "35            32               8       362\n",
       "36            32               8       362\n",
       "37            32               8       362\n",
       "38            32               8       362\n",
       "39            32               8       362\n",
       "40            32               8       362\n",
       "41            32               8       362\n",
       "42            32               8       362\n",
       "43            32               8       362\n",
       "44            32               8       362\n",
       "45            32               8       362\n",
       "46            32               8       362\n",
       "47            32               8       362\n",
       "48            32               8       362"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "Epoch 1/10000\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 15.8590 - accuracy: 0.2455 - val_loss: 14.6178 - val_accuracy: 0.2347\n",
      "Epoch 2/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 13.5347 - accuracy: 0.2634 - val_loss: 12.5848 - val_accuracy: 0.2730\n",
      "Epoch 3/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 11.6363 - accuracy: 0.2915 - val_loss: 10.9360 - val_accuracy: 0.3036\n",
      "Epoch 4/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 10.0538 - accuracy: 0.3176 - val_loss: 9.5929 - val_accuracy: 0.3469\n",
      "Epoch 5/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 8.7296 - accuracy: 0.3622 - val_loss: 8.4858 - val_accuracy: 0.3801\n",
      "Epoch 6/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 7.6195 - accuracy: 0.3960 - val_loss: 7.5676 - val_accuracy: 0.4133\n",
      "Epoch 7/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 6.6811 - accuracy: 0.4362 - val_loss: 6.7904 - val_accuracy: 0.4464\n",
      "Epoch 8/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 5.9315 - accuracy: 0.4739 - val_loss: 6.1597 - val_accuracy: 0.5000\n",
      "Epoch 9/10000\n",
      "49/49 [==============================] - 0s 896us/step - loss: 5.3188 - accuracy: 0.5121 - val_loss: 5.6381 - val_accuracy: 0.5153\n",
      "Epoch 10/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 4.8059 - accuracy: 0.5357 - val_loss: 5.2207 - val_accuracy: 0.5485\n",
      "Epoch 11/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 4.3837 - accuracy: 0.5695 - val_loss: 4.8587 - val_accuracy: 0.5612\n",
      "Epoch 12/10000\n",
      "49/49 [==============================] - 0s 837us/step - loss: 4.0105 - accuracy: 0.5989 - val_loss: 4.5452 - val_accuracy: 0.5791\n",
      "Epoch 13/10000\n",
      "49/49 [==============================] - 0s 916us/step - loss: 3.6899 - accuracy: 0.6167 - val_loss: 4.2756 - val_accuracy: 0.6148\n",
      "Epoch 14/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 3.4333 - accuracy: 0.6473 - val_loss: 4.0756 - val_accuracy: 0.6403\n",
      "Epoch 15/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 3.2243 - accuracy: 0.6684 - val_loss: 3.8974 - val_accuracy: 0.6607\n",
      "Epoch 16/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 3.0434 - accuracy: 0.6932 - val_loss: 3.7624 - val_accuracy: 0.6760\n",
      "Epoch 17/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 2.8828 - accuracy: 0.6990 - val_loss: 3.6288 - val_accuracy: 0.6913\n",
      "Epoch 18/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 2.7439 - accuracy: 0.7092 - val_loss: 3.5079 - val_accuracy: 0.6964\n",
      "Epoch 19/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 2.6230 - accuracy: 0.7226 - val_loss: 3.4198 - val_accuracy: 0.7092\n",
      "Epoch 20/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 2.5067 - accuracy: 0.7417 - val_loss: 3.3293 - val_accuracy: 0.7219\n",
      "Epoch 21/10000\n",
      "49/49 [==============================] - 0s 947us/step - loss: 2.4072 - accuracy: 0.7455 - val_loss: 3.2509 - val_accuracy: 0.7270\n",
      "Epoch 22/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 2.3051 - accuracy: 0.7506 - val_loss: 3.1806 - val_accuracy: 0.7372\n",
      "Epoch 23/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 2.2142 - accuracy: 0.7615 - val_loss: 3.0960 - val_accuracy: 0.7423\n",
      "Epoch 24/10000\n",
      "49/49 [==============================] - 0s 945us/step - loss: 2.1266 - accuracy: 0.7659 - val_loss: 3.0338 - val_accuracy: 0.7551\n",
      "Epoch 25/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 2.0501 - accuracy: 0.7742 - val_loss: 2.9813 - val_accuracy: 0.7526\n",
      "Epoch 26/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.9793 - accuracy: 0.7832 - val_loss: 2.9137 - val_accuracy: 0.7551\n",
      "Epoch 27/10000\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 1.9133 - accuracy: 0.7844 - val_loss: 2.8667 - val_accuracy: 0.7602\n",
      "Epoch 28/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.8447 - accuracy: 0.7857 - val_loss: 2.8156 - val_accuracy: 0.7704\n",
      "Epoch 29/10000\n",
      "49/49 [==============================] - 0s 997us/step - loss: 1.7852 - accuracy: 0.7966 - val_loss: 2.7693 - val_accuracy: 0.7653\n",
      "Epoch 30/10000\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 1.7333 - accuracy: 0.8029 - val_loss: 2.7265 - val_accuracy: 0.7781\n",
      "Epoch 31/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.6762 - accuracy: 0.8093 - val_loss: 2.6792 - val_accuracy: 0.7806\n",
      "Epoch 32/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.6248 - accuracy: 0.8099 - val_loss: 2.6458 - val_accuracy: 0.7806\n",
      "Epoch 33/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.5733 - accuracy: 0.8138 - val_loss: 2.5976 - val_accuracy: 0.7883\n",
      "Epoch 34/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.5283 - accuracy: 0.8246 - val_loss: 2.5694 - val_accuracy: 0.7883\n",
      "Epoch 35/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.4855 - accuracy: 0.8182 - val_loss: 2.5255 - val_accuracy: 0.7934\n",
      "Epoch 36/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.4403 - accuracy: 0.8291 - val_loss: 2.4966 - val_accuracy: 0.8010\n",
      "Epoch 37/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.3985 - accuracy: 0.8284 - val_loss: 2.4549 - val_accuracy: 0.8036\n",
      "Epoch 38/10000\n",
      "49/49 [==============================] - 0s 992us/step - loss: 1.3608 - accuracy: 0.8361 - val_loss: 2.4260 - val_accuracy: 0.8036\n",
      "Epoch 39/10000\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 1.3229 - accuracy: 0.8310 - val_loss: 2.3969 - val_accuracy: 0.8138\n",
      "Epoch 40/10000\n",
      "49/49 [==============================] - 0s 872us/step - loss: 1.2875 - accuracy: 0.8348 - val_loss: 2.3650 - val_accuracy: 0.8087\n",
      "Epoch 41/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.2487 - accuracy: 0.8393 - val_loss: 2.3392 - val_accuracy: 0.8138\n",
      "Epoch 42/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.2142 - accuracy: 0.8438 - val_loss: 2.3112 - val_accuracy: 0.8138\n",
      "Epoch 43/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.1825 - accuracy: 0.8457 - val_loss: 2.2910 - val_accuracy: 0.8112\n",
      "Epoch 44/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.1519 - accuracy: 0.8406 - val_loss: 2.2588 - val_accuracy: 0.8214\n",
      "Epoch 45/10000\n",
      "49/49 [==============================] - 0s 702us/step - loss: 1.1141 - accuracy: 0.8482 - val_loss: 2.2322 - val_accuracy: 0.8138\n",
      "Epoch 46/10000\n",
      "49/49 [==============================] - 0s 678us/step - loss: 1.0830 - accuracy: 0.8508 - val_loss: 2.2126 - val_accuracy: 0.8214\n",
      "Epoch 47/10000\n",
      "49/49 [==============================] - 0s 938us/step - loss: 1.0558 - accuracy: 0.8489 - val_loss: 2.1897 - val_accuracy: 0.8214\n",
      "Epoch 48/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.0223 - accuracy: 0.8501 - val_loss: 2.1700 - val_accuracy: 0.8240\n",
      "Epoch 49/10000\n",
      "49/49 [==============================] - 0s 834us/step - loss: 1.0002 - accuracy: 0.8559 - val_loss: 2.1496 - val_accuracy: 0.8189\n",
      "Epoch 50/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.9724 - accuracy: 0.8597 - val_loss: 2.1307 - val_accuracy: 0.8214\n",
      "Epoch 51/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.9450 - accuracy: 0.8622 - val_loss: 2.1020 - val_accuracy: 0.8214\n",
      "Epoch 52/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.9200 - accuracy: 0.8648 - val_loss: 2.0898 - val_accuracy: 0.8189\n",
      "Epoch 53/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.8963 - accuracy: 0.8610 - val_loss: 2.0754 - val_accuracy: 0.8240\n",
      "Epoch 54/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.8754 - accuracy: 0.8680 - val_loss: 2.0545 - val_accuracy: 0.8214\n",
      "Epoch 55/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.8486 - accuracy: 0.8699 - val_loss: 2.0379 - val_accuracy: 0.8265\n",
      "Epoch 56/10000\n",
      "49/49 [==============================] - 0s 956us/step - loss: 0.8295 - accuracy: 0.8737 - val_loss: 2.0305 - val_accuracy: 0.8265\n",
      "Epoch 57/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.8136 - accuracy: 0.8724 - val_loss: 2.0089 - val_accuracy: 0.8240\n",
      "Epoch 58/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.7890 - accuracy: 0.8744 - val_loss: 2.0040 - val_accuracy: 0.8214\n",
      "Epoch 59/10000\n",
      "49/49 [==============================] - 0s 736us/step - loss: 0.7697 - accuracy: 0.8801 - val_loss: 1.9887 - val_accuracy: 0.8316\n",
      "Epoch 60/10000\n",
      "49/49 [==============================] - 0s 772us/step - loss: 0.7507 - accuracy: 0.8807 - val_loss: 1.9724 - val_accuracy: 0.8265\n",
      "Epoch 61/10000\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.7333 - accuracy: 0.8820 - val_loss: 1.9800 - val_accuracy: 0.8342\n",
      "Epoch 62/10000\n",
      "49/49 [==============================] - 0s 867us/step - loss: 0.7144 - accuracy: 0.8852 - val_loss: 1.9605 - val_accuracy: 0.8291\n",
      "Epoch 63/10000\n",
      "49/49 [==============================] - 0s 712us/step - loss: 0.6988 - accuracy: 0.8807 - val_loss: 1.9529 - val_accuracy: 0.8291\n",
      "Epoch 64/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.6824 - accuracy: 0.8878 - val_loss: 1.9398 - val_accuracy: 0.8393\n",
      "Epoch 65/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.6653 - accuracy: 0.8890 - val_loss: 1.9342 - val_accuracy: 0.8393\n",
      "Epoch 66/10000\n",
      "49/49 [==============================] - 0s 875us/step - loss: 0.6512 - accuracy: 0.8897 - val_loss: 1.9265 - val_accuracy: 0.8393\n",
      "Epoch 67/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.6362 - accuracy: 0.8973 - val_loss: 1.9193 - val_accuracy: 0.8393\n",
      "Epoch 68/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.6233 - accuracy: 0.8960 - val_loss: 1.9108 - val_accuracy: 0.8469\n",
      "Epoch 69/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.6118 - accuracy: 0.8941 - val_loss: 1.8928 - val_accuracy: 0.8469\n",
      "Epoch 70/10000\n",
      "49/49 [==============================] - 0s 696us/step - loss: 0.5981 - accuracy: 0.9005 - val_loss: 1.9002 - val_accuracy: 0.8444\n",
      "Epoch 71/10000\n",
      "49/49 [==============================] - 0s 717us/step - loss: 0.5859 - accuracy: 0.8960 - val_loss: 1.8852 - val_accuracy: 0.8444\n",
      "Epoch 72/10000\n",
      "49/49 [==============================] - 0s 794us/step - loss: 0.5761 - accuracy: 0.9018 - val_loss: 1.8756 - val_accuracy: 0.8444\n",
      "Epoch 73/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.5625 - accuracy: 0.8999 - val_loss: 1.8626 - val_accuracy: 0.8520\n",
      "Epoch 74/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.5510 - accuracy: 0.9075 - val_loss: 1.8654 - val_accuracy: 0.8469\n",
      "Epoch 75/10000\n",
      "49/49 [==============================] - 0s 764us/step - loss: 0.5397 - accuracy: 0.9069 - val_loss: 1.8588 - val_accuracy: 0.8469\n",
      "Epoch 76/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.5305 - accuracy: 0.9126 - val_loss: 1.8506 - val_accuracy: 0.8495\n",
      "Epoch 77/10000\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.5228 - accuracy: 0.9056 - val_loss: 1.8385 - val_accuracy: 0.8495\n",
      "Epoch 78/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.5095 - accuracy: 0.9133 - val_loss: 1.8403 - val_accuracy: 0.8444\n",
      "Epoch 79/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.5013 - accuracy: 0.9126 - val_loss: 1.8297 - val_accuracy: 0.8520\n",
      "Epoch 80/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.4937 - accuracy: 0.9126 - val_loss: 1.8293 - val_accuracy: 0.8520\n",
      "Epoch 81/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.4835 - accuracy: 0.9120 - val_loss: 1.8113 - val_accuracy: 0.8546\n",
      "Epoch 82/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.4740 - accuracy: 0.9203 - val_loss: 1.8038 - val_accuracy: 0.8571\n",
      "Epoch 83/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.4664 - accuracy: 0.9165 - val_loss: 1.8064 - val_accuracy: 0.8546\n",
      "Epoch 84/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.4580 - accuracy: 0.9171 - val_loss: 1.7954 - val_accuracy: 0.8546\n",
      "Epoch 85/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.4533 - accuracy: 0.9184 - val_loss: 1.7896 - val_accuracy: 0.8495\n",
      "Epoch 86/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.4412 - accuracy: 0.9196 - val_loss: 1.7845 - val_accuracy: 0.8597\n",
      "Epoch 87/10000\n",
      "49/49 [==============================] - 0s 887us/step - loss: 0.4341 - accuracy: 0.9216 - val_loss: 1.7815 - val_accuracy: 0.8648\n",
      "Epoch 88/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.4299 - accuracy: 0.9152 - val_loss: 1.7737 - val_accuracy: 0.8597\n",
      "Epoch 89/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.4223 - accuracy: 0.9196 - val_loss: 1.7697 - val_accuracy: 0.8597\n",
      "Epoch 90/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.4132 - accuracy: 0.9267 - val_loss: 1.7593 - val_accuracy: 0.8571\n",
      "Epoch 91/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.4086 - accuracy: 0.9267 - val_loss: 1.7603 - val_accuracy: 0.8597\n",
      "Epoch 92/10000\n",
      "49/49 [==============================] - 0s 885us/step - loss: 0.4004 - accuracy: 0.9222 - val_loss: 1.7546 - val_accuracy: 0.8597\n",
      "Epoch 93/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3939 - accuracy: 0.9267 - val_loss: 1.7481 - val_accuracy: 0.8597\n",
      "Epoch 94/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3886 - accuracy: 0.9279 - val_loss: 1.7424 - val_accuracy: 0.8648\n",
      "Epoch 95/10000\n",
      "49/49 [==============================] - 0s 971us/step - loss: 0.3815 - accuracy: 0.9318 - val_loss: 1.7425 - val_accuracy: 0.8648\n",
      "Epoch 96/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3779 - accuracy: 0.9279 - val_loss: 1.7415 - val_accuracy: 0.8597\n",
      "Epoch 97/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3720 - accuracy: 0.9311 - val_loss: 1.7279 - val_accuracy: 0.8597\n",
      "Epoch 98/10000\n",
      "49/49 [==============================] - 0s 775us/step - loss: 0.3658 - accuracy: 0.9324 - val_loss: 1.7447 - val_accuracy: 0.8571\n",
      "Epoch 99/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.3614 - accuracy: 0.9273 - val_loss: 1.7333 - val_accuracy: 0.8597\n",
      "Epoch 100/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3554 - accuracy: 0.9330 - val_loss: 1.7232 - val_accuracy: 0.8622\n",
      "Epoch 101/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3499 - accuracy: 0.9343 - val_loss: 1.7195 - val_accuracy: 0.8648\n",
      "Epoch 102/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3424 - accuracy: 0.9330 - val_loss: 1.7262 - val_accuracy: 0.8648\n",
      "Epoch 103/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3413 - accuracy: 0.9311 - val_loss: 1.7203 - val_accuracy: 0.8622\n",
      "Epoch 104/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3383 - accuracy: 0.9305 - val_loss: 1.7136 - val_accuracy: 0.8648\n",
      "Epoch 105/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.3305 - accuracy: 0.9375 - val_loss: 1.7111 - val_accuracy: 0.8648\n",
      "Epoch 106/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3249 - accuracy: 0.9381 - val_loss: 1.7104 - val_accuracy: 0.8648\n",
      "Epoch 107/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3196 - accuracy: 0.9369 - val_loss: 1.7183 - val_accuracy: 0.8622\n",
      "Epoch 108/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.3153 - accuracy: 0.9401 - val_loss: 1.7111 - val_accuracy: 0.8648\n",
      "Epoch 109/10000\n",
      "49/49 [==============================] - 0s 815us/step - loss: 0.3090 - accuracy: 0.9362 - val_loss: 1.7038 - val_accuracy: 0.8673\n",
      "Epoch 110/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3058 - accuracy: 0.9407 - val_loss: 1.6988 - val_accuracy: 0.8648\n",
      "Epoch 111/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3010 - accuracy: 0.9394 - val_loss: 1.7066 - val_accuracy: 0.8622\n",
      "Epoch 112/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2979 - accuracy: 0.9381 - val_loss: 1.6925 - val_accuracy: 0.8673\n",
      "Epoch 113/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2934 - accuracy: 0.9388 - val_loss: 1.6968 - val_accuracy: 0.8648\n",
      "Epoch 114/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2886 - accuracy: 0.9420 - val_loss: 1.7015 - val_accuracy: 0.8648\n",
      "Epoch 115/10000\n",
      "49/49 [==============================] - 0s 957us/step - loss: 0.2842 - accuracy: 0.9407 - val_loss: 1.7021 - val_accuracy: 0.8673\n",
      "Epoch 116/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2805 - accuracy: 0.9420 - val_loss: 1.6940 - val_accuracy: 0.8699\n",
      "Epoch 117/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2762 - accuracy: 0.9432 - val_loss: 1.7052 - val_accuracy: 0.8648\n",
      "Epoch 118/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2746 - accuracy: 0.9388 - val_loss: 1.7035 - val_accuracy: 0.8597\n",
      "Epoch 119/10000\n",
      "49/49 [==============================] - 0s 689us/step - loss: 0.2710 - accuracy: 0.9439 - val_loss: 1.6917 - val_accuracy: 0.8673\n",
      "Epoch 120/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2642 - accuracy: 0.9439 - val_loss: 1.6924 - val_accuracy: 0.8673\n",
      "Epoch 121/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2609 - accuracy: 0.9445 - val_loss: 1.6974 - val_accuracy: 0.8699\n",
      "Epoch 122/10000\n",
      "49/49 [==============================] - 0s 987us/step - loss: 0.2568 - accuracy: 0.9452 - val_loss: 1.6941 - val_accuracy: 0.8673\n",
      "Epoch 123/10000\n",
      "49/49 [==============================] - 0s 871us/step - loss: 0.2524 - accuracy: 0.9426 - val_loss: 1.6877 - val_accuracy: 0.8699\n",
      "Epoch 124/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2489 - accuracy: 0.9464 - val_loss: 1.6944 - val_accuracy: 0.8622\n",
      "Epoch 125/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2460 - accuracy: 0.9471 - val_loss: 1.6984 - val_accuracy: 0.8648\n",
      "Epoch 126/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2424 - accuracy: 0.9452 - val_loss: 1.6927 - val_accuracy: 0.8673\n",
      "Epoch 127/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2384 - accuracy: 0.9477 - val_loss: 1.6895 - val_accuracy: 0.8724\n",
      "Epoch 128/10000\n",
      "49/49 [==============================] - 0s 875us/step - loss: 0.2350 - accuracy: 0.9471 - val_loss: 1.6958 - val_accuracy: 0.8673\n",
      "Epoch 129/10000\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.2320 - accuracy: 0.9509 - val_loss: 1.7002 - val_accuracy: 0.8673\n",
      "Epoch 130/10000\n",
      "49/49 [==============================] - 0s 799us/step - loss: 0.2269 - accuracy: 0.9503 - val_loss: 1.6943 - val_accuracy: 0.8699\n",
      "Epoch 131/10000\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.2272 - accuracy: 0.9445 - val_loss: 1.6985 - val_accuracy: 0.8673\n",
      "Epoch 132/10000\n",
      "49/49 [==============================] - 0s 782us/step - loss: 0.2225 - accuracy: 0.9483 - val_loss: 1.6928 - val_accuracy: 0.8673\n",
      "Epoch 133/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.2195 - accuracy: 0.9477 - val_loss: 1.6963 - val_accuracy: 0.8724\n",
      "訓練時間：9.11 秒\n",
      "555/555 [==============================] - 0s 636us/step - loss: 1.3207 - accuracy: 0.8503\n",
      "Evaluation on 90% unused data - Loss: 1.3207, Accuracy: 0.8503\n",
      "616/616 [==============================] - 1s 940us/step\n",
      "Test Data MDE report saved to: 40 data per RP transfer revised/DNN 0mcAPs BEST_40data_2025_01_10.json\n",
      "\n",
      "Test Data Mean MDE: 0.3367 meters\n",
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_104 (Dense)           (None, 64)                320       \n",
      "                                                                 \n",
      " dense_105 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_106 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_107 (Dense)           (None, 49)                3185      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20081 (78.44 KB)\n",
      "Trainable params: 3185 (12.44 KB)\n",
      "Non-trainable params: 16896 (66.00 KB)\n",
      "_________________________________________________________________\n",
      "   AP1_Rssi  AP2_Rssi  AP3_Rssi  AP4_Rssi\n",
      "0     -61.0     -75.0     -62.0     -48.0\n",
      "1     -61.0     -75.0     -66.0     -52.0\n",
      "2     -61.0     -75.0     -67.0     -52.0\n",
      "3     -60.0     -75.0     -65.0     -53.0\n",
      "4     -61.0     -76.0     -67.0     -53.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcslab/anaconda3/envs/yang_cuda3/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n",
      "/tmp/ipykernel_3268413/3821025327.py:45: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  test_processed_data = pd.concat([test_processed_data, group], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Set</th>\n",
       "      <th>Validation Set</th>\n",
       "      <th>Test Set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Training Set  Validation Set  Test Set\n",
       "0             32               8       346\n",
       "1             32               8       346\n",
       "2             32               8       346\n",
       "3             32               8       346\n",
       "4             32               8       346\n",
       "5             32               8       346\n",
       "6             32               8       346\n",
       "7             32               8       346\n",
       "8             32               8       346\n",
       "9             32               8       346\n",
       "10            32               8       346\n",
       "11            32               8       346\n",
       "12            32               8       346\n",
       "13            32               8       346\n",
       "14            32               8       346\n",
       "15            32               8       346\n",
       "16            32               8       346\n",
       "17            32               8       346\n",
       "18            32               8       346\n",
       "19            32               8       346\n",
       "20            32               8       346\n",
       "21            32               8       346\n",
       "22            32               8       346\n",
       "23            32               8       346\n",
       "24            32               8       346\n",
       "25            32               8       346\n",
       "26            32               8       346\n",
       "27            32               8       346\n",
       "28            32               8       346\n",
       "29            32               8       346\n",
       "30            32               8       346\n",
       "31            32               8       346\n",
       "32            32               8       346\n",
       "33            32               8       346\n",
       "34            32               8       346\n",
       "35            32               8       346\n",
       "36            32               8       346\n",
       "37            32               8       346\n",
       "38            32               8       346\n",
       "39            32               8       346\n",
       "40            32               8       346\n",
       "41            32               8       346\n",
       "42            32               8       346\n",
       "43            32               8       346\n",
       "44            32               8       346\n",
       "45            32               8       346\n",
       "46            32               8       346\n",
       "47            32               8       346\n",
       "48            32               8       346"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "Epoch 1/10000\n",
      "49/49 [==============================] - 1s 2ms/step - loss: 40.8816 - accuracy: 0.0969 - val_loss: 38.6477 - val_accuracy: 0.0918\n",
      "Epoch 2/10000\n",
      "49/49 [==============================] - 0s 839us/step - loss: 34.8053 - accuracy: 0.1193 - val_loss: 33.1032 - val_accuracy: 0.1046\n",
      "Epoch 3/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 29.9095 - accuracy: 0.1378 - val_loss: 28.7321 - val_accuracy: 0.1327\n",
      "Epoch 4/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 25.9874 - accuracy: 0.1824 - val_loss: 25.2949 - val_accuracy: 0.1990\n",
      "Epoch 5/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 22.8334 - accuracy: 0.2245 - val_loss: 22.4098 - val_accuracy: 0.2347\n",
      "Epoch 6/10000\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 20.2115 - accuracy: 0.2545 - val_loss: 19.9301 - val_accuracy: 0.2500\n",
      "Epoch 7/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 17.9743 - accuracy: 0.2774 - val_loss: 17.7995 - val_accuracy: 0.2883\n",
      "Epoch 8/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 15.9931 - accuracy: 0.3029 - val_loss: 15.8856 - val_accuracy: 0.3036\n",
      "Epoch 9/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 14.1985 - accuracy: 0.3170 - val_loss: 14.1461 - val_accuracy: 0.3240\n",
      "Epoch 10/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 12.5621 - accuracy: 0.3412 - val_loss: 12.6133 - val_accuracy: 0.3316\n",
      "Epoch 11/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 11.1184 - accuracy: 0.3648 - val_loss: 11.2927 - val_accuracy: 0.3724\n",
      "Epoch 12/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 9.8746 - accuracy: 0.3909 - val_loss: 10.0895 - val_accuracy: 0.3776\n",
      "Epoch 13/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 8.7728 - accuracy: 0.4120 - val_loss: 9.0042 - val_accuracy: 0.4082\n",
      "Epoch 14/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 7.7842 - accuracy: 0.4349 - val_loss: 8.0353 - val_accuracy: 0.4260\n",
      "Epoch 15/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 6.9079 - accuracy: 0.4656 - val_loss: 7.2227 - val_accuracy: 0.4388\n",
      "Epoch 16/10000\n",
      "49/49 [==============================] - 0s 767us/step - loss: 6.2050 - accuracy: 0.4968 - val_loss: 6.5247 - val_accuracy: 0.4719\n",
      "Epoch 17/10000\n",
      "49/49 [==============================] - 0s 870us/step - loss: 5.6105 - accuracy: 0.5306 - val_loss: 5.9833 - val_accuracy: 0.4923\n",
      "Epoch 18/10000\n",
      "49/49 [==============================] - 0s 793us/step - loss: 5.1041 - accuracy: 0.5485 - val_loss: 5.4678 - val_accuracy: 0.5102\n",
      "Epoch 19/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 4.6488 - accuracy: 0.5638 - val_loss: 5.0305 - val_accuracy: 0.5408\n",
      "Epoch 20/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 4.2471 - accuracy: 0.5989 - val_loss: 4.6456 - val_accuracy: 0.5663\n",
      "Epoch 21/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 3.9047 - accuracy: 0.6333 - val_loss: 4.3220 - val_accuracy: 0.5944\n",
      "Epoch 22/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 3.5948 - accuracy: 0.6448 - val_loss: 4.0182 - val_accuracy: 0.6148\n",
      "Epoch 23/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 3.3061 - accuracy: 0.6620 - val_loss: 3.7560 - val_accuracy: 0.6148\n",
      "Epoch 24/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 3.0454 - accuracy: 0.6767 - val_loss: 3.5020 - val_accuracy: 0.6301\n",
      "Epoch 25/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 2.8166 - accuracy: 0.6964 - val_loss: 3.2859 - val_accuracy: 0.6531\n",
      "Epoch 26/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 2.5991 - accuracy: 0.7073 - val_loss: 3.0974 - val_accuracy: 0.6684\n",
      "Epoch 27/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 2.4159 - accuracy: 0.7226 - val_loss: 2.9143 - val_accuracy: 0.6811\n",
      "Epoch 28/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 2.2613 - accuracy: 0.7360 - val_loss: 2.7641 - val_accuracy: 0.6990\n",
      "Epoch 29/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 2.1167 - accuracy: 0.7430 - val_loss: 2.6375 - val_accuracy: 0.7143\n",
      "Epoch 30/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 2.0003 - accuracy: 0.7640 - val_loss: 2.5160 - val_accuracy: 0.7321\n",
      "Epoch 31/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.8953 - accuracy: 0.7723 - val_loss: 2.4260 - val_accuracy: 0.7270\n",
      "Epoch 32/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.8038 - accuracy: 0.7800 - val_loss: 2.3379 - val_accuracy: 0.7347\n",
      "Epoch 33/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.7171 - accuracy: 0.7959 - val_loss: 2.2552 - val_accuracy: 0.7372\n",
      "Epoch 34/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.6463 - accuracy: 0.8010 - val_loss: 2.1836 - val_accuracy: 0.7526\n",
      "Epoch 35/10000\n",
      "49/49 [==============================] - 0s 827us/step - loss: 1.5794 - accuracy: 0.8106 - val_loss: 2.1225 - val_accuracy: 0.7500\n",
      "Epoch 36/10000\n",
      "49/49 [==============================] - 0s 936us/step - loss: 1.5219 - accuracy: 0.8119 - val_loss: 2.0633 - val_accuracy: 0.7653\n",
      "Epoch 37/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.4673 - accuracy: 0.8221 - val_loss: 2.0144 - val_accuracy: 0.7704\n",
      "Epoch 38/10000\n",
      "49/49 [==============================] - 0s 879us/step - loss: 1.4189 - accuracy: 0.8284 - val_loss: 1.9659 - val_accuracy: 0.7806\n",
      "Epoch 39/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.3709 - accuracy: 0.8310 - val_loss: 1.9270 - val_accuracy: 0.7704\n",
      "Epoch 40/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.3277 - accuracy: 0.8297 - val_loss: 1.8796 - val_accuracy: 0.7755\n",
      "Epoch 41/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.2812 - accuracy: 0.8361 - val_loss: 1.8499 - val_accuracy: 0.7755\n",
      "Epoch 42/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.2376 - accuracy: 0.8355 - val_loss: 1.8059 - val_accuracy: 0.7755\n",
      "Epoch 43/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.2028 - accuracy: 0.8425 - val_loss: 1.7803 - val_accuracy: 0.7781\n",
      "Epoch 44/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.1643 - accuracy: 0.8469 - val_loss: 1.7411 - val_accuracy: 0.7832\n",
      "Epoch 45/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.1300 - accuracy: 0.8489 - val_loss: 1.7174 - val_accuracy: 0.7857\n",
      "Epoch 46/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.0962 - accuracy: 0.8508 - val_loss: 1.6820 - val_accuracy: 0.7755\n",
      "Epoch 47/10000\n",
      "49/49 [==============================] - 0s 917us/step - loss: 1.0631 - accuracy: 0.8520 - val_loss: 1.6521 - val_accuracy: 0.7908\n",
      "Epoch 48/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.0332 - accuracy: 0.8565 - val_loss: 1.6276 - val_accuracy: 0.7908\n",
      "Epoch 49/10000\n",
      "49/49 [==============================] - 0s 996us/step - loss: 0.9971 - accuracy: 0.8546 - val_loss: 1.6068 - val_accuracy: 0.7883\n",
      "Epoch 50/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.9744 - accuracy: 0.8610 - val_loss: 1.5885 - val_accuracy: 0.8061\n",
      "Epoch 51/10000\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.9466 - accuracy: 0.8597 - val_loss: 1.5708 - val_accuracy: 0.8138\n",
      "Epoch 52/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.9172 - accuracy: 0.8629 - val_loss: 1.5431 - val_accuracy: 0.8112\n",
      "Epoch 53/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.8958 - accuracy: 0.8635 - val_loss: 1.5375 - val_accuracy: 0.8214\n",
      "Epoch 54/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.8735 - accuracy: 0.8648 - val_loss: 1.5180 - val_accuracy: 0.8214\n",
      "Epoch 55/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.8492 - accuracy: 0.8718 - val_loss: 1.5170 - val_accuracy: 0.8240\n",
      "Epoch 56/10000\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.8289 - accuracy: 0.8680 - val_loss: 1.4926 - val_accuracy: 0.8291\n",
      "Epoch 57/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.8045 - accuracy: 0.8763 - val_loss: 1.4733 - val_accuracy: 0.8291\n",
      "Epoch 58/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.7917 - accuracy: 0.8788 - val_loss: 1.4699 - val_accuracy: 0.8214\n",
      "Epoch 59/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.7628 - accuracy: 0.8795 - val_loss: 1.4441 - val_accuracy: 0.8316\n",
      "Epoch 60/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.7476 - accuracy: 0.8782 - val_loss: 1.4412 - val_accuracy: 0.8316\n",
      "Epoch 61/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.7271 - accuracy: 0.8807 - val_loss: 1.4225 - val_accuracy: 0.8418\n",
      "Epoch 62/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.7074 - accuracy: 0.8852 - val_loss: 1.4151 - val_accuracy: 0.8418\n",
      "Epoch 63/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.6879 - accuracy: 0.8833 - val_loss: 1.4085 - val_accuracy: 0.8444\n",
      "Epoch 64/10000\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.6753 - accuracy: 0.8839 - val_loss: 1.4100 - val_accuracy: 0.8469\n",
      "Epoch 65/10000\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.6557 - accuracy: 0.8922 - val_loss: 1.3899 - val_accuracy: 0.8495\n",
      "Epoch 66/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.6411 - accuracy: 0.8890 - val_loss: 1.3842 - val_accuracy: 0.8469\n",
      "Epoch 67/10000\n",
      "49/49 [==============================] - 0s 805us/step - loss: 0.6242 - accuracy: 0.8929 - val_loss: 1.3754 - val_accuracy: 0.8469\n",
      "Epoch 68/10000\n",
      "49/49 [==============================] - 0s 805us/step - loss: 0.6116 - accuracy: 0.8980 - val_loss: 1.3789 - val_accuracy: 0.8469\n",
      "Epoch 69/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.5970 - accuracy: 0.8903 - val_loss: 1.3661 - val_accuracy: 0.8469\n",
      "Epoch 70/10000\n",
      "49/49 [==============================] - 0s 901us/step - loss: 0.5828 - accuracy: 0.8960 - val_loss: 1.3609 - val_accuracy: 0.8469\n",
      "Epoch 71/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.5673 - accuracy: 0.9018 - val_loss: 1.3588 - val_accuracy: 0.8469\n",
      "Epoch 72/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.5541 - accuracy: 0.8967 - val_loss: 1.3423 - val_accuracy: 0.8571\n",
      "Epoch 73/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.5421 - accuracy: 0.9011 - val_loss: 1.3318 - val_accuracy: 0.8546\n",
      "Epoch 74/10000\n",
      "49/49 [==============================] - 0s 811us/step - loss: 0.5299 - accuracy: 0.9005 - val_loss: 1.3305 - val_accuracy: 0.8520\n",
      "Epoch 75/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.5162 - accuracy: 0.9043 - val_loss: 1.3362 - val_accuracy: 0.8520\n",
      "Epoch 76/10000\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.5037 - accuracy: 0.9082 - val_loss: 1.3235 - val_accuracy: 0.8571\n",
      "Epoch 77/10000\n",
      "49/49 [==============================] - 0s 764us/step - loss: 0.4913 - accuracy: 0.9062 - val_loss: 1.3209 - val_accuracy: 0.8571\n",
      "Epoch 78/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.4819 - accuracy: 0.9062 - val_loss: 1.3057 - val_accuracy: 0.8571\n",
      "Epoch 79/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.4695 - accuracy: 0.9088 - val_loss: 1.3150 - val_accuracy: 0.8571\n",
      "Epoch 80/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.4616 - accuracy: 0.9094 - val_loss: 1.3003 - val_accuracy: 0.8597\n",
      "Epoch 81/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.4499 - accuracy: 0.9043 - val_loss: 1.3013 - val_accuracy: 0.8648\n",
      "Epoch 82/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.4439 - accuracy: 0.9133 - val_loss: 1.3070 - val_accuracy: 0.8622\n",
      "Epoch 83/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.4326 - accuracy: 0.9139 - val_loss: 1.2930 - val_accuracy: 0.8648\n",
      "Epoch 84/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.4156 - accuracy: 0.9152 - val_loss: 1.2944 - val_accuracy: 0.8648\n",
      "Epoch 85/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.4100 - accuracy: 0.9190 - val_loss: 1.2974 - val_accuracy: 0.8673\n",
      "Epoch 86/10000\n",
      "49/49 [==============================] - 0s 948us/step - loss: 0.4033 - accuracy: 0.9190 - val_loss: 1.2900 - val_accuracy: 0.8622\n",
      "Epoch 87/10000\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.3934 - accuracy: 0.9177 - val_loss: 1.2910 - val_accuracy: 0.8648\n",
      "Epoch 88/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3834 - accuracy: 0.9209 - val_loss: 1.2922 - val_accuracy: 0.8648\n",
      "Epoch 89/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.3756 - accuracy: 0.9184 - val_loss: 1.2932 - val_accuracy: 0.8648\n",
      "Epoch 90/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3649 - accuracy: 0.9241 - val_loss: 1.2887 - val_accuracy: 0.8648\n",
      "Epoch 91/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3594 - accuracy: 0.9190 - val_loss: 1.2882 - val_accuracy: 0.8673\n",
      "Epoch 92/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.3530 - accuracy: 0.9241 - val_loss: 1.2848 - val_accuracy: 0.8648\n",
      "Epoch 93/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.3433 - accuracy: 0.9209 - val_loss: 1.2882 - val_accuracy: 0.8648\n",
      "Epoch 94/10000\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.3359 - accuracy: 0.9267 - val_loss: 1.2889 - val_accuracy: 0.8648\n",
      "Epoch 95/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3299 - accuracy: 0.9260 - val_loss: 1.2889 - val_accuracy: 0.8648\n",
      "Epoch 96/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.9292 - val_loss: 1.2904 - val_accuracy: 0.8648\n",
      "Epoch 97/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3133 - accuracy: 0.9292 - val_loss: 1.2893 - val_accuracy: 0.8673\n",
      "Epoch 98/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.3045 - accuracy: 0.9292 - val_loss: 1.2904 - val_accuracy: 0.8648\n",
      "Epoch 99/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3007 - accuracy: 0.9260 - val_loss: 1.2895 - val_accuracy: 0.8648\n",
      "Epoch 100/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2936 - accuracy: 0.9286 - val_loss: 1.2907 - val_accuracy: 0.8648\n",
      "Epoch 101/10000\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.2877 - accuracy: 0.9311 - val_loss: 1.2896 - val_accuracy: 0.8673\n",
      "Epoch 102/10000\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2831 - accuracy: 0.9349 - val_loss: 1.2861 - val_accuracy: 0.8673\n",
      "訓練時間：7.95 秒\n",
      "530/530 [==============================] - 0s 805us/step - loss: 1.0996 - accuracy: 0.8751\n",
      "Evaluation on 90% unused data - Loss: 1.0996, Accuracy: 0.8751\n",
      "592/592 [==============================] - 0s 678us/step\n",
      "Test Data MDE report saved to: 40 data per RP transfer revised/DNN 0mcAPs BEST_40data_2025_02_28.json\n",
      "\n",
      "Test Data Mean MDE: 0.2490 meters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcslab/anaconda3/envs/yang_cuda3/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "for i in range(number_of_week):\n",
    "\n",
    "    # 載入模型與標準化器\n",
    "    base_model = load_model(f'{root}/DNN_best_model{ap}_{weekrepresent[i][0]}week_to_{weekrepresent[i][1]}week_{dataamount}dataPerRP.h5')\n",
    "    scaler = joblib.load(f'{root}/scaler_{ap}.pkl')\n",
    "\n",
    "    # 讀取測試資料 2024_12_21   2024_12_27   2025_01_03   2025_01_10   2025_02_28\n",
    "    test_file_path = f\"timestamp_allignment_Balanced_{alldatadate[i]}_rtt_logs.csv\"  # 測試資料的檔案名稱\n",
    "    date_test = f\"{alldatadate[i]}\"\n",
    "    modelname = f\"DNN {ap}s BEST_{dataamount}data_{alldatadate[i]}\"\n",
    "    test_data = pd.read_csv(test_file_path, usecols=selected_columns)\n",
    "    # test_data\n",
    "    \n",
    "\n",
    "    # 凍結所有層\n",
    "    for layer in base_model.layers[:-1]:  # 除了最後一層 (Output Layer)\n",
    "        layer.trainable = False\n",
    "\n",
    "    # 確認哪些層可訓練\n",
    "    base_model.summary()\n",
    "\n",
    "    base_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 資料前處理 (一): 刪除前後n筆資料\n",
    "    n = 10\n",
    "    # 確保依據Label排序\n",
    "    test_data = test_data.sort_values(by=label_column).reset_index(drop=True)\n",
    "\n",
    "    # 建立一個空的 DataFrame 用於存放處理後的資料\n",
    "    test_processed_data = pd.DataFrame(columns=test_data.columns)\n",
    "\n",
    "    # 針對每個Label群組進行處理\n",
    "    for label, group in test_data.groupby(label_column):\n",
    "        # 刪除前n筆和後n筆資料\n",
    "        if len(group) > 2 * n:  # 確保群組資料足夠\n",
    "            group = group.iloc[n:-n]\n",
    "        else:\n",
    "            group = pd.DataFrame()  # 若資料不足，刪除整個群組\n",
    "        # 將處理後的群組資料加入\n",
    "        test_processed_data = pd.concat([test_processed_data, group], ignore_index=True)\n",
    "\n",
    "    # test_processed_data\n",
    "    # Calculate the number of rows with NaN values\n",
    "    nan_rows = test_processed_data.isnull().any(axis=1).sum()\n",
    "\n",
    "    # Print the result\n",
    "    # print(f\"Number of rows with NaN values: {nan_rows}\")\n",
    "\n",
    "    # 找出包含 NaN 的列\n",
    "    rows_with_nan = test_processed_data[test_processed_data.isnull().any(axis=1)]\n",
    "\n",
    "    # # 印出這些列\n",
    "    # print(\"Rows with NaN values:\")\n",
    "    # print(rows_with_nan)\n",
    "    test_data_imputed = test_processed_data.groupby(label_column).apply(\n",
    "        lambda group: group.fillna(group.mean())\n",
    "    ).reset_index()\n",
    "\n",
    "    # Calculate the number of rows with NaN values\n",
    "    nan_rows = test_data_imputed.isnull().any(axis=1).sum()\n",
    "\n",
    "    # Print the result\n",
    "    # print(f\"Number of rows with NaN values: {nan_rows}\")\n",
    "\n",
    "    # 找出包含 NaN 的列\n",
    "    rows_with_nan = test_data_imputed[test_data_imputed.isnull().any(axis=1)]\n",
    "\n",
    "    test_data_imputed\n",
    "\n",
    "    reverse_label_mapping = {v: int(k) - 1 for k, v in label_mapping.items()}  # 讓數字標籤 -1\n",
    "\n",
    "    # 建立 Label 映射\n",
    "    y_test = test_data_imputed[target_column]\n",
    "    y_test_numeric = y_test.map(reverse_label_mapping)\n",
    "\n",
    "    # print(\"Final reverse_label_mapping in DNN:\", reverse_label_mapping)\n",
    "    # print(\"y_numeric unique values in DNN:\", y_test_numeric.unique())\n",
    "\n",
    "    y_test_numeric\n",
    "\n",
    "    # 把label部分拿掉\n",
    "    X_test = test_data_imputed.drop(columns=['level_1','Label'])\n",
    "    print(X_test.head())\n",
    "\n",
    "    # 確保測試資料的特徵與訓練資料的特徵一致\n",
    "    X_test = X_test[X_testing_selected_columns]  # 選取相同的特徵\n",
    "\n",
    "    # print(type(X_test))\n",
    "\n",
    "    # 使用之前訓練時的標準化器 (scaler) 來標準化測試數據\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # 會發現如果用 train_test_split的方法會有資料分布不平均問題，解決辦法如下\n",
    "    from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "\n",
    "    # 轉為 DataFrame 方便操作\n",
    "    data = pd.DataFrame(X_test_scaled)\n",
    "    data['label'] = y_test_numeric  # 加入 label 欄位\n",
    "\n",
    "    # print((data['label'] == 10).sum())  # 直接計算 True 的數量\n",
    "\n",
    "    # 轉為 DataFrame 方便操作\n",
    "    data = pd.DataFrame(X_test_scaled)\n",
    "    data['label'] = y_test_numeric  # 加入 label 欄位\n",
    "\n",
    "    # data\n",
    "    # 儲存訓練集（但這時包含 validation 的部分）\n",
    "    # train_data_full = data.groupby('label', group_keys=False).sample(n=N_train, replace=False, random_state=42)\n",
    "    # 儲存訓練集（確保每個類別只選 1 筆，若某類少於 N_train，則取全部）\n",
    "    # train_data_full = data.groupby('label', group_keys=False).apply(lambda x: x.sample(n=min(N_train, len(x)), replace=False, random_state=42)).reset_index(drop=True)\n",
    "    train_data_full = data.groupby('label', group_keys=False).sample(n=N_train, replace=False) # , random_state=42\n",
    "    # 確保 `train_data_full` 內部的 `label` 數量正確\n",
    "    # print(train_data_full['label'].value_counts())  # 每個類別應該剛好 1 筆\n",
    "    # train_data_full\n",
    "\n",
    "\n",
    "    if N_val > 0:\n",
    "        sss = StratifiedShuffleSplit(n_splits=1, test_size=N_val / N_train) # , random_state=42\n",
    "        train_index, val_index = next(sss.split(train_data_full.drop(columns=['label']), train_data_full['label']))\n",
    "        train_data = train_data_full.iloc[train_index]\n",
    "        val_data = train_data_full.iloc[val_index]\n",
    "        \n",
    "    else:\n",
    "        val_data = pd.DataFrame(columns=data.columns)  # 若沒有 validation data，建立空 DataFrame\\\n",
    "        train_data = train_data_full\n",
    "\n",
    "    # 剩下的資料（未被抽入 train_data_full 的部分）直接作為 test set\n",
    "    remaining_data = data.drop(train_data_full.index)\n",
    "    # print(len(remaining_data))\n",
    "\n",
    "\n",
    "    # **轉換為 NumPy 陣列**\n",
    "    X_train, y_train = train_data.drop(columns=['label']).values, train_data['label'].values\n",
    "    X_val, y_val = val_data.drop(columns=['label']).values, val_data['label'].values\n",
    "    X_test, y_test = remaining_data.drop(columns=['label']).values, remaining_data['label'].values\n",
    "\n",
    "    # **確認數據切分結果**\n",
    "    # print(f\"Training set: {len(X_train)} samples, {len(np.unique(y_train))} unique labels\")\n",
    "    # print(f\"Validation set: {len(X_val)} samples, {len(np.unique(y_val))} unique labels\")\n",
    "    # print(f\"Test set: {len(X_test)} samples, {len(np.unique(y_test))} unique labels\")\n",
    "\n",
    "   # **計算每個 Set 內各 Label 的資料數量**\n",
    "    train_label_counts = pd.Series(y_train).value_counts().sort_index()\n",
    "    val_label_counts = pd.Series(y_val).value_counts().sort_index()\n",
    "    test_label_counts = pd.Series(y_test).value_counts().sort_index()\n",
    "\n",
    "    # **確保所有 Labels 都有出現在三個 Set 裡**\n",
    "    all_labels = sorted(set(train_label_counts.index) | set(val_label_counts.index) | set(test_label_counts.index))\n",
    "    label_distribution = pd.DataFrame(index=all_labels)\n",
    "\n",
    "    label_distribution[\"Training Set\"] = train_label_counts\n",
    "    label_distribution[\"Validation Set\"] = val_label_counts\n",
    "    label_distribution[\"Test Set\"] = test_label_counts\n",
    "\n",
    "    # **用 0 填補缺失值（表示該 Label 在該 Set 中沒有數據）**\n",
    "    label_distribution = label_distribution.fillna(0).astype(int)\n",
    "\n",
    "    from IPython.display import display\n",
    "    display(label_distribution)\n",
    "\n",
    "    import time\n",
    "    # 記錄開始時間\n",
    "    start_time = time.time()\n",
    "\n",
    "\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "    if N_val > 0:\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    else:\n",
    "        early_stop = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "\n",
    "    # 確保變數命名一致\n",
    "    X_train_small = X_train  # 確保這裡用的變數和前面一致\n",
    "    y_train_small = y_train\n",
    "\n",
    "    # 設定 batch_size\n",
    "    batch_size = min(32, max(8, len(X_train_small) // 2))  # 避免 batch size 過大\n",
    "    # batch_size = 32\n",
    "    print(batch_size)\n",
    "\n",
    "    if N_val > 0:\n",
    "        base_model.fit(X_train_small, y_train_small,validation_data=(X_val, y_val), epochs=10000, batch_size=batch_size, callbacks=[early_stop])\n",
    "    else:\n",
    "        base_model.fit(X_train_small, y_train_small, epochs=10000, batch_size=batch_size, callbacks=[early_stop])\n",
    "\n",
    "\n",
    "    # 記錄結束時間\n",
    "    end_time = time.time()\n",
    "\n",
    "\n",
    "    # Needsave\n",
    "    # 計算訓練時間（秒）\n",
    "    training_time = end_time - start_time\n",
    "    print(f\"訓練時間：{training_time:.2f} 秒\")\n",
    "    ALL_trainingtime.append(training_time)\n",
    "\n",
    "    # Needsave\n",
    "    loss, accuracy = base_model.evaluate(X_test, y_test)\n",
    "    print(f\"Evaluation on 90% unused data - Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    ALL_loss.append(loss)\n",
    "    ALL_accuracy.append(accuracy)\n",
    "\n",
    "    # 預測測試資料\n",
    "    y_test_pred_numeric = base_model.predict(X_test_scaled)\n",
    "    y_pred_classes = np.argmax(y_test_pred_numeric, axis=1)\n",
    "\n",
    "    # 轉換為原本的 Label\n",
    "    y_test_pred_labels = [label_mapping[str(num + 1)] for num in y_pred_classes]  # 補回 +1\n",
    "    y_test_pred_labels\n",
    "\n",
    "    # 讀取測試資料的實際 Label\n",
    "    y_test_actual = test_data_imputed[target_column]\n",
    "    test_data_imputed\n",
    "\n",
    "    # 取得預測與實際座標\n",
    "    y_test_pred_coordinates = np.array([label_to_coordinates[label] for label in y_test_pred_labels])\n",
    "    y_test_actual_coordinates = np.array([label_to_coordinates[label] for label in y_test_actual])\n",
    "\n",
    "    # 計算 MDE (Mean Distance Error)\n",
    "    distances = np.linalg.norm(y_test_pred_coordinates - y_test_actual_coordinates, axis=1)\n",
    "    mean_mde = np.mean(distances)\n",
    "\n",
    "    # 記錄每個 RP 的 MDE\n",
    "    mde_report_test = {}\n",
    "    for true_label, distance in zip(y_test_actual, distances):\n",
    "        if true_label not in mde_report_test:\n",
    "            mde_report_test[true_label] = []\n",
    "        mde_report_test[true_label].append(distance)\n",
    "\n",
    "    # 計算測試資料的 MDE 平均值\n",
    "    mde_report_test_avg = {label: {\"mde\": np.mean(dists), \"count\": len(dists)} for label, dists in mde_report_test.items()}\n",
    "\n",
    "    # 儲存 MDE 結果到 JSON 檔案\n",
    "    test_file_path = f\"{root}/{modelname}.json\"\n",
    "    with open(test_file_path, \"w\") as f:\n",
    "        json.dump(mde_report_test_avg, f, indent=4)\n",
    "\n",
    "    # Needsave\n",
    "    print(f\"Test Data MDE report saved to: {test_file_path}\")\n",
    "    print(f\"\\nTest Data Mean MDE: {mean_mde:.4f} meters\")\n",
    "    ALL_mean_mde.append(mean_mde)\n",
    "\n",
    "\n",
    "    base_model.save(f'{root}/DNN_best_model{ap}_{weekrepresent[i+1][0]}week_to_{weekrepresent[i+1][1]}week_{dataamount}dataPerRP.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time\n",
      "[12.42, 17.53, 12.48, 9.11, 7.95, ]\n",
      "Loss\n",
      "[1.1431, 1.1532, 0.9214, 1.3207, 1.0996, ]\n",
      "Accuracy\n",
      "[0.8502, 0.8705, 0.8691, 0.8503, 0.8751, ]\n",
      "MDE\n",
      "[0.4092, 0.3465, 0.2971, 0.3367, 0.2490, ]\n"
     ]
    }
   ],
   "source": [
    "print(\"training time\")\n",
    "print(\"[\",end = '')\n",
    "for t in ALL_trainingtime:\n",
    "    print(f\"{t:.2f}\",end = ', ')\n",
    "print(\"]\")\n",
    "\n",
    "print(\"Loss\")\n",
    "print(\"[\",end = '')\n",
    "for l in ALL_loss:\n",
    "    print(f\"{l:.4f}\",end = ', ')\n",
    "print(\"]\")\n",
    "\n",
    "\n",
    "print(\"Accuracy\")\n",
    "print(\"[\",end = '')\n",
    "for a in ALL_accuracy:\n",
    "    print(f\"{a:.4f}\",end = ', ')\n",
    "print(\"]\")\n",
    "\n",
    "\n",
    "print(\"MDE\")\n",
    "print(\"[\",end = '')\n",
    "for m in ALL_mean_mde:\n",
    "    print(f\"{m:.4f}\",end = ', ')\n",
    "print(\"]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yang_cuda3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
