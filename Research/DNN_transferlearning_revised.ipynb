{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 載入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_coordinates = {\n",
    "    \"1-1\": (0, 0), \"1-2\": (0.6, 0), \"1-3\": (1.2, 0), \"1-4\": (1.8, 0), \"1-5\": (2.4, 0), \"1-6\": (3.0, 0),\"1-7\": (3.6, 0), \"1-8\": (4.2, 0), \"1-9\": (4.8, 0), \"1-10\": (5.4, 0), \"1-11\": (6.0, 0),\n",
    "    \"2-1\": (0, 0.6), \"2-11\": (6.0, 0.6),\n",
    "    \"3-1\": (0, 1.2), \"3-11\": (6.0, 1.2),\n",
    "    \"4-1\": (0, 1.8), \"4-11\": (6.0, 1.8),\n",
    "    \"5-1\": (0, 2.4), \"5-11\": (6.0, 2.4),\n",
    "    \"6-1\": (0, 3.0), \"6-2\": (0.6, 3.0), \"6-3\": (1.2, 3.0), \"6-4\": (1.8, 3.0), \"6-5\": (2.4, 3.0),\"6-6\": (3.0, 3.0), \"6-7\": (3.6, 3.0), \"6-8\": (4.2, 3.0), \"6-9\": (4.8, 3.0), \"6-10\": (5.4, 3.0), \"6-11\": (6.0, 3.0),\n",
    "    \"7-1\": (0, 3.6), \"7-11\": (6.0, 3.6),\n",
    "    \"8-1\": (0, 4.2), \"8-11\": (6.0, 4.2),\n",
    "    \"9-1\": (0, 4.8), \"9-11\": (6.0, 4.8),\n",
    "    \"10-1\": (0, 5.4), \"10-11\": (6.0, 5.4),\n",
    "    \"11-1\": (0, 6.0), \"11-2\": (0.6, 6.0), \"11-3\": (1.2, 6.0), \"11-4\": (1.8, 6.0), \"11-5\": (2.4, 6.0),\"11-6\": (3.0, 6.0), \"11-7\": (3.6, 6.0), \"11-8\": (4.2, 6.0), \"11-9\": (4.8, 6.0), \"11-10\": (5.4, 6.0), \"11-11\": (6.0, 6.0)\n",
    "}\n",
    "label_mapping = {\n",
    "    '11': '1-1','10': '1-2','9': '1-3','8': '1-4','7': '1-5','6': '1-6','5': '1-7','4': '1-8','3': '1-9','2': '1-10','1': '1-11',\n",
    "    '12': '2-1','30': '2-11',\n",
    "    '13': '3-1','29': '3-11',\n",
    "    '14': '4-1','28': '4-11',\n",
    "    '15': '5-1','27': '5-11',\n",
    "    '16': '6-1','17': '6-2','18': '6-3','19': '6-4','20': '6-5','21': '6-6','22': '6-7','23': '6-8','24': '6-9','25': '6-10','26': '6-11',\n",
    "    '49': '7-1','31': '7-11',\n",
    "    '48': '8-1','32': '8-11',\n",
    "    '47': '9-1','33': '9-11',\n",
    "    '46': '10-1','34': '10-11',\n",
    "    '45': '11-1','44': '11-2','43': '11-3','42': '11-4','41': '11-5','40': '11-6','39': '11-7','38': '11-8','37': '11-9','36': '11-10','35': '11-11'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import joblib  # 用於保存模型\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = ['Label',\n",
    "                        'AP1_Distance (mm)','AP4_Distance (mm)',\n",
    "                        'AP1_StdDev (mm)','AP4_StdDev (mm)',\n",
    "                                'AP1_Rssi','AP2_Rssi','AP3_Rssi','AP4_Rssi']  \n",
    "\n",
    "X_testing_selected_columns = [\n",
    "                'AP1_Distance (mm)','AP4_Distance (mm)',\n",
    "                        'AP1_StdDev (mm)','AP4_StdDev (mm)',\n",
    "                                'AP1_Rssi','AP2_Rssi','AP3_Rssi','AP4_Rssi']  \n",
    "\n",
    "label_column = 'Label'\n",
    "target_column = 'Label'\n",
    "\n",
    "\n",
    "#  'AP1_Rssi','AP2_Rssi','AP3_Rssi','AP4_Rssi'\n",
    "# 'AP1_Distance (mm)','AP2_Distance (mm)','AP3_Distance (mm)','AP4_Distance (mm)',\n",
    "# 'AP1_StdDev (mm)','AP2_StdDev (mm)','AP3_StdDev (mm)','AP4_StdDev (mm)',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap = '2mcAP_Best'\n",
    "root = '80 data per RP transfer revised'\n",
    "\n",
    "dataamount = 80\n",
    "N_val = 16\n",
    "\n",
    "N_train = dataamount # 訓練集每個類別至少要有 N_train 筆資料\n",
    "test_val_ratio = 1  # 剩餘資料中，50% 作為驗證集，50% 作為測試集\n",
    "\n",
    "weekrepresent = [['0','0'],['0','1'],['1','2'],['2','3'],['3','4'],['4','5']]\n",
    "alldatadate = ['2024_12_21','2024_12_27','2025_01_03','2025_01_10','2025_02_28']\n",
    "number_of_week = len(alldatadate)\n",
    "\n",
    "ALL_trainingtime = []\n",
    "ALL_loss = []\n",
    "ALL_accuracy = []\n",
    "ALL_mean_mde = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,185</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m576\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m8,320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m)             │         \u001b[38;5;34m3,185\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,339</span> (79.45 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m20,339\u001b[0m (79.45 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,185</span> (12.44 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,185\u001b[0m (12.44 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,152</span> (67.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m17,152\u001b[0m (67.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (12.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2\u001b[0m (12.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\吳定洋\\AppData\\Local\\Temp\\ipykernel_20228\\3920517792.py:45: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  test_processed_data = pd.concat([test_processed_data, group], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   AP1_Distance (mm)  AP4_Distance (mm)  AP1_Rssi  AP2_Rssi  AP3_Rssi  \\\n",
      "0             3960.0            13105.0     -63.0     -74.0     -49.0   \n",
      "1             3999.0            13457.0     -63.0     -74.0     -52.0   \n",
      "2             4038.0            13281.0     -64.0     -75.0     -52.0   \n",
      "3             4194.0            13960.0     -63.0     -74.0     -48.0   \n",
      "4             4189.0            13667.0     -58.0     -73.0     -54.0   \n",
      "\n",
      "   AP4_Rssi  AP1_StdDev (mm)  AP4_StdDev (mm)  \n",
      "0     -65.0           1544.0            949.0  \n",
      "1     -67.0           1511.0           1696.0  \n",
      "2     -69.0           1408.0            999.0  \n",
      "3     -63.0           1005.0            948.0  \n",
      "4     -63.0            995.0            860.0  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Set</th>\n",
       "      <th>Validation Set</th>\n",
       "      <th>Test Set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Training Set  Validation Set  Test Set\n",
       "0             64              16       285\n",
       "1             64              16       285\n",
       "2             64              16       285\n",
       "3             64              16       285\n",
       "4             64              16       285\n",
       "5             64              16       285\n",
       "6             64              16       285\n",
       "7             64              16       285\n",
       "8             64              16       285\n",
       "9             64              16       285\n",
       "10            64              16       285\n",
       "11            64              16       285\n",
       "12            64              16       285\n",
       "13            64              16       285\n",
       "14            64              16       285\n",
       "15            64              16       285\n",
       "16            64              16       285\n",
       "17            64              16       285\n",
       "18            64              16       285\n",
       "19            64              16       285\n",
       "20            64              16       285\n",
       "21            64              16       285\n",
       "22            64              16       285\n",
       "23            64              16       285\n",
       "24            64              16       285\n",
       "25            64              16       285\n",
       "26            64              16       285\n",
       "27            64              16       285\n",
       "28            64              16       285\n",
       "29            64              16       285\n",
       "30            64              16       285\n",
       "31            64              16       285\n",
       "32            64              16       285\n",
       "33            64              16       285\n",
       "34            64              16       285\n",
       "35            64              16       285\n",
       "36            64              16       285\n",
       "37            64              16       285\n",
       "38            64              16       285\n",
       "39            64              16       285\n",
       "40            64              16       285\n",
       "41            64              16       285\n",
       "42            64              16       285\n",
       "43            64              16       285\n",
       "44            64              16       285\n",
       "45            64              16       285\n",
       "46            64              16       285\n",
       "47            64              16       285\n",
       "48            64              16       285"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "Epoch 1/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4966 - loss: 2.6113 - val_accuracy: 0.6339 - val_loss: 1.5220\n",
      "Epoch 2/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6448 - loss: 1.3295 - val_accuracy: 0.7219 - val_loss: 0.9740\n",
      "Epoch 3/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7265 - loss: 0.9013 - val_accuracy: 0.8087 - val_loss: 0.6886\n",
      "Epoch 4/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8168 - loss: 0.6198 - val_accuracy: 0.8737 - val_loss: 0.5309\n",
      "Epoch 5/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8787 - loss: 0.4684 - val_accuracy: 0.8916 - val_loss: 0.4441\n",
      "Epoch 6/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8907 - loss: 0.3961 - val_accuracy: 0.9082 - val_loss: 0.3972\n",
      "Epoch 7/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9178 - loss: 0.3440 - val_accuracy: 0.9209 - val_loss: 0.3622\n",
      "Epoch 8/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9218 - loss: 0.3005 - val_accuracy: 0.9324 - val_loss: 0.3384\n",
      "Epoch 9/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9373 - loss: 0.2546 - val_accuracy: 0.9286 - val_loss: 0.3213\n",
      "Epoch 10/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9500 - loss: 0.2074 - val_accuracy: 0.9362 - val_loss: 0.2932\n",
      "Epoch 11/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9462 - loss: 0.2469 - val_accuracy: 0.9401 - val_loss: 0.2774\n",
      "Epoch 12/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9548 - loss: 0.2196 - val_accuracy: 0.9490 - val_loss: 0.2605\n",
      "Epoch 13/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9658 - loss: 0.1681 - val_accuracy: 0.9541 - val_loss: 0.2531\n",
      "Epoch 14/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9559 - loss: 0.1776 - val_accuracy: 0.9541 - val_loss: 0.2496\n",
      "Epoch 15/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9647 - loss: 0.1660 - val_accuracy: 0.9528 - val_loss: 0.2432\n",
      "Epoch 16/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9726 - loss: 0.1499 - val_accuracy: 0.9554 - val_loss: 0.2374\n",
      "Epoch 17/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9738 - loss: 0.1394 - val_accuracy: 0.9643 - val_loss: 0.2189\n",
      "Epoch 18/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9676 - loss: 0.1480 - val_accuracy: 0.9617 - val_loss: 0.2184\n",
      "Epoch 19/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9666 - loss: 0.1402 - val_accuracy: 0.9643 - val_loss: 0.2097\n",
      "Epoch 20/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9685 - loss: 0.1358 - val_accuracy: 0.9668 - val_loss: 0.2150\n",
      "Epoch 21/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9787 - loss: 0.1158 - val_accuracy: 0.9668 - val_loss: 0.2060\n",
      "Epoch 22/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9785 - loss: 0.0920 - val_accuracy: 0.9656 - val_loss: 0.2045\n",
      "Epoch 23/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9763 - loss: 0.1302 - val_accuracy: 0.9617 - val_loss: 0.2042\n",
      "Epoch 24/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9768 - loss: 0.1132 - val_accuracy: 0.9656 - val_loss: 0.2021\n",
      "Epoch 25/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9801 - loss: 0.0984 - val_accuracy: 0.9681 - val_loss: 0.1926\n",
      "Epoch 26/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9760 - loss: 0.1093 - val_accuracy: 0.9681 - val_loss: 0.1942\n",
      "Epoch 27/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9784 - loss: 0.0940 - val_accuracy: 0.9694 - val_loss: 0.1906\n",
      "Epoch 28/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9817 - loss: 0.0924 - val_accuracy: 0.9681 - val_loss: 0.1867\n",
      "Epoch 29/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9796 - loss: 0.0933 - val_accuracy: 0.9630 - val_loss: 0.1999\n",
      "Epoch 30/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9808 - loss: 0.0886 - val_accuracy: 0.9707 - val_loss: 0.1879\n",
      "Epoch 31/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9766 - loss: 0.0926 - val_accuracy: 0.9668 - val_loss: 0.1891\n",
      "Epoch 32/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9806 - loss: 0.0911 - val_accuracy: 0.9681 - val_loss: 0.1871\n",
      "Epoch 33/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9821 - loss: 0.0772 - val_accuracy: 0.9681 - val_loss: 0.1894\n",
      "Epoch 34/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9830 - loss: 0.0986 - val_accuracy: 0.9694 - val_loss: 0.1806\n",
      "Epoch 35/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9809 - loss: 0.0788 - val_accuracy: 0.9707 - val_loss: 0.1809\n",
      "Epoch 36/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9801 - loss: 0.0775 - val_accuracy: 0.9656 - val_loss: 0.1820\n",
      "Epoch 37/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9856 - loss: 0.0580 - val_accuracy: 0.9732 - val_loss: 0.1756\n",
      "Epoch 38/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9840 - loss: 0.0774 - val_accuracy: 0.9694 - val_loss: 0.1780\n",
      "Epoch 39/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9861 - loss: 0.0664 - val_accuracy: 0.9694 - val_loss: 0.1786\n",
      "Epoch 40/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9827 - loss: 0.0670 - val_accuracy: 0.9719 - val_loss: 0.1738\n",
      "Epoch 41/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9822 - loss: 0.0691 - val_accuracy: 0.9719 - val_loss: 0.1788\n",
      "Epoch 42/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9845 - loss: 0.0591 - val_accuracy: 0.9694 - val_loss: 0.1758\n",
      "Epoch 43/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9875 - loss: 0.0696 - val_accuracy: 0.9694 - val_loss: 0.1764\n",
      "Epoch 44/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9793 - loss: 0.0685 - val_accuracy: 0.9719 - val_loss: 0.1758\n",
      "Epoch 45/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9858 - loss: 0.0632 - val_accuracy: 0.9707 - val_loss: 0.1767\n",
      "Epoch 46/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9862 - loss: 0.0594 - val_accuracy: 0.9707 - val_loss: 0.1713\n",
      "Epoch 47/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9807 - loss: 0.0570 - val_accuracy: 0.9656 - val_loss: 0.1737\n",
      "Epoch 48/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9874 - loss: 0.0577 - val_accuracy: 0.9707 - val_loss: 0.1657\n",
      "Epoch 49/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9858 - loss: 0.0529 - val_accuracy: 0.9707 - val_loss: 0.1793\n",
      "Epoch 50/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9882 - loss: 0.0546 - val_accuracy: 0.9694 - val_loss: 0.1693\n",
      "Epoch 51/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9874 - loss: 0.0526 - val_accuracy: 0.9732 - val_loss: 0.1686\n",
      "Epoch 52/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9864 - loss: 0.0605 - val_accuracy: 0.9694 - val_loss: 0.1731\n",
      "Epoch 53/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9887 - loss: 0.0515 - val_accuracy: 0.9732 - val_loss: 0.1712\n",
      "Epoch 54/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9874 - loss: 0.0538 - val_accuracy: 0.9707 - val_loss: 0.1715\n",
      "Epoch 55/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9852 - loss: 0.0494 - val_accuracy: 0.9732 - val_loss: 0.1765\n",
      "Epoch 56/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9881 - loss: 0.0450 - val_accuracy: 0.9681 - val_loss: 0.1748\n",
      "Epoch 57/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9900 - loss: 0.0480 - val_accuracy: 0.9732 - val_loss: 0.1658\n",
      "Epoch 58/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9898 - loss: 0.0428 - val_accuracy: 0.9732 - val_loss: 0.1641\n",
      "Epoch 59/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9923 - loss: 0.0415 - val_accuracy: 0.9719 - val_loss: 0.1680\n",
      "Epoch 60/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9898 - loss: 0.0369 - val_accuracy: 0.9707 - val_loss: 0.1677\n",
      "Epoch 61/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9913 - loss: 0.0413 - val_accuracy: 0.9694 - val_loss: 0.1726\n",
      "Epoch 62/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9935 - loss: 0.0408 - val_accuracy: 0.9707 - val_loss: 0.1707\n",
      "Epoch 63/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9922 - loss: 0.0395 - val_accuracy: 0.9694 - val_loss: 0.1648\n",
      "Epoch 64/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9902 - loss: 0.0421 - val_accuracy: 0.9732 - val_loss: 0.1686\n",
      "Epoch 65/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9921 - loss: 0.0394 - val_accuracy: 0.9707 - val_loss: 0.1627\n",
      "Epoch 66/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9910 - loss: 0.0437 - val_accuracy: 0.9732 - val_loss: 0.1674\n",
      "Epoch 67/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9911 - loss: 0.0404 - val_accuracy: 0.9719 - val_loss: 0.1677\n",
      "Epoch 68/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9954 - loss: 0.0351 - val_accuracy: 0.9707 - val_loss: 0.1675\n",
      "Epoch 69/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9945 - loss: 0.0315 - val_accuracy: 0.9745 - val_loss: 0.1635\n",
      "Epoch 70/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9944 - loss: 0.0354 - val_accuracy: 0.9745 - val_loss: 0.1630\n",
      "Epoch 71/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9915 - loss: 0.0382 - val_accuracy: 0.9707 - val_loss: 0.1616\n",
      "Epoch 72/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9915 - loss: 0.0385 - val_accuracy: 0.9719 - val_loss: 0.1674\n",
      "Epoch 73/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9933 - loss: 0.0340 - val_accuracy: 0.9732 - val_loss: 0.1662\n",
      "Epoch 74/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9922 - loss: 0.0338 - val_accuracy: 0.9732 - val_loss: 0.1603\n",
      "Epoch 75/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9928 - loss: 0.0320 - val_accuracy: 0.9694 - val_loss: 0.1784\n",
      "Epoch 76/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9939 - loss: 0.0365 - val_accuracy: 0.9681 - val_loss: 0.1787\n",
      "Epoch 77/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9909 - loss: 0.0361 - val_accuracy: 0.9732 - val_loss: 0.1677\n",
      "Epoch 78/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9928 - loss: 0.0298 - val_accuracy: 0.9719 - val_loss: 0.1638\n",
      "Epoch 79/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9939 - loss: 0.0284 - val_accuracy: 0.9707 - val_loss: 0.1666\n",
      "Epoch 80/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9927 - loss: 0.0330 - val_accuracy: 0.9719 - val_loss: 0.1678\n",
      "Epoch 81/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9944 - loss: 0.0271 - val_accuracy: 0.9719 - val_loss: 0.1804\n",
      "Epoch 82/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9913 - loss: 0.0319 - val_accuracy: 0.9732 - val_loss: 0.1745\n",
      "Epoch 83/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9935 - loss: 0.0305 - val_accuracy: 0.9745 - val_loss: 0.1729\n",
      "Epoch 84/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9917 - loss: 0.0300 - val_accuracy: 0.9719 - val_loss: 0.1665\n",
      "訓練時間：16.12 秒\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 904us/step - accuracy: 0.9782 - loss: 0.1048\n",
      "Evaluation on 90% unused data - Loss: 0.1039, Accuracy: 0.9808\n",
      "\u001b[1m559/559\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 611us/step\n",
      "Test Data MDE report saved to: 80 data per RP transfer revised/DNN 2mcAP_Bests BEST_80data_2024_12_21.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Test Data Mean MDE: 0.0285 meters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,185</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m576\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m8,320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m)             │         \u001b[38;5;34m3,185\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,339</span> (79.45 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m20,339\u001b[0m (79.45 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,185</span> (12.44 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,185\u001b[0m (12.44 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,152</span> (67.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m17,152\u001b[0m (67.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (12.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2\u001b[0m (12.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\吳定洋\\AppData\\Local\\Temp\\ipykernel_20228\\3920517792.py:45: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  test_processed_data = pd.concat([test_processed_data, group], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   AP1_Distance (mm)  AP4_Distance (mm)  AP1_Rssi  AP2_Rssi  AP3_Rssi  \\\n",
      "0             4385.0            12344.0     -54.0     -72.0     -45.0   \n",
      "1             3843.0            12461.0     -62.0     -72.0     -43.0   \n",
      "2             3764.0            12822.0     -57.0     -72.0     -45.0   \n",
      "3             3804.0            12402.0     -62.0     -71.0     -47.0   \n",
      "4             3960.0            12490.0     -62.0     -72.0     -47.0   \n",
      "\n",
      "   AP4_Rssi  AP1_StdDev (mm)  AP4_StdDev (mm)  \n",
      "0     -62.0           2279.0           1497.0  \n",
      "1     -64.0           1185.0           1941.0  \n",
      "2     -62.0           1159.0           1254.0  \n",
      "3     -62.0           1161.0           2073.0  \n",
      "4     -64.0           1069.0           1913.0  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Set</th>\n",
       "      <th>Validation Set</th>\n",
       "      <th>Test Set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Training Set  Validation Set  Test Set\n",
       "0             64              16       336\n",
       "1             64              16       336\n",
       "2             64              16       336\n",
       "3             64              16       336\n",
       "4             64              16       336\n",
       "5             64              16       336\n",
       "6             64              16       336\n",
       "7             64              16       336\n",
       "8             64              16       336\n",
       "9             64              16       336\n",
       "10            64              16       336\n",
       "11            64              16       336\n",
       "12            64              16       336\n",
       "13            64              16       336\n",
       "14            64              16       336\n",
       "15            64              16       336\n",
       "16            64              16       336\n",
       "17            64              16       336\n",
       "18            64              16       336\n",
       "19            64              16       336\n",
       "20            64              16       336\n",
       "21            64              16       336\n",
       "22            64              16       336\n",
       "23            64              16       336\n",
       "24            64              16       336\n",
       "25            64              16       336\n",
       "26            64              16       336\n",
       "27            64              16       336\n",
       "28            64              16       336\n",
       "29            64              16       336\n",
       "30            64              16       336\n",
       "31            64              16       336\n",
       "32            64              16       336\n",
       "33            64              16       336\n",
       "34            64              16       336\n",
       "35            64              16       336\n",
       "36            64              16       336\n",
       "37            64              16       336\n",
       "38            64              16       336\n",
       "39            64              16       336\n",
       "40            64              16       336\n",
       "41            64              16       336\n",
       "42            64              16       336\n",
       "43            64              16       336\n",
       "44            64              16       336\n",
       "45            64              16       336\n",
       "46            64              16       336\n",
       "47            64              16       336\n",
       "48            64              16       336"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "Epoch 1/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.3692 - loss: 5.2626 - val_accuracy: 0.4503 - val_loss: 3.3103\n",
      "Epoch 2/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4927 - loss: 2.9420 - val_accuracy: 0.6033 - val_loss: 2.0191\n",
      "Epoch 3/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6348 - loss: 1.7234 - val_accuracy: 0.7283 - val_loss: 1.3257\n",
      "Epoch 4/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7497 - loss: 1.1082 - val_accuracy: 0.7857 - val_loss: 0.9486\n",
      "Epoch 5/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8102 - loss: 0.7847 - val_accuracy: 0.8444 - val_loss: 0.7299\n",
      "Epoch 6/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8571 - loss: 0.5645 - val_accuracy: 0.8827 - val_loss: 0.6138\n",
      "Epoch 7/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8916 - loss: 0.4453 - val_accuracy: 0.9031 - val_loss: 0.5338\n",
      "Epoch 8/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9154 - loss: 0.3410 - val_accuracy: 0.9031 - val_loss: 0.4967\n",
      "Epoch 9/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9232 - loss: 0.3397 - val_accuracy: 0.9298 - val_loss: 0.4395\n",
      "Epoch 10/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9350 - loss: 0.4297 - val_accuracy: 0.9388 - val_loss: 0.4130\n",
      "Epoch 11/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9430 - loss: 0.2704 - val_accuracy: 0.9286 - val_loss: 0.3992\n",
      "Epoch 12/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9439 - loss: 0.2309 - val_accuracy: 0.9464 - val_loss: 0.3726\n",
      "Epoch 13/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9640 - loss: 0.1872 - val_accuracy: 0.9464 - val_loss: 0.3581\n",
      "Epoch 14/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9655 - loss: 0.1605 - val_accuracy: 0.9528 - val_loss: 0.3429\n",
      "Epoch 15/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9622 - loss: 0.2173 - val_accuracy: 0.9503 - val_loss: 0.3273\n",
      "Epoch 16/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9667 - loss: 0.1596 - val_accuracy: 0.9541 - val_loss: 0.3173\n",
      "Epoch 17/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9635 - loss: 0.1821 - val_accuracy: 0.9554 - val_loss: 0.3092\n",
      "Epoch 18/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9697 - loss: 0.1328 - val_accuracy: 0.9579 - val_loss: 0.3044\n",
      "Epoch 19/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9689 - loss: 0.1280 - val_accuracy: 0.9541 - val_loss: 0.2979\n",
      "Epoch 20/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9700 - loss: 0.1208 - val_accuracy: 0.9617 - val_loss: 0.2891\n",
      "Epoch 21/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9733 - loss: 0.0997 - val_accuracy: 0.9617 - val_loss: 0.2848\n",
      "Epoch 22/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9778 - loss: 0.0945 - val_accuracy: 0.9554 - val_loss: 0.2835\n",
      "Epoch 23/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9775 - loss: 0.1186 - val_accuracy: 0.9656 - val_loss: 0.2742\n",
      "Epoch 24/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9788 - loss: 0.1306 - val_accuracy: 0.9643 - val_loss: 0.2730\n",
      "Epoch 25/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9763 - loss: 0.1305 - val_accuracy: 0.9605 - val_loss: 0.2662\n",
      "Epoch 26/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9802 - loss: 0.0884 - val_accuracy: 0.9617 - val_loss: 0.2675\n",
      "Epoch 27/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9813 - loss: 0.0878 - val_accuracy: 0.9643 - val_loss: 0.2549\n",
      "Epoch 28/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9834 - loss: 0.0909 - val_accuracy: 0.9630 - val_loss: 0.2602\n",
      "Epoch 29/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9791 - loss: 0.0689 - val_accuracy: 0.9694 - val_loss: 0.2573\n",
      "Epoch 30/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9837 - loss: 0.0716 - val_accuracy: 0.9681 - val_loss: 0.2508\n",
      "Epoch 31/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9832 - loss: 0.1077 - val_accuracy: 0.9732 - val_loss: 0.2415\n",
      "Epoch 32/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9784 - loss: 0.0803 - val_accuracy: 0.9719 - val_loss: 0.2410\n",
      "Epoch 33/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9865 - loss: 0.0734 - val_accuracy: 0.9732 - val_loss: 0.2413\n",
      "Epoch 34/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9905 - loss: 0.0580 - val_accuracy: 0.9732 - val_loss: 0.2483\n",
      "Epoch 35/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9864 - loss: 0.0772 - val_accuracy: 0.9732 - val_loss: 0.2356\n",
      "Epoch 36/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9872 - loss: 0.0693 - val_accuracy: 0.9707 - val_loss: 0.2411\n",
      "Epoch 37/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9922 - loss: 0.0513 - val_accuracy: 0.9732 - val_loss: 0.2389\n",
      "Epoch 38/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9905 - loss: 0.0447 - val_accuracy: 0.9758 - val_loss: 0.2319\n",
      "Epoch 39/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9877 - loss: 0.0620 - val_accuracy: 0.9694 - val_loss: 0.2329\n",
      "Epoch 40/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9915 - loss: 0.0443 - val_accuracy: 0.9745 - val_loss: 0.2346\n",
      "Epoch 41/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9882 - loss: 0.0449 - val_accuracy: 0.9719 - val_loss: 0.2295\n",
      "Epoch 42/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9915 - loss: 0.0407 - val_accuracy: 0.9732 - val_loss: 0.2285\n",
      "Epoch 43/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9887 - loss: 0.0518 - val_accuracy: 0.9732 - val_loss: 0.2297\n",
      "Epoch 44/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9948 - loss: 0.0322 - val_accuracy: 0.9745 - val_loss: 0.2286\n",
      "Epoch 45/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9946 - loss: 0.0304 - val_accuracy: 0.9719 - val_loss: 0.2239\n",
      "Epoch 46/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9917 - loss: 0.0425 - val_accuracy: 0.9719 - val_loss: 0.2209\n",
      "Epoch 47/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9942 - loss: 0.0376 - val_accuracy: 0.9758 - val_loss: 0.2187\n",
      "Epoch 48/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9942 - loss: 0.0327 - val_accuracy: 0.9719 - val_loss: 0.2302\n",
      "Epoch 49/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9932 - loss: 0.0359 - val_accuracy: 0.9758 - val_loss: 0.2188\n",
      "Epoch 50/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9951 - loss: 0.0298 - val_accuracy: 0.9719 - val_loss: 0.2234\n",
      "Epoch 51/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9974 - loss: 0.0248 - val_accuracy: 0.9707 - val_loss: 0.2281\n",
      "Epoch 52/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9932 - loss: 0.0314 - val_accuracy: 0.9745 - val_loss: 0.2253\n",
      "Epoch 53/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9946 - loss: 0.0246 - val_accuracy: 0.9758 - val_loss: 0.2199\n",
      "Epoch 54/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9971 - loss: 0.0233 - val_accuracy: 0.9758 - val_loss: 0.2254\n",
      "Epoch 55/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9956 - loss: 0.0311 - val_accuracy: 0.9770 - val_loss: 0.2197\n",
      "Epoch 56/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9955 - loss: 0.0290 - val_accuracy: 0.9732 - val_loss: 0.2263\n",
      "Epoch 57/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9970 - loss: 0.0260 - val_accuracy: 0.9745 - val_loss: 0.2206\n",
      "訓練時間：10.83 秒\n",
      "\u001b[1m515/515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 887us/step - accuracy: 0.9809 - loss: 0.1008\n",
      "Evaluation on 90% unused data - Loss: 0.1189, Accuracy: 0.9772\n",
      "\u001b[1m637/637\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 570us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data MDE report saved to: 80 data per RP transfer revised/DNN 2mcAP_Bests BEST_80data_2024_12_27.json\n",
      "\n",
      "Test Data Mean MDE: 0.0324 meters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,185</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m576\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m8,320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m)             │         \u001b[38;5;34m3,185\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,339</span> (79.45 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m20,339\u001b[0m (79.45 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,185</span> (12.44 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,185\u001b[0m (12.44 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,152</span> (67.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m17,152\u001b[0m (67.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (12.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2\u001b[0m (12.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\吳定洋\\AppData\\Local\\Temp\\ipykernel_20228\\3920517792.py:45: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  test_processed_data = pd.concat([test_processed_data, group], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   AP1_Distance (mm)  AP4_Distance (mm)  AP1_Rssi  AP2_Rssi  AP3_Rssi  \\\n",
      "0             4385.0            12054.0     -53.0     -71.0     -47.0   \n",
      "1             3999.0            10518.0     -61.0     -71.0     -47.0   \n",
      "2             4116.0            10440.0     -60.0     -71.0     -45.0   \n",
      "3             3999.0            10478.0     -61.0     -71.0     -47.0   \n",
      "4             4038.0            10479.0     -61.0     -71.0     -47.0   \n",
      "\n",
      "   AP4_Rssi  AP1_StdDev (mm)  AP4_StdDev (mm)  \n",
      "0     -60.0           2406.0           1566.0  \n",
      "1     -64.0           1083.0           1001.0  \n",
      "2     -64.0           1073.0           1253.0  \n",
      "3     -65.0           1161.0           1177.0  \n",
      "4     -64.0           1029.0           1131.0  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Set</th>\n",
       "      <th>Validation Set</th>\n",
       "      <th>Test Set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Training Set  Validation Set  Test Set\n",
       "0             64              16       318\n",
       "1             64              16       318\n",
       "2             64              16       318\n",
       "3             64              16       318\n",
       "4             64              16       318\n",
       "5             64              16       318\n",
       "6             64              16       318\n",
       "7             64              16       318\n",
       "8             64              16       318\n",
       "9             64              16       318\n",
       "10            64              16       318\n",
       "11            64              16       318\n",
       "12            64              16       318\n",
       "13            64              16       318\n",
       "14            64              16       318\n",
       "15            64              16       318\n",
       "16            64              16       318\n",
       "17            64              16       318\n",
       "18            64              16       318\n",
       "19            64              16       318\n",
       "20            64              16       318\n",
       "21            64              16       318\n",
       "22            64              16       318\n",
       "23            64              16       318\n",
       "24            64              16       318\n",
       "25            64              16       318\n",
       "26            64              16       318\n",
       "27            64              16       318\n",
       "28            64              16       318\n",
       "29            64              16       318\n",
       "30            64              16       318\n",
       "31            64              16       318\n",
       "32            64              16       318\n",
       "33            64              16       318\n",
       "34            64              16       318\n",
       "35            64              16       318\n",
       "36            64              16       318\n",
       "37            64              16       318\n",
       "38            64              16       318\n",
       "39            64              16       318\n",
       "40            64              16       318\n",
       "41            64              16       318\n",
       "42            64              16       318\n",
       "43            64              16       318\n",
       "44            64              16       318\n",
       "45            64              16       318\n",
       "46            64              16       318\n",
       "47            64              16       318\n",
       "48            64              16       318"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "Epoch 1/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4596 - loss: 4.1686 - val_accuracy: 0.5944 - val_loss: 2.4499\n",
      "Epoch 2/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5754 - loss: 2.3259 - val_accuracy: 0.6582 - val_loss: 1.4674\n",
      "Epoch 3/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6656 - loss: 1.4011 - val_accuracy: 0.7640 - val_loss: 0.9033\n",
      "Epoch 4/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7599 - loss: 0.9362 - val_accuracy: 0.7985 - val_loss: 0.6435\n",
      "Epoch 5/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8283 - loss: 0.6713 - val_accuracy: 0.8712 - val_loss: 0.4764\n",
      "Epoch 6/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8629 - loss: 0.5454 - val_accuracy: 0.8827 - val_loss: 0.4071\n",
      "Epoch 7/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8961 - loss: 0.4265 - val_accuracy: 0.9043 - val_loss: 0.3319\n",
      "Epoch 8/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9130 - loss: 0.3567 - val_accuracy: 0.9311 - val_loss: 0.2908\n",
      "Epoch 9/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9361 - loss: 0.2684 - val_accuracy: 0.9286 - val_loss: 0.2629\n",
      "Epoch 10/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9316 - loss: 0.2936 - val_accuracy: 0.9388 - val_loss: 0.2443\n",
      "Epoch 11/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9568 - loss: 0.2168 - val_accuracy: 0.9464 - val_loss: 0.2267\n",
      "Epoch 12/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9435 - loss: 0.2510 - val_accuracy: 0.9503 - val_loss: 0.2237\n",
      "Epoch 13/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9558 - loss: 0.2293 - val_accuracy: 0.9528 - val_loss: 0.2003\n",
      "Epoch 14/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9625 - loss: 0.1745 - val_accuracy: 0.9592 - val_loss: 0.1958\n",
      "Epoch 15/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9578 - loss: 0.1957 - val_accuracy: 0.9554 - val_loss: 0.1852\n",
      "Epoch 16/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9620 - loss: 0.1593 - val_accuracy: 0.9579 - val_loss: 0.1902\n",
      "Epoch 17/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9626 - loss: 0.1812 - val_accuracy: 0.9656 - val_loss: 0.1658\n",
      "Epoch 18/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9676 - loss: 0.1619 - val_accuracy: 0.9668 - val_loss: 0.1660\n",
      "Epoch 19/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9662 - loss: 0.1589 - val_accuracy: 0.9707 - val_loss: 0.1554\n",
      "Epoch 20/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9736 - loss: 0.1415 - val_accuracy: 0.9707 - val_loss: 0.1561\n",
      "Epoch 21/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9771 - loss: 0.1152 - val_accuracy: 0.9719 - val_loss: 0.1524\n",
      "Epoch 22/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9734 - loss: 0.1305 - val_accuracy: 0.9719 - val_loss: 0.1428\n",
      "Epoch 23/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9769 - loss: 0.1128 - val_accuracy: 0.9770 - val_loss: 0.1408\n",
      "Epoch 24/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9715 - loss: 0.1185 - val_accuracy: 0.9745 - val_loss: 0.1321\n",
      "Epoch 25/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9749 - loss: 0.1244 - val_accuracy: 0.9796 - val_loss: 0.1346\n",
      "Epoch 26/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9755 - loss: 0.1139 - val_accuracy: 0.9758 - val_loss: 0.1312\n",
      "Epoch 27/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9799 - loss: 0.1039 - val_accuracy: 0.9770 - val_loss: 0.1280\n",
      "Epoch 28/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9783 - loss: 0.1028 - val_accuracy: 0.9783 - val_loss: 0.1323\n",
      "Epoch 29/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9775 - loss: 0.0999 - val_accuracy: 0.9770 - val_loss: 0.1314\n",
      "Epoch 30/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9753 - loss: 0.1254 - val_accuracy: 0.9783 - val_loss: 0.1256\n",
      "Epoch 31/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9780 - loss: 0.1257 - val_accuracy: 0.9821 - val_loss: 0.1258\n",
      "Epoch 32/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9826 - loss: 0.0776 - val_accuracy: 0.9783 - val_loss: 0.1258\n",
      "Epoch 33/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9831 - loss: 0.0757 - val_accuracy: 0.9745 - val_loss: 0.1199\n",
      "Epoch 34/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9760 - loss: 0.0854 - val_accuracy: 0.9809 - val_loss: 0.1179\n",
      "Epoch 35/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9858 - loss: 0.0732 - val_accuracy: 0.9770 - val_loss: 0.1173\n",
      "Epoch 36/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9840 - loss: 0.0778 - val_accuracy: 0.9770 - val_loss: 0.1194\n",
      "Epoch 37/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9850 - loss: 0.0609 - val_accuracy: 0.9758 - val_loss: 0.1129\n",
      "Epoch 38/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9818 - loss: 0.0890 - val_accuracy: 0.9783 - val_loss: 0.1125\n",
      "Epoch 39/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9841 - loss: 0.0649 - val_accuracy: 0.9783 - val_loss: 0.1183\n",
      "Epoch 40/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9848 - loss: 0.0750 - val_accuracy: 0.9758 - val_loss: 0.1154\n",
      "Epoch 41/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9898 - loss: 0.0513 - val_accuracy: 0.9783 - val_loss: 0.1150\n",
      "Epoch 42/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9883 - loss: 0.0584 - val_accuracy: 0.9783 - val_loss: 0.1071\n",
      "Epoch 43/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9857 - loss: 0.0764 - val_accuracy: 0.9796 - val_loss: 0.1096\n",
      "Epoch 44/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9829 - loss: 0.0664 - val_accuracy: 0.9809 - val_loss: 0.1057\n",
      "Epoch 45/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9832 - loss: 0.0543 - val_accuracy: 0.9783 - val_loss: 0.1105\n",
      "Epoch 46/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9891 - loss: 0.0500 - val_accuracy: 0.9758 - val_loss: 0.1114\n",
      "Epoch 47/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9910 - loss: 0.0457 - val_accuracy: 0.9796 - val_loss: 0.1037\n",
      "Epoch 48/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9913 - loss: 0.0416 - val_accuracy: 0.9745 - val_loss: 0.1096\n",
      "Epoch 49/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9853 - loss: 0.0553 - val_accuracy: 0.9783 - val_loss: 0.1081\n",
      "Epoch 50/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9904 - loss: 0.0447 - val_accuracy: 0.9809 - val_loss: 0.1024\n",
      "Epoch 51/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9899 - loss: 0.0460 - val_accuracy: 0.9758 - val_loss: 0.1065\n",
      "Epoch 52/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9854 - loss: 0.0492 - val_accuracy: 0.9770 - val_loss: 0.1042\n",
      "Epoch 53/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9893 - loss: 0.0488 - val_accuracy: 0.9758 - val_loss: 0.1108\n",
      "Epoch 54/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9917 - loss: 0.0378 - val_accuracy: 0.9783 - val_loss: 0.1028\n",
      "Epoch 55/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9929 - loss: 0.0343 - val_accuracy: 0.9783 - val_loss: 0.0996\n",
      "Epoch 56/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9869 - loss: 0.0544 - val_accuracy: 0.9796 - val_loss: 0.1020\n",
      "Epoch 57/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9911 - loss: 0.0358 - val_accuracy: 0.9770 - val_loss: 0.0972\n",
      "Epoch 58/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9875 - loss: 0.0454 - val_accuracy: 0.9783 - val_loss: 0.1059\n",
      "Epoch 59/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9933 - loss: 0.0338 - val_accuracy: 0.9783 - val_loss: 0.1009\n",
      "Epoch 60/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9909 - loss: 0.0382 - val_accuracy: 0.9783 - val_loss: 0.0981\n",
      "Epoch 61/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9907 - loss: 0.0398 - val_accuracy: 0.9758 - val_loss: 0.0962\n",
      "Epoch 62/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9938 - loss: 0.0273 - val_accuracy: 0.9758 - val_loss: 0.1047\n",
      "Epoch 63/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9940 - loss: 0.0318 - val_accuracy: 0.9783 - val_loss: 0.0995\n",
      "Epoch 64/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9921 - loss: 0.0383 - val_accuracy: 0.9796 - val_loss: 0.0977\n",
      "Epoch 65/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9934 - loss: 0.0336 - val_accuracy: 0.9809 - val_loss: 0.0966\n",
      "Epoch 66/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9928 - loss: 0.0305 - val_accuracy: 0.9783 - val_loss: 0.0950\n",
      "Epoch 67/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9959 - loss: 0.0246 - val_accuracy: 0.9796 - val_loss: 0.0912\n",
      "Epoch 68/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9929 - loss: 0.0272 - val_accuracy: 0.9809 - val_loss: 0.0972\n",
      "Epoch 69/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9949 - loss: 0.0247 - val_accuracy: 0.9821 - val_loss: 0.0934\n",
      "Epoch 70/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9922 - loss: 0.0334 - val_accuracy: 0.9796 - val_loss: 0.0976\n",
      "Epoch 71/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9937 - loss: 0.0273 - val_accuracy: 0.9783 - val_loss: 0.0973\n",
      "Epoch 72/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9949 - loss: 0.0232 - val_accuracy: 0.9809 - val_loss: 0.0935\n",
      "Epoch 73/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9946 - loss: 0.0223 - val_accuracy: 0.9809 - val_loss: 0.0976\n",
      "Epoch 74/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9936 - loss: 0.0257 - val_accuracy: 0.9796 - val_loss: 0.0907\n",
      "Epoch 75/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9925 - loss: 0.0244 - val_accuracy: 0.9821 - val_loss: 0.0949\n",
      "Epoch 76/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9916 - loss: 0.0286 - val_accuracy: 0.9809 - val_loss: 0.0905\n",
      "Epoch 77/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9933 - loss: 0.0224 - val_accuracy: 0.9809 - val_loss: 0.0916\n",
      "Epoch 78/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9945 - loss: 0.0235 - val_accuracy: 0.9809 - val_loss: 0.0917\n",
      "Epoch 79/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9951 - loss: 0.0247 - val_accuracy: 0.9860 - val_loss: 0.0870\n",
      "Epoch 80/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9965 - loss: 0.0192 - val_accuracy: 0.9796 - val_loss: 0.0997\n",
      "Epoch 81/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9942 - loss: 0.0236 - val_accuracy: 0.9809 - val_loss: 0.0921\n",
      "Epoch 82/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9958 - loss: 0.0183 - val_accuracy: 0.9821 - val_loss: 0.0921\n",
      "Epoch 83/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9914 - loss: 0.0267 - val_accuracy: 0.9796 - val_loss: 0.0937\n",
      "Epoch 84/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9951 - loss: 0.0187 - val_accuracy: 0.9783 - val_loss: 0.0944\n",
      "Epoch 85/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9935 - loss: 0.0224 - val_accuracy: 0.9796 - val_loss: 0.0976\n",
      "Epoch 86/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9968 - loss: 0.0165 - val_accuracy: 0.9821 - val_loss: 0.0933\n",
      "Epoch 87/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9951 - loss: 0.0197 - val_accuracy: 0.9809 - val_loss: 0.0916\n",
      "Epoch 88/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9953 - loss: 0.0239 - val_accuracy: 0.9796 - val_loss: 0.0898\n",
      "Epoch 89/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9954 - loss: 0.0202 - val_accuracy: 0.9847 - val_loss: 0.0930\n",
      "訓練時間：16.80 秒\n",
      "\u001b[1m487/487\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 915us/step - accuracy: 0.9821 - loss: 0.0972\n",
      "Evaluation on 90% unused data - Loss: 0.0973, Accuracy: 0.9815\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 630us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data MDE report saved to: 80 data per RP transfer revised/DNN 2mcAP_Bests BEST_80data_2025_01_03.json\n",
      "\n",
      "Test Data Mean MDE: 0.0252 meters\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,185</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m576\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m8,320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m)             │         \u001b[38;5;34m3,185\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,339</span> (79.45 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m20,339\u001b[0m (79.45 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,185</span> (12.44 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,185\u001b[0m (12.44 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,152</span> (67.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m17,152\u001b[0m (67.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (12.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2\u001b[0m (12.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\吳定洋\\AppData\\Local\\Temp\\ipykernel_20228\\3920517792.py:45: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  test_processed_data = pd.concat([test_processed_data, group], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   AP1_Distance (mm)  AP4_Distance (mm)  AP1_Rssi  AP2_Rssi  AP3_Rssi  \\\n",
      "0             4038.0            13018.0     -65.0     -68.0     -50.0   \n",
      "1             3960.0            13252.0     -65.0     -68.0     -54.0   \n",
      "2             3843.0            13838.0     -65.0     -68.0     -54.0   \n",
      "3             3960.0            14307.0     -65.0     -68.0     -54.0   \n",
      "4             3999.0            13174.0     -65.0     -68.0     -54.0   \n",
      "\n",
      "   AP4_Rssi  AP1_StdDev (mm)  AP4_StdDev (mm)  \n",
      "0     -64.0            965.0            314.0  \n",
      "1     -66.0           1056.0            160.0  \n",
      "2     -64.0           1053.0            714.0  \n",
      "3     -64.0            938.0            265.0  \n",
      "4     -66.0            959.0            341.0  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Set</th>\n",
       "      <th>Validation Set</th>\n",
       "      <th>Test Set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Training Set  Validation Set  Test Set\n",
       "0             64              16       322\n",
       "1             64              16       322\n",
       "2             64              16       322\n",
       "3             64              16       322\n",
       "4             64              16       322\n",
       "5             64              16       322\n",
       "6             64              16       322\n",
       "7             64              16       322\n",
       "8             64              16       322\n",
       "9             64              16       322\n",
       "10            64              16       322\n",
       "11            64              16       322\n",
       "12            64              16       322\n",
       "13            64              16       322\n",
       "14            64              16       322\n",
       "15            64              16       322\n",
       "16            64              16       322\n",
       "17            64              16       322\n",
       "18            64              16       322\n",
       "19            64              16       322\n",
       "20            64              16       322\n",
       "21            64              16       322\n",
       "22            64              16       322\n",
       "23            64              16       322\n",
       "24            64              16       322\n",
       "25            64              16       322\n",
       "26            64              16       322\n",
       "27            64              16       322\n",
       "28            64              16       322\n",
       "29            64              16       322\n",
       "30            64              16       322\n",
       "31            64              16       322\n",
       "32            64              16       322\n",
       "33            64              16       322\n",
       "34            64              16       322\n",
       "35            64              16       322\n",
       "36            64              16       322\n",
       "37            64              16       322\n",
       "38            64              16       322\n",
       "39            64              16       322\n",
       "40            64              16       322\n",
       "41            64              16       322\n",
       "42            64              16       322\n",
       "43            64              16       322\n",
       "44            64              16       322\n",
       "45            64              16       322\n",
       "46            64              16       322\n",
       "47            64              16       322\n",
       "48            64              16       322"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "Epoch 1/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5294 - loss: 2.9006 - val_accuracy: 0.6441 - val_loss: 1.8300\n",
      "Epoch 2/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6388 - loss: 1.6687 - val_accuracy: 0.7117 - val_loss: 1.1941\n",
      "Epoch 3/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7482 - loss: 1.1186 - val_accuracy: 0.8010 - val_loss: 0.8429\n",
      "Epoch 4/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8107 - loss: 0.7281 - val_accuracy: 0.8406 - val_loss: 0.6650\n",
      "Epoch 5/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8796 - loss: 0.5688 - val_accuracy: 0.8814 - val_loss: 0.5357\n",
      "Epoch 6/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8994 - loss: 0.3961 - val_accuracy: 0.8980 - val_loss: 0.4489\n",
      "Epoch 7/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9040 - loss: 0.4738 - val_accuracy: 0.9145 - val_loss: 0.4048\n",
      "Epoch 8/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9285 - loss: 0.3888 - val_accuracy: 0.9273 - val_loss: 0.3746\n",
      "Epoch 9/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9367 - loss: 0.2905 - val_accuracy: 0.9311 - val_loss: 0.3352\n",
      "Epoch 10/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9406 - loss: 0.2912 - val_accuracy: 0.9401 - val_loss: 0.3156\n",
      "Epoch 11/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9498 - loss: 0.2574 - val_accuracy: 0.9375 - val_loss: 0.3045\n",
      "Epoch 12/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9518 - loss: 0.2819 - val_accuracy: 0.9426 - val_loss: 0.2769\n",
      "Epoch 13/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9545 - loss: 0.2028 - val_accuracy: 0.9515 - val_loss: 0.2668\n",
      "Epoch 14/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9573 - loss: 0.2214 - val_accuracy: 0.9528 - val_loss: 0.2608\n",
      "Epoch 15/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9636 - loss: 0.1697 - val_accuracy: 0.9566 - val_loss: 0.2492\n",
      "Epoch 16/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9611 - loss: 0.1610 - val_accuracy: 0.9617 - val_loss: 0.2378\n",
      "Epoch 17/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9599 - loss: 0.2157 - val_accuracy: 0.9681 - val_loss: 0.2258\n",
      "Epoch 18/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9688 - loss: 0.1433 - val_accuracy: 0.9643 - val_loss: 0.2306\n",
      "Epoch 19/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9728 - loss: 0.1212 - val_accuracy: 0.9694 - val_loss: 0.2191\n",
      "Epoch 20/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9668 - loss: 0.1402 - val_accuracy: 0.9681 - val_loss: 0.2202\n",
      "Epoch 21/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9672 - loss: 0.1663 - val_accuracy: 0.9681 - val_loss: 0.2192\n",
      "Epoch 22/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9705 - loss: 0.1363 - val_accuracy: 0.9719 - val_loss: 0.2029\n",
      "Epoch 23/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9741 - loss: 0.1535 - val_accuracy: 0.9732 - val_loss: 0.2052\n",
      "Epoch 24/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9711 - loss: 0.1031 - val_accuracy: 0.9694 - val_loss: 0.2052\n",
      "Epoch 25/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9718 - loss: 0.1070 - val_accuracy: 0.9745 - val_loss: 0.1980\n",
      "Epoch 26/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9740 - loss: 0.1019 - val_accuracy: 0.9745 - val_loss: 0.1903\n",
      "Epoch 27/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9737 - loss: 0.1028 - val_accuracy: 0.9745 - val_loss: 0.1773\n",
      "Epoch 28/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9749 - loss: 0.0840 - val_accuracy: 0.9707 - val_loss: 0.1876\n",
      "Epoch 29/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9794 - loss: 0.0835 - val_accuracy: 0.9732 - val_loss: 0.1842\n",
      "Epoch 30/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9785 - loss: 0.0914 - val_accuracy: 0.9719 - val_loss: 0.1772\n",
      "Epoch 31/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9753 - loss: 0.0940 - val_accuracy: 0.9719 - val_loss: 0.1844\n",
      "Epoch 32/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9804 - loss: 0.0866 - val_accuracy: 0.9719 - val_loss: 0.1846\n",
      "Epoch 33/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9847 - loss: 0.0669 - val_accuracy: 0.9707 - val_loss: 0.1784\n",
      "Epoch 34/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9819 - loss: 0.0717 - val_accuracy: 0.9707 - val_loss: 0.1673\n",
      "Epoch 35/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9784 - loss: 0.0751 - val_accuracy: 0.9719 - val_loss: 0.1675\n",
      "Epoch 36/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9801 - loss: 0.0898 - val_accuracy: 0.9732 - val_loss: 0.1672\n",
      "Epoch 37/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9814 - loss: 0.0817 - val_accuracy: 0.9758 - val_loss: 0.1637\n",
      "Epoch 38/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9813 - loss: 0.0736 - val_accuracy: 0.9770 - val_loss: 0.1670\n",
      "Epoch 39/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9841 - loss: 0.0648 - val_accuracy: 0.9719 - val_loss: 0.1624\n",
      "Epoch 40/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9803 - loss: 0.0688 - val_accuracy: 0.9783 - val_loss: 0.1734\n",
      "Epoch 41/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9866 - loss: 0.0648 - val_accuracy: 0.9719 - val_loss: 0.1774\n",
      "Epoch 42/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9846 - loss: 0.0562 - val_accuracy: 0.9796 - val_loss: 0.1590\n",
      "Epoch 43/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9861 - loss: 0.0577 - val_accuracy: 0.9732 - val_loss: 0.1653\n",
      "Epoch 44/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9857 - loss: 0.0583 - val_accuracy: 0.9758 - val_loss: 0.1652\n",
      "Epoch 45/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9845 - loss: 0.0508 - val_accuracy: 0.9770 - val_loss: 0.1680\n",
      "Epoch 46/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9875 - loss: 0.0580 - val_accuracy: 0.9796 - val_loss: 0.1623\n",
      "Epoch 47/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9890 - loss: 0.0405 - val_accuracy: 0.9783 - val_loss: 0.1603\n",
      "Epoch 48/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9851 - loss: 0.0513 - val_accuracy: 0.9809 - val_loss: 0.1588\n",
      "Epoch 49/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9855 - loss: 0.0467 - val_accuracy: 0.9783 - val_loss: 0.1587\n",
      "Epoch 50/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9857 - loss: 0.0527 - val_accuracy: 0.9809 - val_loss: 0.1612\n",
      "Epoch 51/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9852 - loss: 0.0508 - val_accuracy: 0.9796 - val_loss: 0.1556\n",
      "Epoch 52/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9869 - loss: 0.0450 - val_accuracy: 0.9770 - val_loss: 0.1562\n",
      "Epoch 53/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9865 - loss: 0.0482 - val_accuracy: 0.9783 - val_loss: 0.1535\n",
      "Epoch 54/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9871 - loss: 0.0408 - val_accuracy: 0.9770 - val_loss: 0.1524\n",
      "Epoch 55/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9907 - loss: 0.0336 - val_accuracy: 0.9783 - val_loss: 0.1580\n",
      "Epoch 56/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9865 - loss: 0.0417 - val_accuracy: 0.9745 - val_loss: 0.1541\n",
      "Epoch 57/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9901 - loss: 0.0365 - val_accuracy: 0.9783 - val_loss: 0.1565\n",
      "Epoch 58/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9889 - loss: 0.0500 - val_accuracy: 0.9783 - val_loss: 0.1578\n",
      "Epoch 59/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9884 - loss: 0.0453 - val_accuracy: 0.9783 - val_loss: 0.1529\n",
      "Epoch 60/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9868 - loss: 0.0366 - val_accuracy: 0.9758 - val_loss: 0.1575\n",
      "Epoch 61/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9872 - loss: 0.0442 - val_accuracy: 0.9796 - val_loss: 0.1558\n",
      "Epoch 62/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9873 - loss: 0.0421 - val_accuracy: 0.9770 - val_loss: 0.1588\n",
      "Epoch 63/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9894 - loss: 0.0346 - val_accuracy: 0.9809 - val_loss: 0.1550\n",
      "Epoch 64/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9879 - loss: 0.0398 - val_accuracy: 0.9770 - val_loss: 0.1487\n",
      "Epoch 65/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9901 - loss: 0.0326 - val_accuracy: 0.9758 - val_loss: 0.1584\n",
      "Epoch 66/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9911 - loss: 0.0305 - val_accuracy: 0.9809 - val_loss: 0.1530\n",
      "Epoch 67/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9902 - loss: 0.0327 - val_accuracy: 0.9809 - val_loss: 0.1526\n",
      "Epoch 68/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9886 - loss: 0.0389 - val_accuracy: 0.9758 - val_loss: 0.1539\n",
      "Epoch 69/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9909 - loss: 0.0342 - val_accuracy: 0.9796 - val_loss: 0.1499\n",
      "Epoch 70/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9905 - loss: 0.0305 - val_accuracy: 0.9796 - val_loss: 0.1595\n",
      "Epoch 71/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9861 - loss: 0.0420 - val_accuracy: 0.9796 - val_loss: 0.1538\n",
      "Epoch 72/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9929 - loss: 0.0315 - val_accuracy: 0.9796 - val_loss: 0.1570\n",
      "Epoch 73/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9898 - loss: 0.0392 - val_accuracy: 0.9796 - val_loss: 0.1560\n",
      "Epoch 74/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9937 - loss: 0.0257 - val_accuracy: 0.9796 - val_loss: 0.1592\n",
      "訓練時間：13.97 秒\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 883us/step - accuracy: 0.9768 - loss: 0.1149\n",
      "Evaluation on 90% unused data - Loss: 0.1355, Accuracy: 0.9741\n",
      "\u001b[1m616/616\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 619us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data MDE report saved to: 80 data per RP transfer revised/DNN 2mcAP_Bests BEST_80data_2025_01_10.json\n",
      "\n",
      "Test Data Mean MDE: 0.0315 meters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,185</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m576\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m8,320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m)             │         \u001b[38;5;34m3,185\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,339</span> (79.45 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m20,339\u001b[0m (79.45 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,185</span> (12.44 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,185\u001b[0m (12.44 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,152</span> (67.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m17,152\u001b[0m (67.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (12.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2\u001b[0m (12.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   AP1_Distance (mm)  AP4_Distance (mm)  AP1_Rssi  AP2_Rssi  AP3_Rssi  \\\n",
      "0             4116.0             9619.0     -61.0     -75.0     -48.0   \n",
      "1             3960.0             9658.0     -61.0     -75.0     -52.0   \n",
      "2             4038.0             9541.0     -61.0     -75.0     -52.0   \n",
      "3             4116.0             9619.0     -60.0     -75.0     -53.0   \n",
      "4             3999.0             9580.0     -61.0     -76.0     -53.0   \n",
      "\n",
      "   AP4_Rssi  AP1_StdDev (mm)  AP4_StdDev (mm)  \n",
      "0     -62.0            867.0           1129.0  \n",
      "1     -66.0           2079.0           1091.0  \n",
      "2     -67.0           2055.0           1178.0  \n",
      "3     -65.0           2016.0           1166.0  \n",
      "4     -67.0           2076.0           1233.0  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\吳定洋\\AppData\\Local\\Temp\\ipykernel_20228\\3920517792.py:45: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  test_processed_data = pd.concat([test_processed_data, group], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Set</th>\n",
       "      <th>Validation Set</th>\n",
       "      <th>Test Set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Training Set  Validation Set  Test Set\n",
       "0             64              16       306\n",
       "1             64              16       306\n",
       "2             64              16       306\n",
       "3             64              16       306\n",
       "4             64              16       306\n",
       "5             64              16       306\n",
       "6             64              16       306\n",
       "7             64              16       306\n",
       "8             64              16       306\n",
       "9             64              16       306\n",
       "10            64              16       306\n",
       "11            64              16       306\n",
       "12            64              16       306\n",
       "13            64              16       306\n",
       "14            64              16       306\n",
       "15            64              16       306\n",
       "16            64              16       306\n",
       "17            64              16       306\n",
       "18            64              16       306\n",
       "19            64              16       306\n",
       "20            64              16       306\n",
       "21            64              16       306\n",
       "22            64              16       306\n",
       "23            64              16       306\n",
       "24            64              16       306\n",
       "25            64              16       306\n",
       "26            64              16       306\n",
       "27            64              16       306\n",
       "28            64              16       306\n",
       "29            64              16       306\n",
       "30            64              16       306\n",
       "31            64              16       306\n",
       "32            64              16       306\n",
       "33            64              16       306\n",
       "34            64              16       306\n",
       "35            64              16       306\n",
       "36            64              16       306\n",
       "37            64              16       306\n",
       "38            64              16       306\n",
       "39            64              16       306\n",
       "40            64              16       306\n",
       "41            64              16       306\n",
       "42            64              16       306\n",
       "43            64              16       306\n",
       "44            64              16       306\n",
       "45            64              16       306\n",
       "46            64              16       306\n",
       "47            64              16       306\n",
       "48            64              16       306"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "Epoch 1/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.1972 - loss: 11.4304 - val_accuracy: 0.3125 - val_loss: 6.8883\n",
      "Epoch 2/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3303 - loss: 6.5984 - val_accuracy: 0.4375 - val_loss: 4.6152\n",
      "Epoch 3/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4341 - loss: 4.3353 - val_accuracy: 0.5268 - val_loss: 3.0635\n",
      "Epoch 4/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5405 - loss: 2.7828 - val_accuracy: 0.5969 - val_loss: 2.0983\n",
      "Epoch 5/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6231 - loss: 1.9833 - val_accuracy: 0.6786 - val_loss: 1.4665\n",
      "Epoch 6/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6971 - loss: 1.3971 - val_accuracy: 0.7207 - val_loss: 1.0785\n",
      "Epoch 7/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7505 - loss: 1.0096 - val_accuracy: 0.7819 - val_loss: 0.7823\n",
      "Epoch 8/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7904 - loss: 0.7506 - val_accuracy: 0.8342 - val_loss: 0.5797\n",
      "Epoch 9/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8518 - loss: 0.5368 - val_accuracy: 0.8801 - val_loss: 0.4695\n",
      "Epoch 10/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9013 - loss: 0.4607 - val_accuracy: 0.8992 - val_loss: 0.4032\n",
      "Epoch 11/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9066 - loss: 0.3947 - val_accuracy: 0.9158 - val_loss: 0.3512\n",
      "Epoch 12/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9241 - loss: 0.3650 - val_accuracy: 0.9286 - val_loss: 0.3213\n",
      "Epoch 13/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9361 - loss: 0.3371 - val_accuracy: 0.9298 - val_loss: 0.2789\n",
      "Epoch 14/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9347 - loss: 0.3399 - val_accuracy: 0.9388 - val_loss: 0.2635\n",
      "Epoch 15/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9467 - loss: 0.2599 - val_accuracy: 0.9464 - val_loss: 0.2398\n",
      "Epoch 16/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9548 - loss: 0.2440 - val_accuracy: 0.9439 - val_loss: 0.2364\n",
      "Epoch 17/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9581 - loss: 0.2313 - val_accuracy: 0.9464 - val_loss: 0.2246\n",
      "Epoch 18/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9677 - loss: 0.2217 - val_accuracy: 0.9477 - val_loss: 0.2112\n",
      "Epoch 19/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9719 - loss: 0.1587 - val_accuracy: 0.9541 - val_loss: 0.2008\n",
      "Epoch 20/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9712 - loss: 0.1617 - val_accuracy: 0.9528 - val_loss: 0.1973\n",
      "Epoch 21/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9708 - loss: 0.1578 - val_accuracy: 0.9566 - val_loss: 0.1820\n",
      "Epoch 22/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9716 - loss: 0.1427 - val_accuracy: 0.9617 - val_loss: 0.1796\n",
      "Epoch 23/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9747 - loss: 0.1442 - val_accuracy: 0.9592 - val_loss: 0.1853\n",
      "Epoch 24/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9761 - loss: 0.1714 - val_accuracy: 0.9592 - val_loss: 0.1756\n",
      "Epoch 25/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9749 - loss: 0.1743 - val_accuracy: 0.9605 - val_loss: 0.1802\n",
      "Epoch 26/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9758 - loss: 0.1250 - val_accuracy: 0.9643 - val_loss: 0.1650\n",
      "Epoch 27/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9793 - loss: 0.1479 - val_accuracy: 0.9630 - val_loss: 0.1683\n",
      "Epoch 28/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9851 - loss: 0.0924 - val_accuracy: 0.9681 - val_loss: 0.1589\n",
      "Epoch 29/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9872 - loss: 0.0725 - val_accuracy: 0.9694 - val_loss: 0.1634\n",
      "Epoch 30/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9838 - loss: 0.0957 - val_accuracy: 0.9643 - val_loss: 0.1613\n",
      "Epoch 31/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9835 - loss: 0.1182 - val_accuracy: 0.9656 - val_loss: 0.1498\n",
      "Epoch 32/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9835 - loss: 0.1050 - val_accuracy: 0.9707 - val_loss: 0.1499\n",
      "Epoch 33/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9856 - loss: 0.1149 - val_accuracy: 0.9707 - val_loss: 0.1469\n",
      "Epoch 34/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9894 - loss: 0.0646 - val_accuracy: 0.9656 - val_loss: 0.1564\n",
      "Epoch 35/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9867 - loss: 0.0737 - val_accuracy: 0.9707 - val_loss: 0.1427\n",
      "Epoch 36/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9860 - loss: 0.0945 - val_accuracy: 0.9681 - val_loss: 0.1499\n",
      "Epoch 37/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9849 - loss: 0.0795 - val_accuracy: 0.9745 - val_loss: 0.1340\n",
      "Epoch 38/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9916 - loss: 0.0527 - val_accuracy: 0.9732 - val_loss: 0.1409\n",
      "Epoch 39/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9829 - loss: 0.0854 - val_accuracy: 0.9707 - val_loss: 0.1422\n",
      "Epoch 40/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9898 - loss: 0.0640 - val_accuracy: 0.9732 - val_loss: 0.1348\n",
      "Epoch 41/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9920 - loss: 0.0704 - val_accuracy: 0.9719 - val_loss: 0.1398\n",
      "Epoch 42/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9894 - loss: 0.0620 - val_accuracy: 0.9681 - val_loss: 0.1438\n",
      "Epoch 43/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9875 - loss: 0.0749 - val_accuracy: 0.9694 - val_loss: 0.1407\n",
      "Epoch 44/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9892 - loss: 0.0805 - val_accuracy: 0.9719 - val_loss: 0.1332\n",
      "Epoch 45/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9908 - loss: 0.0602 - val_accuracy: 0.9770 - val_loss: 0.1297\n",
      "Epoch 46/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9880 - loss: 0.0740 - val_accuracy: 0.9707 - val_loss: 0.1328\n",
      "Epoch 47/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9892 - loss: 0.0543 - val_accuracy: 0.9732 - val_loss: 0.1346\n",
      "Epoch 48/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9913 - loss: 0.0471 - val_accuracy: 0.9694 - val_loss: 0.1385\n",
      "Epoch 49/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9914 - loss: 0.0410 - val_accuracy: 0.9745 - val_loss: 0.1314\n",
      "Epoch 50/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9909 - loss: 0.0663 - val_accuracy: 0.9732 - val_loss: 0.1349\n",
      "Epoch 51/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9874 - loss: 0.0636 - val_accuracy: 0.9770 - val_loss: 0.1326\n",
      "Epoch 52/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9921 - loss: 0.0470 - val_accuracy: 0.9758 - val_loss: 0.1277\n",
      "Epoch 53/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9924 - loss: 0.0594 - val_accuracy: 0.9745 - val_loss: 0.1337\n",
      "Epoch 54/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9934 - loss: 0.0334 - val_accuracy: 0.9809 - val_loss: 0.1299\n",
      "Epoch 55/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9930 - loss: 0.0487 - val_accuracy: 0.9770 - val_loss: 0.1301\n",
      "Epoch 56/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9905 - loss: 0.0411 - val_accuracy: 0.9707 - val_loss: 0.1395\n",
      "Epoch 57/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9932 - loss: 0.0388 - val_accuracy: 0.9783 - val_loss: 0.1346\n",
      "Epoch 58/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9915 - loss: 0.0407 - val_accuracy: 0.9796 - val_loss: 0.1304\n",
      "Epoch 59/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9885 - loss: 0.0555 - val_accuracy: 0.9745 - val_loss: 0.1348\n",
      "Epoch 60/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9935 - loss: 0.0335 - val_accuracy: 0.9796 - val_loss: 0.1288\n",
      "Epoch 61/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9900 - loss: 0.0431 - val_accuracy: 0.9796 - val_loss: 0.1368\n",
      "Epoch 62/10000\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9921 - loss: 0.0522 - val_accuracy: 0.9783 - val_loss: 0.1363\n",
      "訓練時間：11.91 秒\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 918us/step - accuracy: 0.9750 - loss: 0.1451\n",
      "Evaluation on 90% unused data - Loss: 0.1179, Accuracy: 0.9809\n",
      "\u001b[1m592/592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 618us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data MDE report saved to: 80 data per RP transfer revised/DNN 2mcAP_Bests BEST_80data_2025_02_28.json\n",
      "\n",
      "Test Data Mean MDE: 0.0295 meters\n"
     ]
    }
   ],
   "source": [
    "for i in range(number_of_week):\n",
    "    \n",
    "    # 載入模型與標準化器\n",
    "    base_model = load_model(f'{root}/DNN_best_model{ap}_{weekrepresent[i][0]}week_to_{weekrepresent[i][1]}week_{dataamount}dataPerRP.h5')\n",
    "    scaler = joblib.load(f'{root}/scaler_{ap}.pkl')\n",
    "\n",
    "    # 讀取測試資料 2024_12_21   2024_12_27   2025_01_03   2025_01_10   2025_02_28\n",
    "    test_file_path = f\"timestamp_allignment_Balanced_{alldatadate[i]}_rtt_logs.csv\"  # 測試資料的檔案名稱\n",
    "    date_test = f\"{alldatadate[i]}\"\n",
    "    modelname = f\"DNN {ap}s BEST_{dataamount}data_{alldatadate[i]}\"\n",
    "    test_data = pd.read_csv(test_file_path, usecols=selected_columns)\n",
    "    # test_data\n",
    "    \n",
    "\n",
    "    # 凍結所有層\n",
    "    for layer in base_model.layers[:-1]:  # 除了最後一層 (Output Layer)\n",
    "        layer.trainable = False\n",
    "\n",
    "    # 確認哪些層可訓練\n",
    "    base_model.summary()\n",
    "\n",
    "    base_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 資料前處理 (一): 刪除前後n筆資料\n",
    "    n = 10\n",
    "    # 確保依據Label排序\n",
    "    test_data = test_data.sort_values(by=label_column).reset_index(drop=True)\n",
    "\n",
    "    # 建立一個空的 DataFrame 用於存放處理後的資料\n",
    "    test_processed_data = pd.DataFrame(columns=test_data.columns)\n",
    "\n",
    "    # 針對每個Label群組進行處理\n",
    "    for label, group in test_data.groupby(label_column):\n",
    "        # 刪除前n筆和後n筆資料\n",
    "        if len(group) > 2 * n:  # 確保群組資料足夠\n",
    "            group = group.iloc[n:-n]\n",
    "        else:\n",
    "            group = pd.DataFrame()  # 若資料不足，刪除整個群組\n",
    "        # 將處理後的群組資料加入\n",
    "        test_processed_data = pd.concat([test_processed_data, group], ignore_index=True)\n",
    "\n",
    "    # test_processed_data\n",
    "    # Calculate the number of rows with NaN values\n",
    "    nan_rows = test_processed_data.isnull().any(axis=1).sum()\n",
    "\n",
    "    # Print the result\n",
    "    # print(f\"Number of rows with NaN values: {nan_rows}\")\n",
    "\n",
    "    # 找出包含 NaN 的列\n",
    "    rows_with_nan = test_processed_data[test_processed_data.isnull().any(axis=1)]\n",
    "\n",
    "    # # 印出這些列\n",
    "    # print(\"Rows with NaN values:\")\n",
    "    # print(rows_with_nan)\n",
    "    test_data_imputed = test_processed_data.groupby(label_column).apply(\n",
    "        lambda group: group.fillna(group.mean())\n",
    "    ).reset_index()\n",
    "\n",
    "    # Calculate the number of rows with NaN values\n",
    "    nan_rows = test_data_imputed.isnull().any(axis=1).sum()\n",
    "\n",
    "    # Print the result\n",
    "    # print(f\"Number of rows with NaN values: {nan_rows}\")\n",
    "\n",
    "    # 找出包含 NaN 的列\n",
    "    rows_with_nan = test_data_imputed[test_data_imputed.isnull().any(axis=1)]\n",
    "\n",
    "    test_data_imputed\n",
    "\n",
    "    reverse_label_mapping = {v: int(k) - 1 for k, v in label_mapping.items()}  # 讓數字標籤 -1\n",
    "\n",
    "    # 建立 Label 映射\n",
    "    y_test = test_data_imputed[target_column]\n",
    "    y_test_numeric = y_test.map(reverse_label_mapping)\n",
    "\n",
    "    # print(\"Final reverse_label_mapping in DNN:\", reverse_label_mapping)\n",
    "    # print(\"y_numeric unique values in DNN:\", y_test_numeric.unique())\n",
    "\n",
    "    y_test_numeric\n",
    "\n",
    "    # 把label部分拿掉\n",
    "    X_test = test_data_imputed.drop(columns=['level_1','Label'])\n",
    "    print(X_test.head())\n",
    "\n",
    "    # 確保測試資料的特徵與訓練資料的特徵一致\n",
    "    X_test = X_test[X_testing_selected_columns]  # 選取相同的特徵\n",
    "\n",
    "    # print(type(X_test))\n",
    "\n",
    "    # 使用之前訓練時的標準化器 (scaler) 來標準化測試數據\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # 會發現如果用 train_test_split的方法會有資料分布不平均問題，解決辦法如下\n",
    "    from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "\n",
    "    # 轉為 DataFrame 方便操作\n",
    "    data = pd.DataFrame(X_test_scaled)\n",
    "    data['label'] = y_test_numeric  # 加入 label 欄位\n",
    "\n",
    "    # print((data['label'] == 10).sum())  # 直接計算 True 的數量\n",
    "\n",
    "    # 轉為 DataFrame 方便操作\n",
    "    data = pd.DataFrame(X_test_scaled)\n",
    "    data['label'] = y_test_numeric  # 加入 label 欄位\n",
    "\n",
    "    # data\n",
    "    # 儲存訓練集（但這時包含 validation 的部分）\n",
    "    # train_data_full = data.groupby('label', group_keys=False).sample(n=N_train, replace=False, random_state=42)\n",
    "    # 儲存訓練集（確保每個類別只選 1 筆，若某類少於 N_train，則取全部）\n",
    "    # train_data_full = data.groupby('label', group_keys=False).apply(lambda x: x.sample(n=min(N_train, len(x)), replace=False, random_state=42)).reset_index(drop=True)\n",
    "    train_data_full = data.groupby('label', group_keys=False).sample(n=N_train, replace=False, random_state=42) # \n",
    "    # 確保 `train_data_full` 內部的 `label` 數量正確\n",
    "    # print(train_data_full['label'].value_counts())  # 每個類別應該剛好 1 筆\n",
    "    # train_data_full\n",
    "\n",
    "\n",
    "    if N_val > 0:\n",
    "        sss = StratifiedShuffleSplit(n_splits=1, test_size=N_val / N_train, random_state=42) # \n",
    "        train_index, val_index = next(sss.split(train_data_full.drop(columns=['label']), train_data_full['label']))\n",
    "        train_data = train_data_full.iloc[train_index]\n",
    "        val_data = train_data_full.iloc[val_index]\n",
    "        \n",
    "    else:\n",
    "        val_data = pd.DataFrame(columns=data.columns)  # 若沒有 validation data，建立空 DataFrame\\\n",
    "        train_data = train_data_full\n",
    "\n",
    "    # 剩下的資料（未被抽入 train_data_full 的部分）直接作為 test set\n",
    "    remaining_data = data.drop(train_data_full.index)\n",
    "    # print(len(remaining_data))\n",
    "\n",
    "\n",
    "    # **轉換為 NumPy 陣列**\n",
    "    X_train, y_train = train_data.drop(columns=['label']).values, train_data['label'].values\n",
    "    X_val, y_val = val_data.drop(columns=['label']).values, val_data['label'].values\n",
    "    X_test, y_test = remaining_data.drop(columns=['label']).values, remaining_data['label'].values\n",
    "\n",
    "    # **確認數據切分結果**\n",
    "    # print(f\"Training set: {len(X_train)} samples, {len(np.unique(y_train))} unique labels\")\n",
    "    # print(f\"Validation set: {len(X_val)} samples, {len(np.unique(y_val))} unique labels\")\n",
    "    # print(f\"Test set: {len(X_test)} samples, {len(np.unique(y_test))} unique labels\")\n",
    "\n",
    "   # **計算每個 Set 內各 Label 的資料數量**\n",
    "    train_label_counts = pd.Series(y_train).value_counts().sort_index()\n",
    "    val_label_counts = pd.Series(y_val).value_counts().sort_index()\n",
    "    test_label_counts = pd.Series(y_test).value_counts().sort_index()\n",
    "\n",
    "    # **確保所有 Labels 都有出現在三個 Set 裡**\n",
    "    all_labels = sorted(set(train_label_counts.index) | set(val_label_counts.index) | set(test_label_counts.index))\n",
    "    label_distribution = pd.DataFrame(index=all_labels)\n",
    "\n",
    "    label_distribution[\"Training Set\"] = train_label_counts\n",
    "    label_distribution[\"Validation Set\"] = val_label_counts\n",
    "    label_distribution[\"Test Set\"] = test_label_counts\n",
    "\n",
    "    # **用 0 填補缺失值（表示該 Label 在該 Set 中沒有數據）**\n",
    "    label_distribution = label_distribution.fillna(0).astype(int)\n",
    "\n",
    "    from IPython.display import display\n",
    "    display(label_distribution)\n",
    "\n",
    "    import time\n",
    "    # 記錄開始時間\n",
    "    start_time = time.time()\n",
    "\n",
    "\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "    if N_val > 0:\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    else:\n",
    "        early_stop = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "\n",
    "    # 確保變數命名一致\n",
    "    X_train_small = X_train  # 確保這裡用的變數和前面一致\n",
    "    y_train_small = y_train\n",
    "\n",
    "    # 設定 batch_size\n",
    "    batch_size = min(32, max(8, len(X_train_small) // 2))  # 避免 batch size 過大\n",
    "    # batch_size = 32\n",
    "    print(batch_size)\n",
    "\n",
    "    if N_val > 0:\n",
    "        base_model.fit(X_train_small, y_train_small,validation_data=(X_val, y_val), epochs=10000, batch_size=batch_size, callbacks=[early_stop])\n",
    "    else:\n",
    "        base_model.fit(X_train_small, y_train_small, epochs=10000, batch_size=batch_size, callbacks=[early_stop])\n",
    "\n",
    "\n",
    "    # 記錄結束時間\n",
    "    end_time = time.time()\n",
    "\n",
    "\n",
    "    # Needsave\n",
    "    # 計算訓練時間（秒）\n",
    "    training_time = end_time - start_time\n",
    "    print(f\"訓練時間：{training_time:.2f} 秒\")\n",
    "    ALL_trainingtime.append(training_time)\n",
    "\n",
    "    # Needsave\n",
    "    loss, accuracy = base_model.evaluate(X_test, y_test)\n",
    "    print(f\"Evaluation on 90% unused data - Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    ALL_loss.append(loss)\n",
    "    ALL_accuracy.append(accuracy)\n",
    "\n",
    "    # 預測測試資料\n",
    "    y_test_pred_numeric = base_model.predict(X_test_scaled)\n",
    "    y_pred_classes = np.argmax(y_test_pred_numeric, axis=1)\n",
    "\n",
    "    # 轉換為原本的 Label\n",
    "    y_test_pred_labels = [label_mapping[str(num + 1)] for num in y_pred_classes]  # 補回 +1\n",
    "    y_test_pred_labels\n",
    "\n",
    "    # 讀取測試資料的實際 Label\n",
    "    y_test_actual = test_data_imputed[target_column]\n",
    "    test_data_imputed\n",
    "\n",
    "    # 取得預測與實際座標\n",
    "    y_test_pred_coordinates = np.array([label_to_coordinates[label] for label in y_test_pred_labels])\n",
    "    y_test_actual_coordinates = np.array([label_to_coordinates[label] for label in y_test_actual])\n",
    "\n",
    "    # 計算 MDE (Mean Distance Error)\n",
    "    distances = np.linalg.norm(y_test_pred_coordinates - y_test_actual_coordinates, axis=1)\n",
    "    mean_mde = np.mean(distances)\n",
    "\n",
    "    # 記錄每個 RP 的 MDE\n",
    "    mde_report_test = {}\n",
    "    for true_label, distance in zip(y_test_actual, distances):\n",
    "        if true_label not in mde_report_test:\n",
    "            mde_report_test[true_label] = []\n",
    "        mde_report_test[true_label].append(distance)\n",
    "\n",
    "    # 計算測試資料的 MDE 平均值\n",
    "    mde_report_test_avg = {label: {\"mde\": np.mean(dists), \"count\": len(dists)} for label, dists in mde_report_test.items()}\n",
    "\n",
    "    # 儲存 MDE 結果到 JSON 檔案\n",
    "    test_file_path = f\"{root}/{modelname}.json\"\n",
    "    with open(test_file_path, \"w\") as f:\n",
    "        json.dump(mde_report_test_avg, f, indent=4)\n",
    "\n",
    "    # Needsave\n",
    "    print(f\"Test Data MDE report saved to: {test_file_path}\")\n",
    "    print(f\"\\nTest Data Mean MDE: {mean_mde:.4f} meters\")\n",
    "    ALL_mean_mde.append(mean_mde)\n",
    "\n",
    "\n",
    "    base_model.save(f'{root}/DNN_best_model{ap}_{weekrepresent[i+1][0]}week_to_{weekrepresent[i+1][1]}week_{dataamount}dataPerRP.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time\n",
      "[16.12, 10.83, 16.80, 13.97, 11.91, ]\n",
      "Loss\n",
      "[0.1039, 0.1189, 0.0973, 0.1355, 0.1179, ]\n",
      "Accuracy\n",
      "[0.9808, 0.9772, 0.9815, 0.9741, 0.9809, ]\n",
      "MDE\n",
      "[0.0285, 0.0324, 0.0252, 0.0315, 0.0295, ]\n"
     ]
    }
   ],
   "source": [
    "print(\"training time\")\n",
    "print(\"[\",end = '')\n",
    "for t in ALL_trainingtime:\n",
    "    print(f\"{t:.2f}\",end = ', ')\n",
    "print(\"]\")\n",
    "\n",
    "print(\"Loss\")\n",
    "print(\"[\",end = '')\n",
    "for l in ALL_loss:\n",
    "    print(f\"{l:.4f}\",end = ', ')\n",
    "print(\"]\")\n",
    "\n",
    "\n",
    "print(\"Accuracy\")\n",
    "print(\"[\",end = '')\n",
    "for a in ALL_accuracy:\n",
    "    print(f\"{a:.4f}\",end = ', ')\n",
    "print(\"]\")\n",
    "\n",
    "\n",
    "print(\"MDE\")\n",
    "print(\"[\",end = '')\n",
    "for m in ALL_mean_mde:\n",
    "    print(f\"{m:.4f}\",end = ', ')\n",
    "print(\"]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
