{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 載入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_coordinates = {\n",
    "    \"1-1\": (0, 0), \"1-2\": (0.6, 0), \"1-3\": (1.2, 0), \"1-4\": (1.8, 0), \"1-5\": (2.4, 0), \"1-6\": (3.0, 0),\"1-7\": (3.6, 0), \"1-8\": (4.2, 0), \"1-9\": (4.8, 0), \"1-10\": (5.4, 0), \"1-11\": (6.0, 0),\n",
    "    \"2-1\": (0, 0.6), \"2-11\": (6.0, 0.6),\n",
    "    \"3-1\": (0, 1.2), \"3-11\": (6.0, 1.2),\n",
    "    \"4-1\": (0, 1.8), \"4-11\": (6.0, 1.8),\n",
    "    \"5-1\": (0, 2.4), \"5-11\": (6.0, 2.4),\n",
    "    \"6-1\": (0, 3.0), \"6-2\": (0.6, 3.0), \"6-3\": (1.2, 3.0), \"6-4\": (1.8, 3.0), \"6-5\": (2.4, 3.0),\"6-6\": (3.0, 3.0), \"6-7\": (3.6, 3.0), \"6-8\": (4.2, 3.0), \"6-9\": (4.8, 3.0), \"6-10\": (5.4, 3.0), \"6-11\": (6.0, 3.0),\n",
    "    \"7-1\": (0, 3.6), \"7-11\": (6.0, 3.6),\n",
    "    \"8-1\": (0, 4.2), \"8-11\": (6.0, 4.2),\n",
    "    \"9-1\": (0, 4.8), \"9-11\": (6.0, 4.8),\n",
    "    \"10-1\": (0, 5.4), \"10-11\": (6.0, 5.4),\n",
    "    \"11-1\": (0, 6.0), \"11-2\": (0.6, 6.0), \"11-3\": (1.2, 6.0), \"11-4\": (1.8, 6.0), \"11-5\": (2.4, 6.0),\"11-6\": (3.0, 6.0), \"11-7\": (3.6, 6.0), \"11-8\": (4.2, 6.0), \"11-9\": (4.8, 6.0), \"11-10\": (5.4, 6.0), \"11-11\": (6.0, 6.0)\n",
    "}\n",
    "label_mapping = {\n",
    "    '11': '1-1','10': '1-2','9': '1-3','8': '1-4','7': '1-5','6': '1-6','5': '1-7','4': '1-8','3': '1-9','2': '1-10','1': '1-11',\n",
    "    '12': '2-1','30': '2-11',\n",
    "    '13': '3-1','29': '3-11',\n",
    "    '14': '4-1','28': '4-11',\n",
    "    '15': '5-1','27': '5-11',\n",
    "    '16': '6-1','17': '6-2','18': '6-3','19': '6-4','20': '6-5','21': '6-6','22': '6-7','23': '6-8','24': '6-9','25': '6-10','26': '6-11',\n",
    "    '49': '7-1','31': '7-11',\n",
    "    '48': '8-1','32': '8-11',\n",
    "    '47': '9-1','33': '9-11',\n",
    "    '46': '10-1','34': '10-11',\n",
    "    '45': '11-1','44': '11-2','43': '11-3','42': '11-4','41': '11-5','40': '11-6','39': '11-7','38': '11-8','37': '11-9','36': '11-10','35': '11-11'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import joblib  # 用於保存模型\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = ['Label',\n",
    "                    'AP1_Distance (mm)','AP2_Distance (mm)','AP3_Distance (mm)','AP4_Distance (mm)',\n",
    "'AP1_StdDev (mm)','AP2_StdDev (mm)','AP3_StdDev (mm)','AP4_StdDev (mm)',\n",
    "                                'AP1_Rssi','AP2_Rssi','AP3_Rssi','AP4_Rssi']  \n",
    "\n",
    "X_testing_selected_columns = [\n",
    "        'AP1_Distance (mm)','AP2_Distance (mm)','AP3_Distance (mm)','AP4_Distance (mm)',\n",
    "        'AP1_StdDev (mm)','AP2_StdDev (mm)','AP3_StdDev (mm)','AP4_StdDev (mm)',\n",
    "                                'AP1_Rssi','AP2_Rssi','AP3_Rssi','AP4_Rssi']  \n",
    "\n",
    "label_column = 'Label'\n",
    "target_column = 'Label'\n",
    "\n",
    "\n",
    "#  'AP1_Rssi','AP2_Rssi','AP3_Rssi','AP4_Rssi'\n",
    "# 'AP1_Distance (mm)','AP2_Distance (mm)','AP3_Distance (mm)','AP4_Distance (mm)',\n",
    "# 'AP1_StdDev (mm)','AP2_StdDev (mm)','AP3_StdDev (mm)','AP4_StdDev (mm)',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '10 data per RP transfer revised'\n",
    "\n",
    "dataamount = 10\n",
    "N_val = 2\n",
    "\n",
    "N_train = dataamount # 訓練集每個類別至少要有 N_train 筆資料\n",
    "test_val_ratio = 1  # 剩餘資料中，50% 作為驗證集，50% 作為測試集\n",
    "\n",
    "weekrepresent = [['0','0'],['0','1'],['1','2'],['2','3'],['3','4'],['4','5']]\n",
    "alldatadate = ['2024_12_21','2024_12_27','2025_01_03','2025_01_10','2025_02_28']\n",
    "number_of_week = len(alldatadate)\n",
    "\n",
    "ALL_trainingtime = []\n",
    "ALL_loss = []\n",
    "ALL_accuracy = []\n",
    "ALL_mean_mde = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_44 (Dense)            (None, 64)                832       \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 49)                3185      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20593 (80.44 KB)\n",
      "Trainable params: 3185 (12.44 KB)\n",
      "Non-trainable params: 17408 (68.00 KB)\n",
      "_________________________________________________________________\n",
      "Number of rows with NaN values: 1112\n",
      "Number of rows with NaN values: 0\n",
      "Final reverse_label_mapping in DNN: {'1-1': 10, '1-2': 9, '1-3': 8, '1-4': 7, '1-5': 6, '1-6': 5, '1-7': 4, '1-8': 3, '1-9': 2, '1-10': 1, '1-11': 0, '2-1': 11, '2-11': 29, '3-1': 12, '3-11': 28, '4-1': 13, '4-11': 27, '5-1': 14, '5-11': 26, '6-1': 15, '6-2': 16, '6-3': 17, '6-4': 18, '6-5': 19, '6-6': 20, '6-7': 21, '6-8': 22, '6-9': 23, '6-10': 24, '6-11': 25, '7-1': 48, '7-11': 30, '8-1': 47, '8-11': 31, '9-1': 46, '9-11': 32, '10-1': 45, '10-11': 33, '11-1': 44, '11-2': 43, '11-3': 42, '11-4': 41, '11-5': 40, '11-6': 39, '11-7': 38, '11-8': 37, '11-9': 36, '11-10': 35, '11-11': 34}\n",
      "y_numeric unique values in DNN: [10  1  0  9  8  7  6  5  4  3  2 45 33 44 35 34 43 42 41 40 39 38 37 36\n",
      " 11 29 12 28 13 27 14 26 15 24 25 16 17 18 19 20 21 22 23 48 30 47 31 46\n",
      " 32]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "365\n",
      "label\n",
      "0     5\n",
      "25    5\n",
      "27    5\n",
      "28    5\n",
      "29    5\n",
      "30    5\n",
      "31    5\n",
      "32    5\n",
      "33    5\n",
      "34    5\n",
      "35    5\n",
      "36    5\n",
      "37    5\n",
      "38    5\n",
      "39    5\n",
      "40    5\n",
      "41    5\n",
      "42    5\n",
      "43    5\n",
      "44    5\n",
      "45    5\n",
      "46    5\n",
      "47    5\n",
      "26    5\n",
      "24    5\n",
      "1     5\n",
      "23    5\n",
      "2     5\n",
      "3     5\n",
      "4     5\n",
      "5     5\n",
      "6     5\n",
      "7     5\n",
      "8     5\n",
      "9     5\n",
      "10    5\n",
      "11    5\n",
      "12    5\n",
      "13    5\n",
      "14    5\n",
      "15    5\n",
      "16    5\n",
      "17    5\n",
      "18    5\n",
      "19    5\n",
      "20    5\n",
      "21    5\n",
      "22    5\n",
      "48    5\n",
      "Name: count, dtype: int64\n",
      "17640\n",
      "Training set: 196 samples, 49 unique labels\n",
      "Validation set: 49 samples, 49 unique labels\n",
      "Test set: 17640 samples, 49 unique labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcslab/anaconda3/envs/yang_cuda/lib/python3.10/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.3.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_1914507/3564637810.py:45: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  test_processed_data = pd.concat([test_processed_data, group], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Set</th>\n",
       "      <th>Validation Set</th>\n",
       "      <th>Test Set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Training Set  Validation Set  Test Set\n",
       "0              4               1       360\n",
       "1              4               1       360\n",
       "2              4               1       360\n",
       "3              4               1       360\n",
       "4              4               1       360\n",
       "5              4               1       360\n",
       "6              4               1       360\n",
       "7              4               1       360\n",
       "8              4               1       360\n",
       "9              4               1       360\n",
       "10             4               1       360\n",
       "11             4               1       360\n",
       "12             4               1       360\n",
       "13             4               1       360\n",
       "14             4               1       360\n",
       "15             4               1       360\n",
       "16             4               1       360\n",
       "17             4               1       360\n",
       "18             4               1       360\n",
       "19             4               1       360\n",
       "20             4               1       360\n",
       "21             4               1       360\n",
       "22             4               1       360\n",
       "23             4               1       360\n",
       "24             4               1       360\n",
       "25             4               1       360\n",
       "26             4               1       360\n",
       "27             4               1       360\n",
       "28             4               1       360\n",
       "29             4               1       360\n",
       "30             4               1       360\n",
       "31             4               1       360\n",
       "32             4               1       360\n",
       "33             4               1       360\n",
       "34             4               1       360\n",
       "35             4               1       360\n",
       "36             4               1       360\n",
       "37             4               1       360\n",
       "38             4               1       360\n",
       "39             4               1       360\n",
       "40             4               1       360\n",
       "41             4               1       360\n",
       "42             4               1       360\n",
       "43             4               1       360\n",
       "44             4               1       360\n",
       "45             4               1       360\n",
       "46             4               1       360\n",
       "47             4               1       360\n",
       "48             4               1       360"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "Epoch 1/10000\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 5.4839 - accuracy: 0.4694 - val_loss: 5.4974 - val_accuracy: 0.5102\n",
      "Epoch 2/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.9637 - accuracy: 0.4847 - val_loss: 5.1345 - val_accuracy: 0.5102\n",
      "Epoch 3/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.5549 - accuracy: 0.4847 - val_loss: 4.8278 - val_accuracy: 0.5102\n",
      "Epoch 4/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 4.2163 - accuracy: 0.4898 - val_loss: 4.5988 - val_accuracy: 0.5102\n",
      "Epoch 5/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 3.9275 - accuracy: 0.4949 - val_loss: 4.3817 - val_accuracy: 0.5306\n",
      "Epoch 6/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 3.6766 - accuracy: 0.5153 - val_loss: 4.1930 - val_accuracy: 0.5510\n",
      "Epoch 7/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 3.4633 - accuracy: 0.5306 - val_loss: 4.0178 - val_accuracy: 0.5714\n",
      "Epoch 8/10000\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 3.2660 - accuracy: 0.5510 - val_loss: 3.8223 - val_accuracy: 0.5714\n",
      "Epoch 9/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 3.0904 - accuracy: 0.5714 - val_loss: 3.6598 - val_accuracy: 0.5918\n",
      "Epoch 10/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 2.9081 - accuracy: 0.5918 - val_loss: 3.4857 - val_accuracy: 0.5918\n",
      "Epoch 11/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 2.7401 - accuracy: 0.5969 - val_loss: 3.3489 - val_accuracy: 0.5918\n",
      "Epoch 12/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 2.5897 - accuracy: 0.6020 - val_loss: 3.2065 - val_accuracy: 0.6122\n",
      "Epoch 13/10000\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 2.4320 - accuracy: 0.6224 - val_loss: 3.0609 - val_accuracy: 0.6327\n",
      "Epoch 14/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 2.2663 - accuracy: 0.6224 - val_loss: 2.9214 - val_accuracy: 0.6327\n",
      "Epoch 15/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.1344 - accuracy: 0.6378 - val_loss: 2.7783 - val_accuracy: 0.6122\n",
      "Epoch 16/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.9943 - accuracy: 0.6480 - val_loss: 2.6604 - val_accuracy: 0.6122\n",
      "Epoch 17/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.8736 - accuracy: 0.6735 - val_loss: 2.5617 - val_accuracy: 0.6122\n",
      "Epoch 18/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.7705 - accuracy: 0.6786 - val_loss: 2.4706 - val_accuracy: 0.6122\n",
      "Epoch 19/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1.6593 - accuracy: 0.6837 - val_loss: 2.3725 - val_accuracy: 0.6531\n",
      "Epoch 20/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1.5584 - accuracy: 0.7041 - val_loss: 2.2485 - val_accuracy: 0.6735\n",
      "Epoch 21/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.4629 - accuracy: 0.7092 - val_loss: 2.1495 - val_accuracy: 0.6939\n",
      "Epoch 22/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.3686 - accuracy: 0.7194 - val_loss: 2.0638 - val_accuracy: 0.6735\n",
      "Epoch 23/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.2871 - accuracy: 0.7296 - val_loss: 1.9958 - val_accuracy: 0.6735\n",
      "Epoch 24/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.2112 - accuracy: 0.7347 - val_loss: 1.9299 - val_accuracy: 0.7347\n",
      "Epoch 25/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.1418 - accuracy: 0.7602 - val_loss: 1.8670 - val_accuracy: 0.7347\n",
      "Epoch 26/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.0715 - accuracy: 0.7806 - val_loss: 1.7957 - val_accuracy: 0.7551\n",
      "Epoch 27/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1.0048 - accuracy: 0.7806 - val_loss: 1.7363 - val_accuracy: 0.7551\n",
      "Epoch 28/10000\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.9433 - accuracy: 0.7908 - val_loss: 1.6843 - val_accuracy: 0.7755\n",
      "Epoch 29/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.8902 - accuracy: 0.8061 - val_loss: 1.6424 - val_accuracy: 0.7959\n",
      "Epoch 30/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.8379 - accuracy: 0.8214 - val_loss: 1.5956 - val_accuracy: 0.8367\n",
      "Epoch 31/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.7861 - accuracy: 0.8469 - val_loss: 1.5604 - val_accuracy: 0.8367\n",
      "Epoch 32/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.7406 - accuracy: 0.8520 - val_loss: 1.5315 - val_accuracy: 0.8367\n",
      "Epoch 33/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.7021 - accuracy: 0.8622 - val_loss: 1.5021 - val_accuracy: 0.8163\n",
      "Epoch 34/10000\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.6613 - accuracy: 0.8776 - val_loss: 1.4763 - val_accuracy: 0.8163\n",
      "Epoch 35/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.6296 - accuracy: 0.8827 - val_loss: 1.4415 - val_accuracy: 0.8367\n",
      "Epoch 36/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.5884 - accuracy: 0.8622 - val_loss: 1.4241 - val_accuracy: 0.8571\n",
      "Epoch 37/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5586 - accuracy: 0.8827 - val_loss: 1.3920 - val_accuracy: 0.8571\n",
      "Epoch 38/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5299 - accuracy: 0.8827 - val_loss: 1.3504 - val_accuracy: 0.8571\n",
      "Epoch 39/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5098 - accuracy: 0.8827 - val_loss: 1.3220 - val_accuracy: 0.8571\n",
      "Epoch 40/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4847 - accuracy: 0.9031 - val_loss: 1.2957 - val_accuracy: 0.8571\n",
      "Epoch 41/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4675 - accuracy: 0.9082 - val_loss: 1.2758 - val_accuracy: 0.8571\n",
      "Epoch 42/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4469 - accuracy: 0.9082 - val_loss: 1.2580 - val_accuracy: 0.8571\n",
      "Epoch 43/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4281 - accuracy: 0.9133 - val_loss: 1.2377 - val_accuracy: 0.8571\n",
      "Epoch 44/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4114 - accuracy: 0.9082 - val_loss: 1.2257 - val_accuracy: 0.8776\n",
      "Epoch 45/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3929 - accuracy: 0.9133 - val_loss: 1.2184 - val_accuracy: 0.8776\n",
      "Epoch 46/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3770 - accuracy: 0.9133 - val_loss: 1.2071 - val_accuracy: 0.8776\n",
      "Epoch 47/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3609 - accuracy: 0.9184 - val_loss: 1.2003 - val_accuracy: 0.8776\n",
      "Epoch 48/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3495 - accuracy: 0.9184 - val_loss: 1.1894 - val_accuracy: 0.8776\n",
      "Epoch 49/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3343 - accuracy: 0.9184 - val_loss: 1.1809 - val_accuracy: 0.8776\n",
      "Epoch 50/10000\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.3217 - accuracy: 0.9235 - val_loss: 1.1659 - val_accuracy: 0.8776\n",
      "Epoch 51/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.3061 - accuracy: 0.9286 - val_loss: 1.1621 - val_accuracy: 0.8776\n",
      "Epoch 52/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.2932 - accuracy: 0.9337 - val_loss: 1.1472 - val_accuracy: 0.8776\n",
      "Epoch 53/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.2851 - accuracy: 0.9337 - val_loss: 1.1273 - val_accuracy: 0.8776\n",
      "Epoch 54/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.2744 - accuracy: 0.9337 - val_loss: 1.1119 - val_accuracy: 0.8776\n",
      "Epoch 55/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.2626 - accuracy: 0.9439 - val_loss: 1.0911 - val_accuracy: 0.8776\n",
      "Epoch 56/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2536 - accuracy: 0.9490 - val_loss: 1.0743 - val_accuracy: 0.8776\n",
      "Epoch 57/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2319 - accuracy: 0.9490 - val_loss: 1.0491 - val_accuracy: 0.8776\n",
      "Epoch 58/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2231 - accuracy: 0.9541 - val_loss: 1.0299 - val_accuracy: 0.8776\n",
      "Epoch 59/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2143 - accuracy: 0.9592 - val_loss: 1.0186 - val_accuracy: 0.8776\n",
      "Epoch 60/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2018 - accuracy: 0.9592 - val_loss: 1.0071 - val_accuracy: 0.8776\n",
      "Epoch 61/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1912 - accuracy: 0.9643 - val_loss: 0.9945 - val_accuracy: 0.8776\n",
      "Epoch 62/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1825 - accuracy: 0.9592 - val_loss: 0.9910 - val_accuracy: 0.8776\n",
      "Epoch 63/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1731 - accuracy: 0.9592 - val_loss: 0.9932 - val_accuracy: 0.8776\n",
      "Epoch 64/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1653 - accuracy: 0.9592 - val_loss: 0.9894 - val_accuracy: 0.8776\n",
      "Epoch 65/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.1541 - accuracy: 0.9643 - val_loss: 0.9860 - val_accuracy: 0.8776\n",
      "Epoch 66/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.1517 - accuracy: 0.9541 - val_loss: 0.9803 - val_accuracy: 0.8776\n",
      "Epoch 67/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1471 - accuracy: 0.9541 - val_loss: 0.9703 - val_accuracy: 0.8776\n",
      "Epoch 68/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1410 - accuracy: 0.9643 - val_loss: 0.9572 - val_accuracy: 0.8776\n",
      "Epoch 69/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1358 - accuracy: 0.9694 - val_loss: 0.9426 - val_accuracy: 0.8776\n",
      "Epoch 70/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1289 - accuracy: 0.9694 - val_loss: 0.9277 - val_accuracy: 0.8776\n",
      "Epoch 71/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1215 - accuracy: 0.9694 - val_loss: 0.9085 - val_accuracy: 0.8776\n",
      "Epoch 72/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.1179 - accuracy: 0.9694 - val_loss: 0.8894 - val_accuracy: 0.8776\n",
      "Epoch 73/10000\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1136 - accuracy: 0.9694 - val_loss: 0.8741 - val_accuracy: 0.8776\n",
      "Epoch 74/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1137 - accuracy: 0.9694 - val_loss: 0.8608 - val_accuracy: 0.8980\n",
      "Epoch 75/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1074 - accuracy: 0.9745 - val_loss: 0.8513 - val_accuracy: 0.8980\n",
      "Epoch 76/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1052 - accuracy: 0.9745 - val_loss: 0.8435 - val_accuracy: 0.8980\n",
      "Epoch 77/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.1018 - accuracy: 0.9694 - val_loss: 0.8305 - val_accuracy: 0.8980\n",
      "Epoch 78/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0978 - accuracy: 0.9694 - val_loss: 0.8313 - val_accuracy: 0.8776\n",
      "Epoch 79/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0947 - accuracy: 0.9694 - val_loss: 0.8335 - val_accuracy: 0.8776\n",
      "Epoch 80/10000\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0922 - accuracy: 0.9694 - val_loss: 0.8301 - val_accuracy: 0.8776\n",
      "Epoch 81/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0892 - accuracy: 0.9745 - val_loss: 0.8236 - val_accuracy: 0.8776\n",
      "Epoch 82/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0867 - accuracy: 0.9796 - val_loss: 0.8156 - val_accuracy: 0.8776\n",
      "Epoch 83/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0831 - accuracy: 0.9796 - val_loss: 0.8062 - val_accuracy: 0.8776\n",
      "Epoch 84/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0803 - accuracy: 0.9847 - val_loss: 0.8100 - val_accuracy: 0.8776\n",
      "Epoch 85/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0786 - accuracy: 0.9796 - val_loss: 0.8079 - val_accuracy: 0.8776\n",
      "Epoch 86/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0769 - accuracy: 0.9796 - val_loss: 0.8000 - val_accuracy: 0.8980\n",
      "Epoch 87/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0750 - accuracy: 0.9796 - val_loss: 0.7939 - val_accuracy: 0.8980\n",
      "Epoch 88/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0733 - accuracy: 0.9796 - val_loss: 0.7859 - val_accuracy: 0.8980\n",
      "Epoch 89/10000\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0713 - accuracy: 0.9796 - val_loss: 0.7785 - val_accuracy: 0.8980\n",
      "Epoch 90/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0687 - accuracy: 0.9847 - val_loss: 0.7774 - val_accuracy: 0.8980\n",
      "Epoch 91/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0673 - accuracy: 0.9847 - val_loss: 0.7739 - val_accuracy: 0.8980\n",
      "Epoch 92/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0651 - accuracy: 0.9847 - val_loss: 0.7704 - val_accuracy: 0.8980\n",
      "Epoch 93/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0634 - accuracy: 0.9847 - val_loss: 0.7635 - val_accuracy: 0.8980\n",
      "Epoch 94/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0617 - accuracy: 0.9847 - val_loss: 0.7572 - val_accuracy: 0.8980\n",
      "Epoch 95/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0606 - accuracy: 0.9847 - val_loss: 0.7499 - val_accuracy: 0.8980\n",
      "Epoch 96/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0594 - accuracy: 0.9847 - val_loss: 0.7446 - val_accuracy: 0.8980\n",
      "Epoch 97/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0561 - accuracy: 0.9847 - val_loss: 0.7330 - val_accuracy: 0.8980\n",
      "Epoch 98/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0593 - accuracy: 0.9796 - val_loss: 0.7270 - val_accuracy: 0.8980\n",
      "Epoch 99/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0573 - accuracy: 0.9898 - val_loss: 0.7244 - val_accuracy: 0.8980\n",
      "Epoch 100/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0556 - accuracy: 0.9898 - val_loss: 0.7231 - val_accuracy: 0.8980\n",
      "Epoch 101/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0538 - accuracy: 0.9898 - val_loss: 0.7219 - val_accuracy: 0.8980\n",
      "Epoch 102/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0524 - accuracy: 0.9898 - val_loss: 0.7197 - val_accuracy: 0.8980\n",
      "Epoch 103/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0513 - accuracy: 0.9898 - val_loss: 0.7173 - val_accuracy: 0.8980\n",
      "Epoch 104/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0509 - accuracy: 0.9847 - val_loss: 0.7117 - val_accuracy: 0.8980\n",
      "Epoch 105/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0472 - accuracy: 0.9847 - val_loss: 0.7037 - val_accuracy: 0.8980\n",
      "Epoch 106/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0450 - accuracy: 0.9847 - val_loss: 0.7004 - val_accuracy: 0.8980\n",
      "Epoch 107/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0440 - accuracy: 0.9898 - val_loss: 0.7005 - val_accuracy: 0.8980\n",
      "Epoch 108/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0431 - accuracy: 0.9898 - val_loss: 0.7024 - val_accuracy: 0.8980\n",
      "Epoch 109/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0431 - accuracy: 0.9898 - val_loss: 0.7018 - val_accuracy: 0.8980\n",
      "Epoch 110/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0419 - accuracy: 0.9898 - val_loss: 0.7010 - val_accuracy: 0.8980\n",
      "Epoch 111/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0410 - accuracy: 0.9898 - val_loss: 0.6991 - val_accuracy: 0.8980\n",
      "Epoch 112/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0401 - accuracy: 0.9898 - val_loss: 0.6981 - val_accuracy: 0.8980\n",
      "Epoch 113/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0394 - accuracy: 0.9949 - val_loss: 0.6977 - val_accuracy: 0.8980\n",
      "Epoch 114/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0385 - accuracy: 0.9949 - val_loss: 0.6961 - val_accuracy: 0.8980\n",
      "Epoch 115/10000\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0377 - accuracy: 0.9949 - val_loss: 0.6944 - val_accuracy: 0.8980\n",
      "Epoch 116/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0371 - accuracy: 0.9949 - val_loss: 0.6936 - val_accuracy: 0.8980\n",
      "Epoch 117/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0362 - accuracy: 0.9949 - val_loss: 0.6969 - val_accuracy: 0.8980\n",
      "Epoch 118/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0362 - accuracy: 0.9949 - val_loss: 0.6975 - val_accuracy: 0.8980\n",
      "Epoch 119/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0364 - accuracy: 0.9949 - val_loss: 0.6950 - val_accuracy: 0.8980\n",
      "Epoch 120/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0351 - accuracy: 0.9949 - val_loss: 0.6912 - val_accuracy: 0.8980\n",
      "Epoch 121/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0340 - accuracy: 0.9949 - val_loss: 0.6874 - val_accuracy: 0.8980\n",
      "Epoch 122/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0336 - accuracy: 0.9949 - val_loss: 0.6844 - val_accuracy: 0.8980\n",
      "Epoch 123/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0327 - accuracy: 0.9949 - val_loss: 0.6817 - val_accuracy: 0.8980\n",
      "Epoch 124/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0322 - accuracy: 0.9949 - val_loss: 0.6776 - val_accuracy: 0.8980\n",
      "Epoch 125/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0317 - accuracy: 0.9949 - val_loss: 0.6761 - val_accuracy: 0.8980\n",
      "Epoch 126/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0310 - accuracy: 0.9949 - val_loss: 0.6742 - val_accuracy: 0.8980\n",
      "Epoch 127/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0305 - accuracy: 0.9949 - val_loss: 0.6728 - val_accuracy: 0.8980\n",
      "Epoch 128/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0299 - accuracy: 0.9949 - val_loss: 0.6715 - val_accuracy: 0.8980\n",
      "Epoch 129/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0296 - accuracy: 0.9949 - val_loss: 0.6707 - val_accuracy: 0.8980\n",
      "Epoch 130/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0292 - accuracy: 0.9949 - val_loss: 0.6693 - val_accuracy: 0.8980\n",
      "Epoch 131/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0287 - accuracy: 0.9949 - val_loss: 0.6683 - val_accuracy: 0.8980\n",
      "Epoch 132/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0282 - accuracy: 0.9949 - val_loss: 0.6662 - val_accuracy: 0.8980\n",
      "Epoch 133/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0281 - accuracy: 0.9949 - val_loss: 0.6637 - val_accuracy: 0.8980\n",
      "Epoch 134/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0270 - accuracy: 0.9949 - val_loss: 0.6610 - val_accuracy: 0.8980\n",
      "Epoch 135/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0278 - accuracy: 0.9949 - val_loss: 0.6595 - val_accuracy: 0.8980\n",
      "Epoch 136/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0273 - accuracy: 0.9949 - val_loss: 0.6582 - val_accuracy: 0.8980\n",
      "Epoch 137/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0265 - accuracy: 0.9949 - val_loss: 0.6552 - val_accuracy: 0.8980\n",
      "Epoch 138/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0260 - accuracy: 0.9949 - val_loss: 0.6540 - val_accuracy: 0.8980\n",
      "Epoch 139/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0256 - accuracy: 0.9949 - val_loss: 0.6543 - val_accuracy: 0.8980\n",
      "Epoch 140/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0249 - accuracy: 0.9949 - val_loss: 0.6545 - val_accuracy: 0.8980\n",
      "Epoch 141/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0246 - accuracy: 0.9949 - val_loss: 0.6545 - val_accuracy: 0.8980\n",
      "Epoch 142/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0242 - accuracy: 0.9949 - val_loss: 0.6568 - val_accuracy: 0.8980\n",
      "Epoch 143/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0240 - accuracy: 0.9949 - val_loss: 0.6573 - val_accuracy: 0.8980\n",
      "Epoch 144/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0238 - accuracy: 1.0000 - val_loss: 0.6561 - val_accuracy: 0.8980\n",
      "Epoch 145/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0235 - accuracy: 1.0000 - val_loss: 0.6557 - val_accuracy: 0.8980\n",
      "Epoch 146/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0231 - accuracy: 1.0000 - val_loss: 0.6527 - val_accuracy: 0.8980\n",
      "Epoch 147/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0227 - accuracy: 1.0000 - val_loss: 0.6460 - val_accuracy: 0.8980\n",
      "Epoch 148/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0223 - accuracy: 1.0000 - val_loss: 0.6419 - val_accuracy: 0.8980\n",
      "Epoch 149/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0220 - accuracy: 1.0000 - val_loss: 0.6395 - val_accuracy: 0.8980\n",
      "Epoch 150/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0216 - accuracy: 1.0000 - val_loss: 0.6391 - val_accuracy: 0.8980\n",
      "Epoch 151/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0212 - accuracy: 1.0000 - val_loss: 0.6427 - val_accuracy: 0.8980\n",
      "Epoch 152/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0213 - accuracy: 1.0000 - val_loss: 0.6438 - val_accuracy: 0.8980\n",
      "Epoch 153/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0210 - accuracy: 1.0000 - val_loss: 0.6436 - val_accuracy: 0.8980\n",
      "Epoch 154/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0208 - accuracy: 1.0000 - val_loss: 0.6436 - val_accuracy: 0.8980\n",
      "Epoch 155/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0205 - accuracy: 1.0000 - val_loss: 0.6434 - val_accuracy: 0.8980\n",
      "Epoch 156/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0203 - accuracy: 1.0000 - val_loss: 0.6415 - val_accuracy: 0.8980\n",
      "Epoch 157/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0199 - accuracy: 1.0000 - val_loss: 0.6387 - val_accuracy: 0.8980\n",
      "Epoch 158/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0196 - accuracy: 1.0000 - val_loss: 0.6367 - val_accuracy: 0.8980\n",
      "Epoch 159/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0193 - accuracy: 1.0000 - val_loss: 0.6342 - val_accuracy: 0.8980\n",
      "Epoch 160/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0190 - accuracy: 1.0000 - val_loss: 0.6312 - val_accuracy: 0.8980\n",
      "Epoch 161/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0189 - accuracy: 1.0000 - val_loss: 0.6286 - val_accuracy: 0.8980\n",
      "Epoch 162/10000\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0186 - accuracy: 1.0000 - val_loss: 0.6262 - val_accuracy: 0.8980\n",
      "Epoch 163/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0184 - accuracy: 1.0000 - val_loss: 0.6253 - val_accuracy: 0.8980\n",
      "Epoch 164/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0182 - accuracy: 1.0000 - val_loss: 0.6241 - val_accuracy: 0.8980\n",
      "Epoch 165/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0180 - accuracy: 1.0000 - val_loss: 0.6230 - val_accuracy: 0.8980\n",
      "Epoch 166/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0178 - accuracy: 1.0000 - val_loss: 0.6222 - val_accuracy: 0.8980\n",
      "Epoch 167/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0176 - accuracy: 1.0000 - val_loss: 0.6217 - val_accuracy: 0.8980\n",
      "Epoch 168/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0174 - accuracy: 1.0000 - val_loss: 0.6233 - val_accuracy: 0.8980\n",
      "Epoch 169/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0171 - accuracy: 1.0000 - val_loss: 0.6269 - val_accuracy: 0.8980\n",
      "Epoch 170/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0170 - accuracy: 1.0000 - val_loss: 0.6293 - val_accuracy: 0.8980\n",
      "Epoch 171/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0168 - accuracy: 1.0000 - val_loss: 0.6299 - val_accuracy: 0.8980\n",
      "Epoch 172/10000\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.0167 - accuracy: 1.0000 - val_loss: 0.6292 - val_accuracy: 0.8980\n",
      "Epoch 173/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0164 - accuracy: 1.0000 - val_loss: 0.6270 - val_accuracy: 0.8980\n",
      "Epoch 174/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0162 - accuracy: 1.0000 - val_loss: 0.6227 - val_accuracy: 0.8980\n",
      "Epoch 175/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0160 - accuracy: 1.0000 - val_loss: 0.6210 - val_accuracy: 0.8980\n",
      "Epoch 176/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0158 - accuracy: 1.0000 - val_loss: 0.6187 - val_accuracy: 0.8980\n",
      "Epoch 177/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0156 - accuracy: 1.0000 - val_loss: 0.6158 - val_accuracy: 0.8980\n",
      "Epoch 178/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0154 - accuracy: 1.0000 - val_loss: 0.6146 - val_accuracy: 0.8980\n",
      "Epoch 179/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0153 - accuracy: 1.0000 - val_loss: 0.6135 - val_accuracy: 0.8980\n",
      "Epoch 180/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0151 - accuracy: 1.0000 - val_loss: 0.6127 - val_accuracy: 0.8980\n",
      "Epoch 181/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0149 - accuracy: 1.0000 - val_loss: 0.6113 - val_accuracy: 0.8980\n",
      "Epoch 182/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0148 - accuracy: 1.0000 - val_loss: 0.6104 - val_accuracy: 0.8980\n",
      "Epoch 183/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0146 - accuracy: 1.0000 - val_loss: 0.6096 - val_accuracy: 0.8980\n",
      "Epoch 184/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0145 - accuracy: 1.0000 - val_loss: 0.6093 - val_accuracy: 0.8980\n",
      "Epoch 185/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0150 - accuracy: 1.0000 - val_loss: 0.6081 - val_accuracy: 0.8980\n",
      "Epoch 186/10000\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0149 - accuracy: 1.0000 - val_loss: 0.6079 - val_accuracy: 0.8980\n",
      "Epoch 187/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0146 - accuracy: 1.0000 - val_loss: 0.6071 - val_accuracy: 0.8980\n",
      "Epoch 188/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0142 - accuracy: 1.0000 - val_loss: 0.6061 - val_accuracy: 0.8980\n",
      "Epoch 189/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0138 - accuracy: 1.0000 - val_loss: 0.6061 - val_accuracy: 0.8980\n",
      "Epoch 190/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0136 - accuracy: 1.0000 - val_loss: 0.6072 - val_accuracy: 0.8980\n",
      "Epoch 191/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0134 - accuracy: 1.0000 - val_loss: 0.6074 - val_accuracy: 0.8980\n",
      "Epoch 192/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0133 - accuracy: 1.0000 - val_loss: 0.6072 - val_accuracy: 0.8980\n",
      "Epoch 193/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0132 - accuracy: 1.0000 - val_loss: 0.6068 - val_accuracy: 0.8980\n",
      "Epoch 194/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0132 - accuracy: 1.0000 - val_loss: 0.6064 - val_accuracy: 0.8980\n",
      "Epoch 195/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0131 - accuracy: 1.0000 - val_loss: 0.6074 - val_accuracy: 0.8980\n",
      "Epoch 196/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0128 - accuracy: 1.0000 - val_loss: 0.6070 - val_accuracy: 0.8980\n",
      "Epoch 197/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0126 - accuracy: 1.0000 - val_loss: 0.6068 - val_accuracy: 0.8980\n",
      "Epoch 198/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0123 - accuracy: 1.0000 - val_loss: 0.6065 - val_accuracy: 0.8980\n",
      "訓練時間：4.52 秒\n",
      "552/552 [==============================] - 0s 749us/step - loss: 0.6241 - accuracy: 0.8877\n",
      "Evaluation on 90% unused data - Loss: 0.6241, Accuracy: 0.8877\n",
      "559/559 [==============================] - 0s 728us/step\n",
      "Test Data MDE report saved to: 5 data per RP transfer revised/DNN 4 mcAPs BEST_5data_2024_12_21.json\n",
      "\n",
      "Test Data Mean MDE: 0.1620 meters\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_44 (Dense)            (None, 64)                832       \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 49)                3185      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20593 (80.44 KB)\n",
      "Trainable params: 3185 (12.44 KB)\n",
      "Non-trainable params: 17408 (68.00 KB)\n",
      "_________________________________________________________________\n",
      "Number of rows with NaN values: 1543\n",
      "Number of rows with NaN values: 0\n",
      "Final reverse_label_mapping in DNN: {'1-1': 10, '1-2': 9, '1-3': 8, '1-4': 7, '1-5': 6, '1-6': 5, '1-7': 4, '1-8': 3, '1-9': 2, '1-10': 1, '1-11': 0, '2-1': 11, '2-11': 29, '3-1': 12, '3-11': 28, '4-1': 13, '4-11': 27, '5-1': 14, '5-11': 26, '6-1': 15, '6-2': 16, '6-3': 17, '6-4': 18, '6-5': 19, '6-6': 20, '6-7': 21, '6-8': 22, '6-9': 23, '6-10': 24, '6-11': 25, '7-1': 48, '7-11': 30, '8-1': 47, '8-11': 31, '9-1': 46, '9-11': 32, '10-1': 45, '10-11': 33, '11-1': 44, '11-2': 43, '11-3': 42, '11-4': 41, '11-5': 40, '11-6': 39, '11-7': 38, '11-8': 37, '11-9': 36, '11-10': 35, '11-11': 34}\n",
      "y_numeric unique values in DNN: [10  1  0  9  8  7  6  5  4  3  2 45 33 44 35 34 43 42 41 40 39 38 37 36\n",
      " 11 29 12 28 13 27 14 26 15 24 25 16 17 18 19 20 21 22 23 48 30 47 31 46\n",
      " 32]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "416\n",
      "label\n",
      "0     5\n",
      "25    5\n",
      "27    5\n",
      "28    5\n",
      "29    5\n",
      "30    5\n",
      "31    5\n",
      "32    5\n",
      "33    5\n",
      "34    5\n",
      "35    5\n",
      "36    5\n",
      "37    5\n",
      "38    5\n",
      "39    5\n",
      "40    5\n",
      "41    5\n",
      "42    5\n",
      "43    5\n",
      "44    5\n",
      "45    5\n",
      "46    5\n",
      "47    5\n",
      "26    5\n",
      "24    5\n",
      "1     5\n",
      "23    5\n",
      "2     5\n",
      "3     5\n",
      "4     5\n",
      "5     5\n",
      "6     5\n",
      "7     5\n",
      "8     5\n",
      "9     5\n",
      "10    5\n",
      "11    5\n",
      "12    5\n",
      "13    5\n",
      "14    5\n",
      "15    5\n",
      "16    5\n",
      "17    5\n",
      "18    5\n",
      "19    5\n",
      "20    5\n",
      "21    5\n",
      "22    5\n",
      "48    5\n",
      "Name: count, dtype: int64\n",
      "20139\n",
      "Training set: 196 samples, 49 unique labels\n",
      "Validation set: 49 samples, 49 unique labels\n",
      "Test set: 20139 samples, 49 unique labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcslab/anaconda3/envs/yang_cuda/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n",
      "/home/mcslab/anaconda3/envs/yang_cuda/lib/python3.10/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.3.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_1914507/3564637810.py:45: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  test_processed_data = pd.concat([test_processed_data, group], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Set</th>\n",
       "      <th>Validation Set</th>\n",
       "      <th>Test Set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Training Set  Validation Set  Test Set\n",
       "0              4               1       411\n",
       "1              4               1       411\n",
       "2              4               1       411\n",
       "3              4               1       411\n",
       "4              4               1       411\n",
       "5              4               1       411\n",
       "6              4               1       411\n",
       "7              4               1       411\n",
       "8              4               1       411\n",
       "9              4               1       411\n",
       "10             4               1       411\n",
       "11             4               1       411\n",
       "12             4               1       411\n",
       "13             4               1       411\n",
       "14             4               1       411\n",
       "15             4               1       411\n",
       "16             4               1       411\n",
       "17             4               1       411\n",
       "18             4               1       411\n",
       "19             4               1       411\n",
       "20             4               1       411\n",
       "21             4               1       411\n",
       "22             4               1       411\n",
       "23             4               1       411\n",
       "24             4               1       411\n",
       "25             4               1       411\n",
       "26             4               1       411\n",
       "27             4               1       411\n",
       "28             4               1       411\n",
       "29             4               1       411\n",
       "30             4               1       411\n",
       "31             4               1       411\n",
       "32             4               1       411\n",
       "33             4               1       411\n",
       "34             4               1       411\n",
       "35             4               1       411\n",
       "36             4               1       411\n",
       "37             4               1       411\n",
       "38             4               1       411\n",
       "39             4               1       411\n",
       "40             4               1       411\n",
       "41             4               1       411\n",
       "42             4               1       411\n",
       "43             4               1       411\n",
       "44             4               1       411\n",
       "45             4               1       411\n",
       "46             4               1       411\n",
       "47             4               1       411\n",
       "48             4               1       411"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "Epoch 1/10000\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 6.4745 - accuracy: 0.3316 - val_loss: 5.8464 - val_accuracy: 0.3673\n",
      "Epoch 2/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.8608 - accuracy: 0.3827 - val_loss: 5.4506 - val_accuracy: 0.3673\n",
      "Epoch 3/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 5.4013 - accuracy: 0.4184 - val_loss: 5.1050 - val_accuracy: 0.4286\n",
      "Epoch 4/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 5.0128 - accuracy: 0.4286 - val_loss: 4.8234 - val_accuracy: 0.4286\n",
      "Epoch 5/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 4.6689 - accuracy: 0.4490 - val_loss: 4.5907 - val_accuracy: 0.4286\n",
      "Epoch 6/10000\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 4.3710 - accuracy: 0.4694 - val_loss: 4.4111 - val_accuracy: 0.4490\n",
      "Epoch 7/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.1049 - accuracy: 0.5000 - val_loss: 4.2441 - val_accuracy: 0.4490\n",
      "Epoch 8/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.8754 - accuracy: 0.5204 - val_loss: 4.0760 - val_accuracy: 0.4694\n",
      "Epoch 9/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 3.6573 - accuracy: 0.5408 - val_loss: 3.9345 - val_accuracy: 0.4490\n",
      "Epoch 10/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.4773 - accuracy: 0.5510 - val_loss: 3.8106 - val_accuracy: 0.4694\n",
      "Epoch 11/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.2964 - accuracy: 0.5612 - val_loss: 3.6838 - val_accuracy: 0.4694\n",
      "Epoch 12/10000\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 3.1316 - accuracy: 0.5867 - val_loss: 3.5736 - val_accuracy: 0.4694\n",
      "Epoch 13/10000\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 2.9663 - accuracy: 0.6020 - val_loss: 3.4580 - val_accuracy: 0.4898\n",
      "Epoch 14/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 2.8302 - accuracy: 0.6224 - val_loss: 3.3525 - val_accuracy: 0.5306\n",
      "Epoch 15/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 2.6896 - accuracy: 0.6276 - val_loss: 3.2661 - val_accuracy: 0.5306\n",
      "Epoch 16/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 2.5645 - accuracy: 0.6531 - val_loss: 3.1608 - val_accuracy: 0.5918\n",
      "Epoch 17/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 2.4461 - accuracy: 0.6684 - val_loss: 3.0540 - val_accuracy: 0.5918\n",
      "Epoch 18/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 2.3340 - accuracy: 0.6735 - val_loss: 2.9684 - val_accuracy: 0.5918\n",
      "Epoch 19/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 2.2137 - accuracy: 0.6786 - val_loss: 2.8743 - val_accuracy: 0.5918\n",
      "Epoch 20/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.0980 - accuracy: 0.6990 - val_loss: 2.8022 - val_accuracy: 0.6122\n",
      "Epoch 21/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.9867 - accuracy: 0.6990 - val_loss: 2.7398 - val_accuracy: 0.6122\n",
      "Epoch 22/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.8872 - accuracy: 0.7143 - val_loss: 2.6833 - val_accuracy: 0.6327\n",
      "Epoch 23/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.7783 - accuracy: 0.7194 - val_loss: 2.6047 - val_accuracy: 0.6327\n",
      "Epoch 24/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.6851 - accuracy: 0.7296 - val_loss: 2.5314 - val_accuracy: 0.6327\n",
      "Epoch 25/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.5994 - accuracy: 0.7551 - val_loss: 2.4473 - val_accuracy: 0.6735\n",
      "Epoch 26/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5141 - accuracy: 0.7653 - val_loss: 2.3802 - val_accuracy: 0.6735\n",
      "Epoch 27/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.4329 - accuracy: 0.7653 - val_loss: 2.3055 - val_accuracy: 0.6939\n",
      "Epoch 28/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.3666 - accuracy: 0.7806 - val_loss: 2.2428 - val_accuracy: 0.6939\n",
      "Epoch 29/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1.3060 - accuracy: 0.7959 - val_loss: 2.1892 - val_accuracy: 0.6939\n",
      "Epoch 30/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.2425 - accuracy: 0.7959 - val_loss: 2.1473 - val_accuracy: 0.6939\n",
      "Epoch 31/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.1679 - accuracy: 0.8061 - val_loss: 2.1060 - val_accuracy: 0.6939\n",
      "Epoch 32/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1.1101 - accuracy: 0.8163 - val_loss: 2.0669 - val_accuracy: 0.6939\n",
      "Epoch 33/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.0528 - accuracy: 0.8316 - val_loss: 2.0144 - val_accuracy: 0.6939\n",
      "Epoch 34/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.0051 - accuracy: 0.8418 - val_loss: 1.9606 - val_accuracy: 0.6939\n",
      "Epoch 35/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.9429 - accuracy: 0.8316 - val_loss: 1.9162 - val_accuracy: 0.6939\n",
      "Epoch 36/10000\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.8901 - accuracy: 0.8418 - val_loss: 1.8913 - val_accuracy: 0.6939\n",
      "Epoch 37/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.8450 - accuracy: 0.8418 - val_loss: 1.8595 - val_accuracy: 0.6939\n",
      "Epoch 38/10000\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.8006 - accuracy: 0.8418 - val_loss: 1.8191 - val_accuracy: 0.6939\n",
      "Epoch 39/10000\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.7583 - accuracy: 0.8418 - val_loss: 1.7886 - val_accuracy: 0.7143\n",
      "Epoch 40/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.7251 - accuracy: 0.8571 - val_loss: 1.7600 - val_accuracy: 0.7143\n",
      "Epoch 41/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.6835 - accuracy: 0.8571 - val_loss: 1.7378 - val_accuracy: 0.7143\n",
      "Epoch 42/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.6410 - accuracy: 0.8622 - val_loss: 1.7182 - val_accuracy: 0.6939\n",
      "Epoch 43/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.6051 - accuracy: 0.8622 - val_loss: 1.7177 - val_accuracy: 0.6939\n",
      "Epoch 44/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5690 - accuracy: 0.8724 - val_loss: 1.7064 - val_accuracy: 0.6939\n",
      "Epoch 45/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5309 - accuracy: 0.8724 - val_loss: 1.6829 - val_accuracy: 0.6939\n",
      "Epoch 46/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4922 - accuracy: 0.8827 - val_loss: 1.6446 - val_accuracy: 0.7347\n",
      "Epoch 47/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4603 - accuracy: 0.8980 - val_loss: 1.6146 - val_accuracy: 0.7143\n",
      "Epoch 48/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4321 - accuracy: 0.9133 - val_loss: 1.6022 - val_accuracy: 0.7143\n",
      "Epoch 49/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4097 - accuracy: 0.9184 - val_loss: 1.5925 - val_accuracy: 0.7143\n",
      "Epoch 50/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3791 - accuracy: 0.9235 - val_loss: 1.5862 - val_accuracy: 0.7143\n",
      "Epoch 51/10000\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.3511 - accuracy: 0.9286 - val_loss: 1.5980 - val_accuracy: 0.7347\n",
      "Epoch 52/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.3292 - accuracy: 0.9286 - val_loss: 1.6018 - val_accuracy: 0.7143\n",
      "Epoch 53/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.3081 - accuracy: 0.9286 - val_loss: 1.5912 - val_accuracy: 0.7347\n",
      "Epoch 54/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2873 - accuracy: 0.9337 - val_loss: 1.5791 - val_accuracy: 0.7347\n",
      "Epoch 55/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2675 - accuracy: 0.9439 - val_loss: 1.5643 - val_accuracy: 0.7551\n",
      "Epoch 56/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.2478 - accuracy: 0.9439 - val_loss: 1.5505 - val_accuracy: 0.7551\n",
      "Epoch 57/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2307 - accuracy: 0.9439 - val_loss: 1.5181 - val_accuracy: 0.7755\n",
      "Epoch 58/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2121 - accuracy: 0.9490 - val_loss: 1.4994 - val_accuracy: 0.7755\n",
      "Epoch 59/10000\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2018 - accuracy: 0.9490 - val_loss: 1.4981 - val_accuracy: 0.7551\n",
      "Epoch 60/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1848 - accuracy: 0.9541 - val_loss: 1.4979 - val_accuracy: 0.7755\n",
      "Epoch 61/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.1697 - accuracy: 0.9541 - val_loss: 1.4979 - val_accuracy: 0.7959\n",
      "Epoch 62/10000\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1555 - accuracy: 0.9592 - val_loss: 1.4913 - val_accuracy: 0.7959\n",
      "Epoch 63/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1412 - accuracy: 0.9643 - val_loss: 1.4849 - val_accuracy: 0.7959\n",
      "Epoch 64/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.1303 - accuracy: 0.9592 - val_loss: 1.4594 - val_accuracy: 0.7959\n",
      "Epoch 65/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1176 - accuracy: 0.9643 - val_loss: 1.4486 - val_accuracy: 0.7959\n",
      "Epoch 66/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1111 - accuracy: 0.9643 - val_loss: 1.4421 - val_accuracy: 0.7959\n",
      "Epoch 67/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1042 - accuracy: 0.9694 - val_loss: 1.4440 - val_accuracy: 0.7959\n",
      "Epoch 68/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0968 - accuracy: 0.9796 - val_loss: 1.4542 - val_accuracy: 0.7959\n",
      "Epoch 69/10000\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0905 - accuracy: 0.9796 - val_loss: 1.4596 - val_accuracy: 0.7959\n",
      "Epoch 70/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0870 - accuracy: 0.9847 - val_loss: 1.4632 - val_accuracy: 0.7959\n",
      "Epoch 71/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0832 - accuracy: 0.9847 - val_loss: 1.4683 - val_accuracy: 0.7959\n",
      "Epoch 72/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0795 - accuracy: 0.9847 - val_loss: 1.4677 - val_accuracy: 0.7959\n",
      "Epoch 73/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0755 - accuracy: 0.9898 - val_loss: 1.4593 - val_accuracy: 0.7959\n",
      "Epoch 74/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0724 - accuracy: 0.9949 - val_loss: 1.4510 - val_accuracy: 0.7959\n",
      "Epoch 75/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0694 - accuracy: 0.9949 - val_loss: 1.4430 - val_accuracy: 0.7959\n",
      "Epoch 76/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0666 - accuracy: 0.9949 - val_loss: 1.4370 - val_accuracy: 0.7959\n",
      "Epoch 77/10000\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0637 - accuracy: 0.9949 - val_loss: 1.4313 - val_accuracy: 0.7959\n",
      "Epoch 78/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0616 - accuracy: 0.9949 - val_loss: 1.4256 - val_accuracy: 0.8163\n",
      "Epoch 79/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0593 - accuracy: 0.9949 - val_loss: 1.4214 - val_accuracy: 0.8367\n",
      "Epoch 80/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0575 - accuracy: 0.9949 - val_loss: 1.4182 - val_accuracy: 0.8367\n",
      "Epoch 81/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0556 - accuracy: 1.0000 - val_loss: 1.4141 - val_accuracy: 0.8367\n",
      "Epoch 82/10000\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0540 - accuracy: 1.0000 - val_loss: 1.4110 - val_accuracy: 0.8367\n",
      "Epoch 83/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0525 - accuracy: 1.0000 - val_loss: 1.4089 - val_accuracy: 0.8163\n",
      "Epoch 84/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0514 - accuracy: 1.0000 - val_loss: 1.4060 - val_accuracy: 0.8163\n",
      "Epoch 85/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0501 - accuracy: 1.0000 - val_loss: 1.4031 - val_accuracy: 0.8163\n",
      "Epoch 86/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0482 - accuracy: 1.0000 - val_loss: 1.3993 - val_accuracy: 0.8163\n",
      "Epoch 87/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0472 - accuracy: 1.0000 - val_loss: 1.3976 - val_accuracy: 0.8163\n",
      "Epoch 88/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0455 - accuracy: 1.0000 - val_loss: 1.3970 - val_accuracy: 0.8163\n",
      "Epoch 89/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0441 - accuracy: 1.0000 - val_loss: 1.3943 - val_accuracy: 0.8571\n",
      "Epoch 90/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0429 - accuracy: 1.0000 - val_loss: 1.3934 - val_accuracy: 0.8571\n",
      "Epoch 91/10000\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.0416 - accuracy: 1.0000 - val_loss: 1.3932 - val_accuracy: 0.8571\n",
      "Epoch 92/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0405 - accuracy: 1.0000 - val_loss: 1.3931 - val_accuracy: 0.8571\n",
      "Epoch 93/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0393 - accuracy: 1.0000 - val_loss: 1.3930 - val_accuracy: 0.8571\n",
      "Epoch 94/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0383 - accuracy: 1.0000 - val_loss: 1.3900 - val_accuracy: 0.8571\n",
      "Epoch 95/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0371 - accuracy: 1.0000 - val_loss: 1.3910 - val_accuracy: 0.8571\n",
      "Epoch 96/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0370 - accuracy: 1.0000 - val_loss: 1.3921 - val_accuracy: 0.8571\n",
      "Epoch 97/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0363 - accuracy: 1.0000 - val_loss: 1.3927 - val_accuracy: 0.8571\n",
      "Epoch 98/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0354 - accuracy: 1.0000 - val_loss: 1.3939 - val_accuracy: 0.8571\n",
      "Epoch 99/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0341 - accuracy: 1.0000 - val_loss: 1.3961 - val_accuracy: 0.8571\n",
      "Epoch 100/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0333 - accuracy: 1.0000 - val_loss: 1.3970 - val_accuracy: 0.8571\n",
      "Epoch 101/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0325 - accuracy: 1.0000 - val_loss: 1.3947 - val_accuracy: 0.8571\n",
      "Epoch 102/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0317 - accuracy: 1.0000 - val_loss: 1.3919 - val_accuracy: 0.8571\n",
      "Epoch 103/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0312 - accuracy: 1.0000 - val_loss: 1.3909 - val_accuracy: 0.8571\n",
      "Epoch 104/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0305 - accuracy: 1.0000 - val_loss: 1.3894 - val_accuracy: 0.8571\n",
      "Epoch 105/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0297 - accuracy: 1.0000 - val_loss: 1.3884 - val_accuracy: 0.8571\n",
      "Epoch 106/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0290 - accuracy: 1.0000 - val_loss: 1.3928 - val_accuracy: 0.8571\n",
      "Epoch 107/10000\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.0283 - accuracy: 1.0000 - val_loss: 1.3936 - val_accuracy: 0.8571\n",
      "Epoch 108/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0277 - accuracy: 1.0000 - val_loss: 1.3915 - val_accuracy: 0.8571\n",
      "Epoch 109/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0272 - accuracy: 1.0000 - val_loss: 1.3832 - val_accuracy: 0.8571\n",
      "Epoch 110/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0267 - accuracy: 1.0000 - val_loss: 1.3788 - val_accuracy: 0.8571\n",
      "Epoch 111/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 1.0000 - val_loss: 1.3759 - val_accuracy: 0.8571\n",
      "Epoch 112/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0258 - accuracy: 1.0000 - val_loss: 1.3732 - val_accuracy: 0.8571\n",
      "Epoch 113/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0253 - accuracy: 1.0000 - val_loss: 1.3712 - val_accuracy: 0.8571\n",
      "Epoch 114/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0253 - accuracy: 1.0000 - val_loss: 1.3715 - val_accuracy: 0.8571\n",
      "Epoch 115/10000\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0248 - accuracy: 1.0000 - val_loss: 1.3714 - val_accuracy: 0.8571\n",
      "Epoch 116/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0245 - accuracy: 1.0000 - val_loss: 1.3695 - val_accuracy: 0.8571\n",
      "Epoch 117/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0237 - accuracy: 1.0000 - val_loss: 1.3686 - val_accuracy: 0.8571\n",
      "Epoch 118/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0232 - accuracy: 1.0000 - val_loss: 1.3682 - val_accuracy: 0.8571\n",
      "Epoch 119/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0228 - accuracy: 1.0000 - val_loss: 1.3673 - val_accuracy: 0.8571\n",
      "Epoch 120/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0225 - accuracy: 1.0000 - val_loss: 1.3655 - val_accuracy: 0.8571\n",
      "Epoch 121/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0221 - accuracy: 1.0000 - val_loss: 1.3640 - val_accuracy: 0.8571\n",
      "Epoch 122/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0219 - accuracy: 1.0000 - val_loss: 1.3634 - val_accuracy: 0.8571\n",
      "Epoch 123/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0216 - accuracy: 1.0000 - val_loss: 1.3620 - val_accuracy: 0.8571\n",
      "Epoch 124/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0212 - accuracy: 1.0000 - val_loss: 1.3601 - val_accuracy: 0.8571\n",
      "Epoch 125/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0209 - accuracy: 1.0000 - val_loss: 1.3575 - val_accuracy: 0.8571\n",
      "Epoch 126/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0206 - accuracy: 1.0000 - val_loss: 1.3546 - val_accuracy: 0.8571\n",
      "Epoch 127/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0203 - accuracy: 1.0000 - val_loss: 1.3537 - val_accuracy: 0.8571\n",
      "Epoch 128/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0199 - accuracy: 1.0000 - val_loss: 1.3532 - val_accuracy: 0.8571\n",
      "Epoch 129/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0197 - accuracy: 1.0000 - val_loss: 1.3529 - val_accuracy: 0.8571\n",
      "Epoch 130/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0195 - accuracy: 1.0000 - val_loss: 1.3525 - val_accuracy: 0.8571\n",
      "Epoch 131/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0192 - accuracy: 1.0000 - val_loss: 1.3516 - val_accuracy: 0.8571\n",
      "Epoch 132/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0189 - accuracy: 1.0000 - val_loss: 1.3473 - val_accuracy: 0.8571\n",
      "Epoch 133/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0187 - accuracy: 1.0000 - val_loss: 1.3459 - val_accuracy: 0.8571\n",
      "Epoch 134/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0184 - accuracy: 1.0000 - val_loss: 1.3440 - val_accuracy: 0.8571\n",
      "Epoch 135/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0182 - accuracy: 1.0000 - val_loss: 1.3432 - val_accuracy: 0.8776\n",
      "Epoch 136/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0179 - accuracy: 1.0000 - val_loss: 1.3437 - val_accuracy: 0.8571\n",
      "Epoch 137/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0175 - accuracy: 1.0000 - val_loss: 1.3445 - val_accuracy: 0.8571\n",
      "Epoch 138/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0174 - accuracy: 1.0000 - val_loss: 1.3442 - val_accuracy: 0.8571\n",
      "Epoch 139/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0172 - accuracy: 1.0000 - val_loss: 1.3438 - val_accuracy: 0.8571\n",
      "Epoch 140/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0170 - accuracy: 1.0000 - val_loss: 1.3429 - val_accuracy: 0.8571\n",
      "Epoch 141/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0168 - accuracy: 1.0000 - val_loss: 1.3405 - val_accuracy: 0.8776\n",
      "Epoch 142/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0165 - accuracy: 1.0000 - val_loss: 1.3411 - val_accuracy: 0.8776\n",
      "Epoch 143/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0163 - accuracy: 1.0000 - val_loss: 1.3420 - val_accuracy: 0.8776\n",
      "Epoch 144/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0161 - accuracy: 1.0000 - val_loss: 1.3450 - val_accuracy: 0.8776\n",
      "Epoch 145/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0158 - accuracy: 1.0000 - val_loss: 1.3468 - val_accuracy: 0.8776\n",
      "Epoch 146/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0156 - accuracy: 1.0000 - val_loss: 1.3474 - val_accuracy: 0.8776\n",
      "Epoch 147/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0154 - accuracy: 1.0000 - val_loss: 1.3478 - val_accuracy: 0.8776\n",
      "Epoch 148/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0152 - accuracy: 1.0000 - val_loss: 1.3477 - val_accuracy: 0.8776\n",
      "Epoch 149/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0151 - accuracy: 1.0000 - val_loss: 1.3477 - val_accuracy: 0.8776\n",
      "Epoch 150/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0149 - accuracy: 1.0000 - val_loss: 1.3473 - val_accuracy: 0.8776\n",
      "Epoch 151/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0147 - accuracy: 1.0000 - val_loss: 1.3478 - val_accuracy: 0.8776\n",
      "訓練時間：3.41 秒\n",
      "630/630 [==============================] - 0s 683us/step - loss: 0.6045 - accuracy: 0.8904\n",
      "Evaluation on 90% unused data - Loss: 0.6045, Accuracy: 0.8904\n",
      "637/637 [==============================] - 1s 911us/step\n",
      "Test Data MDE report saved to: 5 data per RP transfer revised/DNN 4 mcAPs BEST_5data_2024_12_27.json\n",
      "\n",
      "Test Data Mean MDE: 0.1372 meters\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_44 (Dense)            (None, 64)                832       \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 49)                3185      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20593 (80.44 KB)\n",
      "Trainable params: 3185 (12.44 KB)\n",
      "Non-trainable params: 17408 (68.00 KB)\n",
      "_________________________________________________________________\n",
      "Number of rows with NaN values: 1299\n",
      "Number of rows with NaN values: 0\n",
      "Final reverse_label_mapping in DNN: {'1-1': 10, '1-2': 9, '1-3': 8, '1-4': 7, '1-5': 6, '1-6': 5, '1-7': 4, '1-8': 3, '1-9': 2, '1-10': 1, '1-11': 0, '2-1': 11, '2-11': 29, '3-1': 12, '3-11': 28, '4-1': 13, '4-11': 27, '5-1': 14, '5-11': 26, '6-1': 15, '6-2': 16, '6-3': 17, '6-4': 18, '6-5': 19, '6-6': 20, '6-7': 21, '6-8': 22, '6-9': 23, '6-10': 24, '6-11': 25, '7-1': 48, '7-11': 30, '8-1': 47, '8-11': 31, '9-1': 46, '9-11': 32, '10-1': 45, '10-11': 33, '11-1': 44, '11-2': 43, '11-3': 42, '11-4': 41, '11-5': 40, '11-6': 39, '11-7': 38, '11-8': 37, '11-9': 36, '11-10': 35, '11-11': 34}\n",
      "y_numeric unique values in DNN: [10  1  0  9  8  7  6  5  4  3  2 45 33 44 35 34 43 42 41 40 39 38 37 36\n",
      " 11 29 12 28 13 27 14 26 15 24 25 16 17 18 19 20 21 22 23 48 30 47 31 46\n",
      " 32]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "398\n",
      "label\n",
      "0     5\n",
      "25    5\n",
      "27    5\n",
      "28    5\n",
      "29    5\n",
      "30    5\n",
      "31    5\n",
      "32    5\n",
      "33    5\n",
      "34    5\n",
      "35    5\n",
      "36    5\n",
      "37    5\n",
      "38    5\n",
      "39    5\n",
      "40    5\n",
      "41    5\n",
      "42    5\n",
      "43    5\n",
      "44    5\n",
      "45    5\n",
      "46    5\n",
      "47    5\n",
      "26    5\n",
      "24    5\n",
      "1     5\n",
      "23    5\n",
      "2     5\n",
      "3     5\n",
      "4     5\n",
      "5     5\n",
      "6     5\n",
      "7     5\n",
      "8     5\n",
      "9     5\n",
      "10    5\n",
      "11    5\n",
      "12    5\n",
      "13    5\n",
      "14    5\n",
      "15    5\n",
      "16    5\n",
      "17    5\n",
      "18    5\n",
      "19    5\n",
      "20    5\n",
      "21    5\n",
      "22    5\n",
      "48    5\n",
      "Name: count, dtype: int64\n",
      "19257\n",
      "Training set: 196 samples, 49 unique labels\n",
      "Validation set: 49 samples, 49 unique labels\n",
      "Test set: 19257 samples, 49 unique labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcslab/anaconda3/envs/yang_cuda/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n",
      "/home/mcslab/anaconda3/envs/yang_cuda/lib/python3.10/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.3.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_1914507/3564637810.py:45: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  test_processed_data = pd.concat([test_processed_data, group], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Set</th>\n",
       "      <th>Validation Set</th>\n",
       "      <th>Test Set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Training Set  Validation Set  Test Set\n",
       "0              4               1       393\n",
       "1              4               1       393\n",
       "2              4               1       393\n",
       "3              4               1       393\n",
       "4              4               1       393\n",
       "5              4               1       393\n",
       "6              4               1       393\n",
       "7              4               1       393\n",
       "8              4               1       393\n",
       "9              4               1       393\n",
       "10             4               1       393\n",
       "11             4               1       393\n",
       "12             4               1       393\n",
       "13             4               1       393\n",
       "14             4               1       393\n",
       "15             4               1       393\n",
       "16             4               1       393\n",
       "17             4               1       393\n",
       "18             4               1       393\n",
       "19             4               1       393\n",
       "20             4               1       393\n",
       "21             4               1       393\n",
       "22             4               1       393\n",
       "23             4               1       393\n",
       "24             4               1       393\n",
       "25             4               1       393\n",
       "26             4               1       393\n",
       "27             4               1       393\n",
       "28             4               1       393\n",
       "29             4               1       393\n",
       "30             4               1       393\n",
       "31             4               1       393\n",
       "32             4               1       393\n",
       "33             4               1       393\n",
       "34             4               1       393\n",
       "35             4               1       393\n",
       "36             4               1       393\n",
       "37             4               1       393\n",
       "38             4               1       393\n",
       "39             4               1       393\n",
       "40             4               1       393\n",
       "41             4               1       393\n",
       "42             4               1       393\n",
       "43             4               1       393\n",
       "44             4               1       393\n",
       "45             4               1       393\n",
       "46             4               1       393\n",
       "47             4               1       393\n",
       "48             4               1       393"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "Epoch 1/10000\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 3.8165 - accuracy: 0.4745 - val_loss: 3.0662 - val_accuracy: 0.4898\n",
      "Epoch 2/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 3.3890 - accuracy: 0.5153 - val_loss: 2.7697 - val_accuracy: 0.5306\n",
      "Epoch 3/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 3.0480 - accuracy: 0.5357 - val_loss: 2.5197 - val_accuracy: 0.5714\n",
      "Epoch 4/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 2.7693 - accuracy: 0.5867 - val_loss: 2.3023 - val_accuracy: 0.5714\n",
      "Epoch 5/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 2.5118 - accuracy: 0.6122 - val_loss: 2.1279 - val_accuracy: 0.5714\n",
      "Epoch 6/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.3088 - accuracy: 0.6429 - val_loss: 1.9714 - val_accuracy: 0.5714\n",
      "Epoch 7/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.1252 - accuracy: 0.6582 - val_loss: 1.8315 - val_accuracy: 0.5714\n",
      "Epoch 8/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.9723 - accuracy: 0.6786 - val_loss: 1.7029 - val_accuracy: 0.5714\n",
      "Epoch 9/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.8334 - accuracy: 0.6990 - val_loss: 1.5902 - val_accuracy: 0.5918\n",
      "Epoch 10/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.7068 - accuracy: 0.6990 - val_loss: 1.4858 - val_accuracy: 0.5918\n",
      "Epoch 11/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5930 - accuracy: 0.7296 - val_loss: 1.4018 - val_accuracy: 0.6327\n",
      "Epoch 12/10000\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.4667 - accuracy: 0.7347 - val_loss: 1.3167 - val_accuracy: 0.6531\n",
      "Epoch 13/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.3791 - accuracy: 0.7500 - val_loss: 1.2673 - val_accuracy: 0.6735\n",
      "Epoch 14/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1.2672 - accuracy: 0.7653 - val_loss: 1.2235 - val_accuracy: 0.6735\n",
      "Epoch 15/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.1891 - accuracy: 0.7755 - val_loss: 1.1802 - val_accuracy: 0.6939\n",
      "Epoch 16/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1.1043 - accuracy: 0.7755 - val_loss: 1.1390 - val_accuracy: 0.6939\n",
      "Epoch 17/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1.0292 - accuracy: 0.7908 - val_loss: 1.0948 - val_accuracy: 0.7143\n",
      "Epoch 18/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.9615 - accuracy: 0.8061 - val_loss: 1.0378 - val_accuracy: 0.7347\n",
      "Epoch 19/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.8924 - accuracy: 0.8112 - val_loss: 0.9862 - val_accuracy: 0.7755\n",
      "Epoch 20/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.8312 - accuracy: 0.8265 - val_loss: 0.9256 - val_accuracy: 0.7755\n",
      "Epoch 21/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.7736 - accuracy: 0.8469 - val_loss: 0.8956 - val_accuracy: 0.7755\n",
      "Epoch 22/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.7218 - accuracy: 0.8571 - val_loss: 0.8805 - val_accuracy: 0.7755\n",
      "Epoch 23/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6730 - accuracy: 0.8520 - val_loss: 0.8654 - val_accuracy: 0.7755\n",
      "Epoch 24/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.6251 - accuracy: 0.8622 - val_loss: 0.8529 - val_accuracy: 0.7755\n",
      "Epoch 25/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.5826 - accuracy: 0.8673 - val_loss: 0.8456 - val_accuracy: 0.7959\n",
      "Epoch 26/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.5439 - accuracy: 0.8673 - val_loss: 0.8316 - val_accuracy: 0.7959\n",
      "Epoch 27/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.5027 - accuracy: 0.8673 - val_loss: 0.8140 - val_accuracy: 0.7959\n",
      "Epoch 28/10000\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.4680 - accuracy: 0.8724 - val_loss: 0.8067 - val_accuracy: 0.7959\n",
      "Epoch 29/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.4360 - accuracy: 0.8827 - val_loss: 0.7967 - val_accuracy: 0.7959\n",
      "Epoch 30/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.4056 - accuracy: 0.8929 - val_loss: 0.7738 - val_accuracy: 0.7959\n",
      "Epoch 31/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.3794 - accuracy: 0.8980 - val_loss: 0.7552 - val_accuracy: 0.7959\n",
      "Epoch 32/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3532 - accuracy: 0.9082 - val_loss: 0.7455 - val_accuracy: 0.7959\n",
      "Epoch 33/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3283 - accuracy: 0.9133 - val_loss: 0.7400 - val_accuracy: 0.7959\n",
      "Epoch 34/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.3083 - accuracy: 0.9286 - val_loss: 0.7421 - val_accuracy: 0.7959\n",
      "Epoch 35/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2874 - accuracy: 0.9337 - val_loss: 0.7360 - val_accuracy: 0.7959\n",
      "Epoch 36/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.2708 - accuracy: 0.9388 - val_loss: 0.7226 - val_accuracy: 0.7959\n",
      "Epoch 37/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2546 - accuracy: 0.9388 - val_loss: 0.7080 - val_accuracy: 0.7959\n",
      "Epoch 38/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.2383 - accuracy: 0.9388 - val_loss: 0.6910 - val_accuracy: 0.8163\n",
      "Epoch 39/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.2229 - accuracy: 0.9388 - val_loss: 0.6890 - val_accuracy: 0.8367\n",
      "Epoch 40/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.2113 - accuracy: 0.9439 - val_loss: 0.6813 - val_accuracy: 0.8367\n",
      "Epoch 41/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1987 - accuracy: 0.9490 - val_loss: 0.6791 - val_accuracy: 0.8163\n",
      "Epoch 42/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.1887 - accuracy: 0.9490 - val_loss: 0.6708 - val_accuracy: 0.8367\n",
      "Epoch 43/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.1781 - accuracy: 0.9490 - val_loss: 0.6634 - val_accuracy: 0.8367\n",
      "Epoch 44/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1685 - accuracy: 0.9592 - val_loss: 0.6572 - val_accuracy: 0.8571\n",
      "Epoch 45/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1586 - accuracy: 0.9592 - val_loss: 0.6533 - val_accuracy: 0.8571\n",
      "Epoch 46/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1484 - accuracy: 0.9694 - val_loss: 0.6546 - val_accuracy: 0.8571\n",
      "Epoch 47/10000\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1409 - accuracy: 0.9694 - val_loss: 0.6505 - val_accuracy: 0.8571\n",
      "Epoch 48/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1297 - accuracy: 0.9694 - val_loss: 0.6451 - val_accuracy: 0.8571\n",
      "Epoch 49/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1225 - accuracy: 0.9745 - val_loss: 0.6506 - val_accuracy: 0.8571\n",
      "Epoch 50/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1170 - accuracy: 0.9745 - val_loss: 0.6504 - val_accuracy: 0.8571\n",
      "Epoch 51/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1103 - accuracy: 0.9796 - val_loss: 0.6532 - val_accuracy: 0.8776\n",
      "Epoch 52/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1056 - accuracy: 0.9796 - val_loss: 0.6481 - val_accuracy: 0.8776\n",
      "Epoch 53/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0974 - accuracy: 0.9847 - val_loss: 0.6453 - val_accuracy: 0.8980\n",
      "Epoch 54/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0921 - accuracy: 0.9847 - val_loss: 0.6437 - val_accuracy: 0.8980\n",
      "Epoch 55/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0855 - accuracy: 0.9898 - val_loss: 0.6430 - val_accuracy: 0.8980\n",
      "Epoch 56/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0838 - accuracy: 0.9898 - val_loss: 0.6633 - val_accuracy: 0.8571\n",
      "Epoch 57/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0761 - accuracy: 0.9949 - val_loss: 0.6648 - val_accuracy: 0.8367\n",
      "Epoch 58/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0705 - accuracy: 0.9949 - val_loss: 0.6537 - val_accuracy: 0.8367\n",
      "Epoch 59/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0654 - accuracy: 0.9949 - val_loss: 0.6386 - val_accuracy: 0.8571\n",
      "Epoch 60/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0625 - accuracy: 0.9949 - val_loss: 0.6314 - val_accuracy: 0.8571\n",
      "Epoch 61/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0596 - accuracy: 0.9949 - val_loss: 0.6322 - val_accuracy: 0.8776\n",
      "Epoch 62/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0566 - accuracy: 1.0000 - val_loss: 0.6301 - val_accuracy: 0.8776\n",
      "Epoch 63/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0548 - accuracy: 1.0000 - val_loss: 0.6269 - val_accuracy: 0.8776\n",
      "Epoch 64/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0523 - accuracy: 1.0000 - val_loss: 0.6245 - val_accuracy: 0.8776\n",
      "Epoch 65/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0505 - accuracy: 1.0000 - val_loss: 0.6243 - val_accuracy: 0.8776\n",
      "Epoch 66/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0484 - accuracy: 1.0000 - val_loss: 0.6267 - val_accuracy: 0.8776\n",
      "Epoch 67/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0469 - accuracy: 1.0000 - val_loss: 0.6330 - val_accuracy: 0.8776\n",
      "Epoch 68/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0455 - accuracy: 1.0000 - val_loss: 0.6364 - val_accuracy: 0.8776\n",
      "Epoch 69/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0440 - accuracy: 1.0000 - val_loss: 0.6354 - val_accuracy: 0.8980\n",
      "Epoch 70/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0427 - accuracy: 1.0000 - val_loss: 0.6299 - val_accuracy: 0.8980\n",
      "Epoch 71/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0414 - accuracy: 1.0000 - val_loss: 0.6250 - val_accuracy: 0.8980\n",
      "Epoch 72/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0400 - accuracy: 1.0000 - val_loss: 0.6200 - val_accuracy: 0.8980\n",
      "Epoch 73/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0390 - accuracy: 1.0000 - val_loss: 0.6175 - val_accuracy: 0.8980\n",
      "Epoch 74/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0381 - accuracy: 1.0000 - val_loss: 0.6189 - val_accuracy: 0.8980\n",
      "Epoch 75/10000\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.0369 - accuracy: 1.0000 - val_loss: 0.6149 - val_accuracy: 0.8980\n",
      "Epoch 76/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0359 - accuracy: 1.0000 - val_loss: 0.6111 - val_accuracy: 0.8980\n",
      "Epoch 77/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0345 - accuracy: 1.0000 - val_loss: 0.6061 - val_accuracy: 0.8980\n",
      "Epoch 78/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0345 - accuracy: 1.0000 - val_loss: 0.6031 - val_accuracy: 0.8980\n",
      "Epoch 79/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0336 - accuracy: 1.0000 - val_loss: 0.6024 - val_accuracy: 0.8980\n",
      "Epoch 80/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0329 - accuracy: 1.0000 - val_loss: 0.6048 - val_accuracy: 0.8980\n",
      "Epoch 81/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0320 - accuracy: 1.0000 - val_loss: 0.6043 - val_accuracy: 0.8980\n",
      "Epoch 82/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0312 - accuracy: 1.0000 - val_loss: 0.6061 - val_accuracy: 0.8980\n",
      "Epoch 83/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0304 - accuracy: 1.0000 - val_loss: 0.6059 - val_accuracy: 0.8980\n",
      "Epoch 84/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0297 - accuracy: 1.0000 - val_loss: 0.6073 - val_accuracy: 0.8980\n",
      "Epoch 85/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0290 - accuracy: 1.0000 - val_loss: 0.6076 - val_accuracy: 0.8980\n",
      "Epoch 86/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0284 - accuracy: 1.0000 - val_loss: 0.6078 - val_accuracy: 0.8980\n",
      "Epoch 87/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0278 - accuracy: 1.0000 - val_loss: 0.6081 - val_accuracy: 0.8980\n",
      "Epoch 88/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0271 - accuracy: 1.0000 - val_loss: 0.6071 - val_accuracy: 0.8980\n",
      "Epoch 89/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 1.0000 - val_loss: 0.6073 - val_accuracy: 0.8980\n",
      "訓練時間：2.16 秒\n",
      "602/602 [==============================] - 0s 745us/step - loss: 0.6343 - accuracy: 0.8591\n",
      "Evaluation on 90% unused data - Loss: 0.6343, Accuracy: 0.8591\n",
      "610/610 [==============================] - 1s 921us/step\n",
      "Test Data MDE report saved to: 5 data per RP transfer revised/DNN 4 mcAPs BEST_5data_2025_01_03.json\n",
      "\n",
      "Test Data Mean MDE: 0.1988 meters\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_44 (Dense)            (None, 64)                832       \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 49)                3185      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20593 (80.44 KB)\n",
      "Trainable params: 3185 (12.44 KB)\n",
      "Non-trainable params: 17408 (68.00 KB)\n",
      "_________________________________________________________________\n",
      "Number of rows with NaN values: 974\n",
      "Number of rows with NaN values: 0\n",
      "Final reverse_label_mapping in DNN: {'1-1': 10, '1-2': 9, '1-3': 8, '1-4': 7, '1-5': 6, '1-6': 5, '1-7': 4, '1-8': 3, '1-9': 2, '1-10': 1, '1-11': 0, '2-1': 11, '2-11': 29, '3-1': 12, '3-11': 28, '4-1': 13, '4-11': 27, '5-1': 14, '5-11': 26, '6-1': 15, '6-2': 16, '6-3': 17, '6-4': 18, '6-5': 19, '6-6': 20, '6-7': 21, '6-8': 22, '6-9': 23, '6-10': 24, '6-11': 25, '7-1': 48, '7-11': 30, '8-1': 47, '8-11': 31, '9-1': 46, '9-11': 32, '10-1': 45, '10-11': 33, '11-1': 44, '11-2': 43, '11-3': 42, '11-4': 41, '11-5': 40, '11-6': 39, '11-7': 38, '11-8': 37, '11-9': 36, '11-10': 35, '11-11': 34}\n",
      "y_numeric unique values in DNN: [10  1  0  9  8  7  6  5  4  3  2 45 33 44 35 34 43 42 41 40 39 38 37 36\n",
      " 11 29 12 28 13 27 14 26 15 24 25 16 17 18 19 20 21 22 23 48 30 47 31 46\n",
      " 32]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "402\n",
      "label\n",
      "0     5\n",
      "25    5\n",
      "27    5\n",
      "28    5\n",
      "29    5\n",
      "30    5\n",
      "31    5\n",
      "32    5\n",
      "33    5\n",
      "34    5\n",
      "35    5\n",
      "36    5\n",
      "37    5\n",
      "38    5\n",
      "39    5\n",
      "40    5\n",
      "41    5\n",
      "42    5\n",
      "43    5\n",
      "44    5\n",
      "45    5\n",
      "46    5\n",
      "47    5\n",
      "26    5\n",
      "24    5\n",
      "1     5\n",
      "23    5\n",
      "2     5\n",
      "3     5\n",
      "4     5\n",
      "5     5\n",
      "6     5\n",
      "7     5\n",
      "8     5\n",
      "9     5\n",
      "10    5\n",
      "11    5\n",
      "12    5\n",
      "13    5\n",
      "14    5\n",
      "15    5\n",
      "16    5\n",
      "17    5\n",
      "18    5\n",
      "19    5\n",
      "20    5\n",
      "21    5\n",
      "22    5\n",
      "48    5\n",
      "Name: count, dtype: int64\n",
      "19453\n",
      "Training set: 196 samples, 49 unique labels\n",
      "Validation set: 49 samples, 49 unique labels\n",
      "Test set: 19453 samples, 49 unique labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcslab/anaconda3/envs/yang_cuda/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n",
      "/home/mcslab/anaconda3/envs/yang_cuda/lib/python3.10/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.3.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_1914507/3564637810.py:45: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  test_processed_data = pd.concat([test_processed_data, group], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Set</th>\n",
       "      <th>Validation Set</th>\n",
       "      <th>Test Set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Training Set  Validation Set  Test Set\n",
       "0              4               1       397\n",
       "1              4               1       397\n",
       "2              4               1       397\n",
       "3              4               1       397\n",
       "4              4               1       397\n",
       "5              4               1       397\n",
       "6              4               1       397\n",
       "7              4               1       397\n",
       "8              4               1       397\n",
       "9              4               1       397\n",
       "10             4               1       397\n",
       "11             4               1       397\n",
       "12             4               1       397\n",
       "13             4               1       397\n",
       "14             4               1       397\n",
       "15             4               1       397\n",
       "16             4               1       397\n",
       "17             4               1       397\n",
       "18             4               1       397\n",
       "19             4               1       397\n",
       "20             4               1       397\n",
       "21             4               1       397\n",
       "22             4               1       397\n",
       "23             4               1       397\n",
       "24             4               1       397\n",
       "25             4               1       397\n",
       "26             4               1       397\n",
       "27             4               1       397\n",
       "28             4               1       397\n",
       "29             4               1       397\n",
       "30             4               1       397\n",
       "31             4               1       397\n",
       "32             4               1       397\n",
       "33             4               1       397\n",
       "34             4               1       397\n",
       "35             4               1       397\n",
       "36             4               1       397\n",
       "37             4               1       397\n",
       "38             4               1       397\n",
       "39             4               1       397\n",
       "40             4               1       397\n",
       "41             4               1       397\n",
       "42             4               1       397\n",
       "43             4               1       397\n",
       "44             4               1       397\n",
       "45             4               1       397\n",
       "46             4               1       397\n",
       "47             4               1       397\n",
       "48             4               1       397"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "Epoch 1/10000\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 4.9764 - accuracy: 0.4235 - val_loss: 4.2068 - val_accuracy: 0.4694\n",
      "Epoch 2/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.4105 - accuracy: 0.4541 - val_loss: 3.7855 - val_accuracy: 0.4694\n",
      "Epoch 3/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 3.9595 - accuracy: 0.4796 - val_loss: 3.4199 - val_accuracy: 0.4898\n",
      "Epoch 4/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 3.6047 - accuracy: 0.5102 - val_loss: 3.1314 - val_accuracy: 0.5102\n",
      "Epoch 5/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 3.3021 - accuracy: 0.5612 - val_loss: 2.9088 - val_accuracy: 0.5306\n",
      "Epoch 6/10000\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 3.0612 - accuracy: 0.5765 - val_loss: 2.7638 - val_accuracy: 0.5510\n",
      "Epoch 7/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 2.8679 - accuracy: 0.5714 - val_loss: 2.6403 - val_accuracy: 0.5306\n",
      "Epoch 8/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 2.6932 - accuracy: 0.5765 - val_loss: 2.5318 - val_accuracy: 0.5306\n",
      "Epoch 9/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 2.5329 - accuracy: 0.6020 - val_loss: 2.4220 - val_accuracy: 0.5306\n",
      "Epoch 10/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.4033 - accuracy: 0.6224 - val_loss: 2.3208 - val_accuracy: 0.5510\n",
      "Epoch 11/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.2725 - accuracy: 0.6480 - val_loss: 2.2153 - val_accuracy: 0.5714\n",
      "Epoch 12/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.1595 - accuracy: 0.6735 - val_loss: 2.1214 - val_accuracy: 0.5918\n",
      "Epoch 13/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.0422 - accuracy: 0.6888 - val_loss: 2.0352 - val_accuracy: 0.6122\n",
      "Epoch 14/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.9493 - accuracy: 0.7041 - val_loss: 1.9722 - val_accuracy: 0.6327\n",
      "Epoch 15/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.8611 - accuracy: 0.7245 - val_loss: 1.9082 - val_accuracy: 0.6531\n",
      "Epoch 16/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.7818 - accuracy: 0.7347 - val_loss: 1.8516 - val_accuracy: 0.6531\n",
      "Epoch 17/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.7080 - accuracy: 0.7500 - val_loss: 1.8048 - val_accuracy: 0.6531\n",
      "Epoch 18/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1.6425 - accuracy: 0.7551 - val_loss: 1.7597 - val_accuracy: 0.6531\n",
      "Epoch 19/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5825 - accuracy: 0.7653 - val_loss: 1.7119 - val_accuracy: 0.6531\n",
      "Epoch 20/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1.5227 - accuracy: 0.7704 - val_loss: 1.6757 - val_accuracy: 0.6531\n",
      "Epoch 21/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1.4699 - accuracy: 0.7602 - val_loss: 1.6452 - val_accuracy: 0.6531\n",
      "Epoch 22/10000\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.4121 - accuracy: 0.7653 - val_loss: 1.6083 - val_accuracy: 0.6531\n",
      "Epoch 23/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.3636 - accuracy: 0.7653 - val_loss: 1.5811 - val_accuracy: 0.6531\n",
      "Epoch 24/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.3056 - accuracy: 0.7755 - val_loss: 1.5484 - val_accuracy: 0.6531\n",
      "Epoch 25/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.2571 - accuracy: 0.7857 - val_loss: 1.5090 - val_accuracy: 0.6531\n",
      "Epoch 26/10000\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.2077 - accuracy: 0.8010 - val_loss: 1.4807 - val_accuracy: 0.6531\n",
      "Epoch 27/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.1703 - accuracy: 0.8061 - val_loss: 1.4561 - val_accuracy: 0.6735\n",
      "Epoch 28/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.1182 - accuracy: 0.8265 - val_loss: 1.4312 - val_accuracy: 0.6735\n",
      "Epoch 29/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.0787 - accuracy: 0.8214 - val_loss: 1.4212 - val_accuracy: 0.6531\n",
      "Epoch 30/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.0337 - accuracy: 0.8214 - val_loss: 1.3962 - val_accuracy: 0.6939\n",
      "Epoch 31/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.9916 - accuracy: 0.8265 - val_loss: 1.3515 - val_accuracy: 0.6939\n",
      "Epoch 32/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.9528 - accuracy: 0.8367 - val_loss: 1.3134 - val_accuracy: 0.6939\n",
      "Epoch 33/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.9189 - accuracy: 0.8265 - val_loss: 1.2786 - val_accuracy: 0.7143\n",
      "Epoch 34/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.8880 - accuracy: 0.8163 - val_loss: 1.2415 - val_accuracy: 0.7347\n",
      "Epoch 35/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.8563 - accuracy: 0.8367 - val_loss: 1.2286 - val_accuracy: 0.7551\n",
      "Epoch 36/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.8282 - accuracy: 0.8469 - val_loss: 1.2236 - val_accuracy: 0.7347\n",
      "Epoch 37/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.7949 - accuracy: 0.8571 - val_loss: 1.2009 - val_accuracy: 0.7755\n",
      "Epoch 38/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.7685 - accuracy: 0.8622 - val_loss: 1.2009 - val_accuracy: 0.7755\n",
      "Epoch 39/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.7425 - accuracy: 0.8724 - val_loss: 1.1905 - val_accuracy: 0.7551\n",
      "Epoch 40/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.7077 - accuracy: 0.8776 - val_loss: 1.1696 - val_accuracy: 0.7551\n",
      "Epoch 41/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.6783 - accuracy: 0.8776 - val_loss: 1.1417 - val_accuracy: 0.7551\n",
      "Epoch 42/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6549 - accuracy: 0.8827 - val_loss: 1.1284 - val_accuracy: 0.7551\n",
      "Epoch 43/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6307 - accuracy: 0.8622 - val_loss: 1.1081 - val_accuracy: 0.7755\n",
      "Epoch 44/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6113 - accuracy: 0.8622 - val_loss: 1.0994 - val_accuracy: 0.7959\n",
      "Epoch 45/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5893 - accuracy: 0.8622 - val_loss: 1.0952 - val_accuracy: 0.7959\n",
      "Epoch 46/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.5692 - accuracy: 0.8724 - val_loss: 1.0815 - val_accuracy: 0.7959\n",
      "Epoch 47/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.5480 - accuracy: 0.8827 - val_loss: 1.0597 - val_accuracy: 0.8163\n",
      "Epoch 48/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5320 - accuracy: 0.8980 - val_loss: 1.0347 - val_accuracy: 0.8163\n",
      "Epoch 49/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5162 - accuracy: 0.9031 - val_loss: 1.0131 - val_accuracy: 0.8367\n",
      "Epoch 50/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5016 - accuracy: 0.9031 - val_loss: 0.9986 - val_accuracy: 0.8367\n",
      "Epoch 51/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4840 - accuracy: 0.9082 - val_loss: 0.9879 - val_accuracy: 0.8367\n",
      "Epoch 52/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4689 - accuracy: 0.9082 - val_loss: 0.9729 - val_accuracy: 0.8571\n",
      "Epoch 53/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4507 - accuracy: 0.9082 - val_loss: 0.9678 - val_accuracy: 0.8163\n",
      "Epoch 54/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4293 - accuracy: 0.9031 - val_loss: 0.9621 - val_accuracy: 0.8367\n",
      "Epoch 55/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4133 - accuracy: 0.9031 - val_loss: 0.9559 - val_accuracy: 0.8571\n",
      "Epoch 56/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3981 - accuracy: 0.9133 - val_loss: 0.9471 - val_accuracy: 0.8571\n",
      "Epoch 57/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3862 - accuracy: 0.9184 - val_loss: 0.9351 - val_accuracy: 0.8776\n",
      "Epoch 58/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3716 - accuracy: 0.9082 - val_loss: 0.9254 - val_accuracy: 0.8367\n",
      "Epoch 59/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3623 - accuracy: 0.9082 - val_loss: 0.9204 - val_accuracy: 0.8367\n",
      "Epoch 60/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3488 - accuracy: 0.9133 - val_loss: 0.9147 - val_accuracy: 0.8571\n",
      "Epoch 61/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3370 - accuracy: 0.9133 - val_loss: 0.9053 - val_accuracy: 0.8776\n",
      "Epoch 62/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.3257 - accuracy: 0.9235 - val_loss: 0.8975 - val_accuracy: 0.8776\n",
      "Epoch 63/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.3109 - accuracy: 0.9286 - val_loss: 0.8908 - val_accuracy: 0.8980\n",
      "Epoch 64/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.3009 - accuracy: 0.9286 - val_loss: 0.8830 - val_accuracy: 0.8776\n",
      "Epoch 65/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.2927 - accuracy: 0.9235 - val_loss: 0.8777 - val_accuracy: 0.8776\n",
      "Epoch 66/10000\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2789 - accuracy: 0.9337 - val_loss: 0.8731 - val_accuracy: 0.8776\n",
      "Epoch 67/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2695 - accuracy: 0.9388 - val_loss: 0.8689 - val_accuracy: 0.8776\n",
      "Epoch 68/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.2616 - accuracy: 0.9490 - val_loss: 0.8730 - val_accuracy: 0.8571\n",
      "Epoch 69/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.2473 - accuracy: 0.9439 - val_loss: 0.8746 - val_accuracy: 0.8571\n",
      "Epoch 70/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.2326 - accuracy: 0.9490 - val_loss: 0.8688 - val_accuracy: 0.8571\n",
      "Epoch 71/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.2238 - accuracy: 0.9439 - val_loss: 0.8750 - val_accuracy: 0.8367\n",
      "Epoch 72/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.2143 - accuracy: 0.9490 - val_loss: 0.8691 - val_accuracy: 0.8571\n",
      "Epoch 73/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.2035 - accuracy: 0.9490 - val_loss: 0.8614 - val_accuracy: 0.8571\n",
      "Epoch 74/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1976 - accuracy: 0.9490 - val_loss: 0.8528 - val_accuracy: 0.8571\n",
      "Epoch 75/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1919 - accuracy: 0.9490 - val_loss: 0.8487 - val_accuracy: 0.8571\n",
      "Epoch 76/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1834 - accuracy: 0.9541 - val_loss: 0.8447 - val_accuracy: 0.8571\n",
      "Epoch 77/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1763 - accuracy: 0.9541 - val_loss: 0.8355 - val_accuracy: 0.8571\n",
      "Epoch 78/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1687 - accuracy: 0.9541 - val_loss: 0.8295 - val_accuracy: 0.8571\n",
      "Epoch 79/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1589 - accuracy: 0.9592 - val_loss: 0.8249 - val_accuracy: 0.8980\n",
      "Epoch 80/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1535 - accuracy: 0.9490 - val_loss: 0.8184 - val_accuracy: 0.8980\n",
      "Epoch 81/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.1494 - accuracy: 0.9541 - val_loss: 0.8134 - val_accuracy: 0.8980\n",
      "Epoch 82/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1449 - accuracy: 0.9541 - val_loss: 0.8137 - val_accuracy: 0.8571\n",
      "Epoch 83/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1350 - accuracy: 0.9592 - val_loss: 0.8117 - val_accuracy: 0.8571\n",
      "Epoch 84/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1328 - accuracy: 0.9592 - val_loss: 0.8102 - val_accuracy: 0.8571\n",
      "Epoch 85/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1257 - accuracy: 0.9592 - val_loss: 0.8074 - val_accuracy: 0.8571\n",
      "Epoch 86/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1219 - accuracy: 0.9592 - val_loss: 0.8044 - val_accuracy: 0.8571\n",
      "Epoch 87/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1178 - accuracy: 0.9643 - val_loss: 0.8010 - val_accuracy: 0.8571\n",
      "Epoch 88/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.1126 - accuracy: 0.9643 - val_loss: 0.7983 - val_accuracy: 0.8571\n",
      "Epoch 89/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1111 - accuracy: 0.9643 - val_loss: 0.7923 - val_accuracy: 0.8571\n",
      "Epoch 90/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1070 - accuracy: 0.9694 - val_loss: 0.7894 - val_accuracy: 0.8571\n",
      "Epoch 91/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1037 - accuracy: 0.9694 - val_loss: 0.7892 - val_accuracy: 0.8571\n",
      "Epoch 92/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1007 - accuracy: 0.9694 - val_loss: 0.7886 - val_accuracy: 0.8571\n",
      "Epoch 93/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0977 - accuracy: 0.9694 - val_loss: 0.7850 - val_accuracy: 0.8571\n",
      "Epoch 94/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0940 - accuracy: 0.9796 - val_loss: 0.7786 - val_accuracy: 0.8571\n",
      "Epoch 95/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0913 - accuracy: 0.9796 - val_loss: 0.7737 - val_accuracy: 0.8571\n",
      "Epoch 96/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0887 - accuracy: 0.9796 - val_loss: 0.7703 - val_accuracy: 0.8571\n",
      "Epoch 97/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0857 - accuracy: 0.9796 - val_loss: 0.7679 - val_accuracy: 0.8571\n",
      "Epoch 98/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0836 - accuracy: 0.9796 - val_loss: 0.7672 - val_accuracy: 0.8776\n",
      "Epoch 99/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0822 - accuracy: 0.9796 - val_loss: 0.7655 - val_accuracy: 0.8776\n",
      "Epoch 100/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0796 - accuracy: 0.9796 - val_loss: 0.7610 - val_accuracy: 0.8776\n",
      "Epoch 101/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0774 - accuracy: 0.9796 - val_loss: 0.7588 - val_accuracy: 0.8776\n",
      "Epoch 102/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0750 - accuracy: 0.9796 - val_loss: 0.7536 - val_accuracy: 0.8776\n",
      "Epoch 103/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0732 - accuracy: 0.9847 - val_loss: 0.7495 - val_accuracy: 0.8571\n",
      "Epoch 104/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0740 - accuracy: 0.9796 - val_loss: 0.7505 - val_accuracy: 0.8776\n",
      "Epoch 105/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0715 - accuracy: 0.9796 - val_loss: 0.7522 - val_accuracy: 0.8776\n",
      "Epoch 106/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0697 - accuracy: 0.9796 - val_loss: 0.7505 - val_accuracy: 0.8776\n",
      "Epoch 107/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0675 - accuracy: 0.9796 - val_loss: 0.7505 - val_accuracy: 0.8776\n",
      "Epoch 108/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0641 - accuracy: 0.9796 - val_loss: 0.7496 - val_accuracy: 0.8776\n",
      "Epoch 109/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0618 - accuracy: 0.9847 - val_loss: 0.7498 - val_accuracy: 0.8776\n",
      "Epoch 110/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0600 - accuracy: 0.9847 - val_loss: 0.7491 - val_accuracy: 0.8776\n",
      "Epoch 111/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0566 - accuracy: 0.9796 - val_loss: 0.7468 - val_accuracy: 0.8776\n",
      "Epoch 112/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0601 - accuracy: 0.9796 - val_loss: 0.7473 - val_accuracy: 0.8776\n",
      "Epoch 113/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0585 - accuracy: 0.9847 - val_loss: 0.7446 - val_accuracy: 0.8776\n",
      "Epoch 114/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0563 - accuracy: 0.9847 - val_loss: 0.7417 - val_accuracy: 0.8776\n",
      "Epoch 115/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0540 - accuracy: 0.9898 - val_loss: 0.7382 - val_accuracy: 0.8776\n",
      "Epoch 116/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0519 - accuracy: 0.9847 - val_loss: 0.7374 - val_accuracy: 0.8776\n",
      "Epoch 117/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0504 - accuracy: 0.9847 - val_loss: 0.7349 - val_accuracy: 0.8776\n",
      "Epoch 118/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0492 - accuracy: 0.9898 - val_loss: 0.7338 - val_accuracy: 0.8776\n",
      "Epoch 119/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0482 - accuracy: 0.9898 - val_loss: 0.7326 - val_accuracy: 0.8776\n",
      "Epoch 120/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0474 - accuracy: 0.9898 - val_loss: 0.7306 - val_accuracy: 0.8776\n",
      "Epoch 121/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0471 - accuracy: 0.9898 - val_loss: 0.7298 - val_accuracy: 0.8776\n",
      "Epoch 122/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0456 - accuracy: 0.9898 - val_loss: 0.7279 - val_accuracy: 0.8776\n",
      "Epoch 123/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0476 - accuracy: 0.9847 - val_loss: 0.7259 - val_accuracy: 0.8776\n",
      "Epoch 124/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0450 - accuracy: 0.9898 - val_loss: 0.7254 - val_accuracy: 0.8776\n",
      "Epoch 125/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0443 - accuracy: 0.9898 - val_loss: 0.7237 - val_accuracy: 0.8776\n",
      "Epoch 126/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0431 - accuracy: 0.9898 - val_loss: 0.7229 - val_accuracy: 0.8776\n",
      "Epoch 127/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0417 - accuracy: 0.9898 - val_loss: 0.7224 - val_accuracy: 0.8776\n",
      "Epoch 128/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0399 - accuracy: 0.9898 - val_loss: 0.7210 - val_accuracy: 0.8776\n",
      "Epoch 129/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0387 - accuracy: 0.9898 - val_loss: 0.7193 - val_accuracy: 0.8776\n",
      "Epoch 130/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0382 - accuracy: 0.9949 - val_loss: 0.7188 - val_accuracy: 0.8776\n",
      "Epoch 131/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0376 - accuracy: 0.9949 - val_loss: 0.7171 - val_accuracy: 0.8776\n",
      "Epoch 132/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0372 - accuracy: 0.9949 - val_loss: 0.7153 - val_accuracy: 0.8776\n",
      "Epoch 133/10000\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.0363 - accuracy: 0.9949 - val_loss: 0.7131 - val_accuracy: 0.8776\n",
      "Epoch 134/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0357 - accuracy: 0.9949 - val_loss: 0.7095 - val_accuracy: 0.8776\n",
      "Epoch 135/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0349 - accuracy: 0.9949 - val_loss: 0.7077 - val_accuracy: 0.8776\n",
      "Epoch 136/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0344 - accuracy: 0.9949 - val_loss: 0.7059 - val_accuracy: 0.8776\n",
      "Epoch 137/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0336 - accuracy: 0.9949 - val_loss: 0.7080 - val_accuracy: 0.8571\n",
      "Epoch 138/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0339 - accuracy: 0.9949 - val_loss: 0.7082 - val_accuracy: 0.8571\n",
      "Epoch 139/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0332 - accuracy: 0.9949 - val_loss: 0.7063 - val_accuracy: 0.8571\n",
      "Epoch 140/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0325 - accuracy: 0.9949 - val_loss: 0.7042 - val_accuracy: 0.8571\n",
      "Epoch 141/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0318 - accuracy: 0.9949 - val_loss: 0.7034 - val_accuracy: 0.8571\n",
      "Epoch 142/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0311 - accuracy: 0.9949 - val_loss: 0.7016 - val_accuracy: 0.8571\n",
      "Epoch 143/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0304 - accuracy: 0.9949 - val_loss: 0.6996 - val_accuracy: 0.8776\n",
      "Epoch 144/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0290 - accuracy: 1.0000 - val_loss: 0.6946 - val_accuracy: 0.8776\n",
      "Epoch 145/10000\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0299 - accuracy: 1.0000 - val_loss: 0.6934 - val_accuracy: 0.8776\n",
      "Epoch 146/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0299 - accuracy: 1.0000 - val_loss: 0.6929 - val_accuracy: 0.8776\n",
      "Epoch 147/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0294 - accuracy: 1.0000 - val_loss: 0.6907 - val_accuracy: 0.8980\n",
      "Epoch 148/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0285 - accuracy: 1.0000 - val_loss: 0.6893 - val_accuracy: 0.8980\n",
      "Epoch 149/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0278 - accuracy: 1.0000 - val_loss: 0.6879 - val_accuracy: 0.8980\n",
      "Epoch 150/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0273 - accuracy: 1.0000 - val_loss: 0.6887 - val_accuracy: 0.8980\n",
      "Epoch 151/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 1.0000 - val_loss: 0.6899 - val_accuracy: 0.8980\n",
      "Epoch 152/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0257 - accuracy: 1.0000 - val_loss: 0.6896 - val_accuracy: 0.8980\n",
      "Epoch 153/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0256 - accuracy: 1.0000 - val_loss: 0.6913 - val_accuracy: 0.8980\n",
      "Epoch 154/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0251 - accuracy: 1.0000 - val_loss: 0.6927 - val_accuracy: 0.8980\n",
      "Epoch 155/10000\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0248 - accuracy: 1.0000 - val_loss: 0.6934 - val_accuracy: 0.8776\n",
      "Epoch 156/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0245 - accuracy: 1.0000 - val_loss: 0.6929 - val_accuracy: 0.8980\n",
      "Epoch 157/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0242 - accuracy: 1.0000 - val_loss: 0.6910 - val_accuracy: 0.8980\n",
      "Epoch 158/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0237 - accuracy: 1.0000 - val_loss: 0.6902 - val_accuracy: 0.8980\n",
      "Epoch 159/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0233 - accuracy: 1.0000 - val_loss: 0.6893 - val_accuracy: 0.8776\n",
      "訓練時間：3.78 秒\n",
      "608/608 [==============================] - 1s 850us/step - loss: 0.5949 - accuracy: 0.8987\n",
      "Evaluation on 90% unused data - Loss: 0.5949, Accuracy: 0.8987\n",
      "616/616 [==============================] - 0s 703us/step\n",
      "Test Data MDE report saved to: 5 data per RP transfer revised/DNN 4 mcAPs BEST_5data_2025_01_10.json\n",
      "\n",
      "Test Data Mean MDE: 0.1393 meters\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_44 (Dense)            (None, 64)                832       \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 49)                3185      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20593 (80.44 KB)\n",
      "Trainable params: 3185 (12.44 KB)\n",
      "Non-trainable params: 17408 (68.00 KB)\n",
      "_________________________________________________________________\n",
      "Number of rows with NaN values: 1078\n",
      "Number of rows with NaN values: 0\n",
      "Final reverse_label_mapping in DNN: {'1-1': 10, '1-2': 9, '1-3': 8, '1-4': 7, '1-5': 6, '1-6': 5, '1-7': 4, '1-8': 3, '1-9': 2, '1-10': 1, '1-11': 0, '2-1': 11, '2-11': 29, '3-1': 12, '3-11': 28, '4-1': 13, '4-11': 27, '5-1': 14, '5-11': 26, '6-1': 15, '6-2': 16, '6-3': 17, '6-4': 18, '6-5': 19, '6-6': 20, '6-7': 21, '6-8': 22, '6-9': 23, '6-10': 24, '6-11': 25, '7-1': 48, '7-11': 30, '8-1': 47, '8-11': 31, '9-1': 46, '9-11': 32, '10-1': 45, '10-11': 33, '11-1': 44, '11-2': 43, '11-3': 42, '11-4': 41, '11-5': 40, '11-6': 39, '11-7': 38, '11-8': 37, '11-9': 36, '11-10': 35, '11-11': 34}\n",
      "y_numeric unique values in DNN: [10  1  0  9  8  7  6  5  4  3  2 45 33 44 35 34 43 42 41 40 39 38 37 36\n",
      " 11 29 12 28 13 27 14 26 15 24 25 16 17 18 19 20 21 22 23 48 30 47 31 46\n",
      " 32]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "386\n",
      "label\n",
      "0     5\n",
      "25    5\n",
      "27    5\n",
      "28    5\n",
      "29    5\n",
      "30    5\n",
      "31    5\n",
      "32    5\n",
      "33    5\n",
      "34    5\n",
      "35    5\n",
      "36    5\n",
      "37    5\n",
      "38    5\n",
      "39    5\n",
      "40    5\n",
      "41    5\n",
      "42    5\n",
      "43    5\n",
      "44    5\n",
      "45    5\n",
      "46    5\n",
      "47    5\n",
      "26    5\n",
      "24    5\n",
      "1     5\n",
      "23    5\n",
      "2     5\n",
      "3     5\n",
      "4     5\n",
      "5     5\n",
      "6     5\n",
      "7     5\n",
      "8     5\n",
      "9     5\n",
      "10    5\n",
      "11    5\n",
      "12    5\n",
      "13    5\n",
      "14    5\n",
      "15    5\n",
      "16    5\n",
      "17    5\n",
      "18    5\n",
      "19    5\n",
      "20    5\n",
      "21    5\n",
      "22    5\n",
      "48    5\n",
      "Name: count, dtype: int64\n",
      "18669\n",
      "Training set: 196 samples, 49 unique labels\n",
      "Validation set: 49 samples, 49 unique labels\n",
      "Test set: 18669 samples, 49 unique labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcslab/anaconda3/envs/yang_cuda/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n",
      "/home/mcslab/anaconda3/envs/yang_cuda/lib/python3.10/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.3.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_1914507/3564637810.py:45: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  test_processed_data = pd.concat([test_processed_data, group], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Set</th>\n",
       "      <th>Validation Set</th>\n",
       "      <th>Test Set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Training Set  Validation Set  Test Set\n",
       "0              4               1       381\n",
       "1              4               1       381\n",
       "2              4               1       381\n",
       "3              4               1       381\n",
       "4              4               1       381\n",
       "5              4               1       381\n",
       "6              4               1       381\n",
       "7              4               1       381\n",
       "8              4               1       381\n",
       "9              4               1       381\n",
       "10             4               1       381\n",
       "11             4               1       381\n",
       "12             4               1       381\n",
       "13             4               1       381\n",
       "14             4               1       381\n",
       "15             4               1       381\n",
       "16             4               1       381\n",
       "17             4               1       381\n",
       "18             4               1       381\n",
       "19             4               1       381\n",
       "20             4               1       381\n",
       "21             4               1       381\n",
       "22             4               1       381\n",
       "23             4               1       381\n",
       "24             4               1       381\n",
       "25             4               1       381\n",
       "26             4               1       381\n",
       "27             4               1       381\n",
       "28             4               1       381\n",
       "29             4               1       381\n",
       "30             4               1       381\n",
       "31             4               1       381\n",
       "32             4               1       381\n",
       "33             4               1       381\n",
       "34             4               1       381\n",
       "35             4               1       381\n",
       "36             4               1       381\n",
       "37             4               1       381\n",
       "38             4               1       381\n",
       "39             4               1       381\n",
       "40             4               1       381\n",
       "41             4               1       381\n",
       "42             4               1       381\n",
       "43             4               1       381\n",
       "44             4               1       381\n",
       "45             4               1       381\n",
       "46             4               1       381\n",
       "47             4               1       381\n",
       "48             4               1       381"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "Epoch 1/10000\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 11.4359 - accuracy: 0.2500 - val_loss: 10.9042 - val_accuracy: 0.1224\n",
      "Epoch 2/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 10.6212 - accuracy: 0.2500 - val_loss: 10.3058 - val_accuracy: 0.1224\n",
      "Epoch 3/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 9.9463 - accuracy: 0.2347 - val_loss: 9.7760 - val_accuracy: 0.1429\n",
      "Epoch 4/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 9.3539 - accuracy: 0.2296 - val_loss: 9.3114 - val_accuracy: 0.1837\n",
      "Epoch 5/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 8.8404 - accuracy: 0.2500 - val_loss: 8.9065 - val_accuracy: 0.1837\n",
      "Epoch 6/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.3667 - accuracy: 0.2449 - val_loss: 8.5455 - val_accuracy: 0.1837\n",
      "Epoch 7/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 7.9688 - accuracy: 0.2500 - val_loss: 8.1865 - val_accuracy: 0.2041\n",
      "Epoch 8/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 7.5583 - accuracy: 0.2806 - val_loss: 7.8537 - val_accuracy: 0.2653\n",
      "Epoch 9/10000\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 7.2176 - accuracy: 0.3061 - val_loss: 7.5368 - val_accuracy: 0.2653\n",
      "Epoch 10/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 6.8734 - accuracy: 0.3418 - val_loss: 7.2459 - val_accuracy: 0.2653\n",
      "Epoch 11/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 6.5458 - accuracy: 0.3673 - val_loss: 6.9748 - val_accuracy: 0.2653\n",
      "Epoch 12/10000\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 6.2641 - accuracy: 0.3776 - val_loss: 6.7164 - val_accuracy: 0.3061\n",
      "Epoch 13/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 5.9550 - accuracy: 0.3929 - val_loss: 6.4791 - val_accuracy: 0.3061\n",
      "Epoch 14/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 5.6793 - accuracy: 0.3929 - val_loss: 6.2427 - val_accuracy: 0.2857\n",
      "Epoch 15/10000\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 5.4211 - accuracy: 0.3878 - val_loss: 6.0216 - val_accuracy: 0.3061\n",
      "Epoch 16/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 5.1902 - accuracy: 0.3929 - val_loss: 5.7997 - val_accuracy: 0.3061\n",
      "Epoch 17/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 4.9513 - accuracy: 0.4082 - val_loss: 5.5905 - val_accuracy: 0.3265\n",
      "Epoch 18/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 4.7383 - accuracy: 0.4337 - val_loss: 5.3927 - val_accuracy: 0.3469\n",
      "Epoch 19/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 4.5344 - accuracy: 0.4541 - val_loss: 5.1970 - val_accuracy: 0.3469\n",
      "Epoch 20/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 4.3274 - accuracy: 0.4694 - val_loss: 5.0417 - val_accuracy: 0.3469\n",
      "Epoch 21/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.1675 - accuracy: 0.4898 - val_loss: 4.8983 - val_accuracy: 0.3469\n",
      "Epoch 22/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.0056 - accuracy: 0.5051 - val_loss: 4.7676 - val_accuracy: 0.3673\n",
      "Epoch 23/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.8716 - accuracy: 0.5357 - val_loss: 4.6350 - val_accuracy: 0.3673\n",
      "Epoch 24/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.7313 - accuracy: 0.5459 - val_loss: 4.4845 - val_accuracy: 0.3878\n",
      "Epoch 25/10000\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 3.5994 - accuracy: 0.5459 - val_loss: 4.3219 - val_accuracy: 0.4082\n",
      "Epoch 26/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 3.4722 - accuracy: 0.5510 - val_loss: 4.1708 - val_accuracy: 0.4082\n",
      "Epoch 27/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 3.3525 - accuracy: 0.5459 - val_loss: 4.0268 - val_accuracy: 0.4082\n",
      "Epoch 28/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 3.2245 - accuracy: 0.5561 - val_loss: 3.8972 - val_accuracy: 0.4286\n",
      "Epoch 29/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.0947 - accuracy: 0.5561 - val_loss: 3.7701 - val_accuracy: 0.4286\n",
      "Epoch 30/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.9769 - accuracy: 0.5714 - val_loss: 3.6595 - val_accuracy: 0.4490\n",
      "Epoch 31/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.8673 - accuracy: 0.5867 - val_loss: 3.5617 - val_accuracy: 0.4490\n",
      "Epoch 32/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.7712 - accuracy: 0.6173 - val_loss: 3.4662 - val_accuracy: 0.4490\n",
      "Epoch 33/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.6726 - accuracy: 0.6224 - val_loss: 3.3659 - val_accuracy: 0.4490\n",
      "Epoch 34/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 2.5771 - accuracy: 0.6173 - val_loss: 3.2673 - val_accuracy: 0.4490\n",
      "Epoch 35/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 2.4751 - accuracy: 0.6327 - val_loss: 3.1764 - val_accuracy: 0.4490\n",
      "Epoch 36/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 2.3755 - accuracy: 0.6480 - val_loss: 3.0788 - val_accuracy: 0.4898\n",
      "Epoch 37/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 2.2865 - accuracy: 0.6429 - val_loss: 2.9720 - val_accuracy: 0.5306\n",
      "Epoch 38/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 2.1938 - accuracy: 0.6429 - val_loss: 2.8748 - val_accuracy: 0.5306\n",
      "Epoch 39/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.1084 - accuracy: 0.6684 - val_loss: 2.7744 - val_accuracy: 0.5510\n",
      "Epoch 40/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.0303 - accuracy: 0.6990 - val_loss: 2.6750 - val_accuracy: 0.5510\n",
      "Epoch 41/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.9430 - accuracy: 0.7041 - val_loss: 2.5918 - val_accuracy: 0.5714\n",
      "Epoch 42/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1.8608 - accuracy: 0.7041 - val_loss: 2.5195 - val_accuracy: 0.5918\n",
      "Epoch 43/10000\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.7828 - accuracy: 0.6990 - val_loss: 2.4457 - val_accuracy: 0.5918\n",
      "Epoch 44/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.7011 - accuracy: 0.7041 - val_loss: 2.3776 - val_accuracy: 0.5918\n",
      "Epoch 45/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6329 - accuracy: 0.7092 - val_loss: 2.3189 - val_accuracy: 0.6122\n",
      "Epoch 46/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5506 - accuracy: 0.7143 - val_loss: 2.2631 - val_accuracy: 0.6327\n",
      "Epoch 47/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1.4851 - accuracy: 0.7194 - val_loss: 2.1996 - val_accuracy: 0.6327\n",
      "Epoch 48/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.4223 - accuracy: 0.7398 - val_loss: 2.1412 - val_accuracy: 0.6531\n",
      "Epoch 49/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.3562 - accuracy: 0.7398 - val_loss: 2.0866 - val_accuracy: 0.6531\n",
      "Epoch 50/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1.2997 - accuracy: 0.7500 - val_loss: 2.0398 - val_accuracy: 0.6531\n",
      "Epoch 51/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1.2539 - accuracy: 0.7653 - val_loss: 2.0030 - val_accuracy: 0.6531\n",
      "Epoch 52/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.1922 - accuracy: 0.7551 - val_loss: 1.9464 - val_accuracy: 0.6531\n",
      "Epoch 53/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.1426 - accuracy: 0.7653 - val_loss: 1.8843 - val_accuracy: 0.6531\n",
      "Epoch 54/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.0982 - accuracy: 0.7806 - val_loss: 1.8355 - val_accuracy: 0.6531\n",
      "Epoch 55/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.0574 - accuracy: 0.7857 - val_loss: 1.7886 - val_accuracy: 0.6531\n",
      "Epoch 56/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1.0016 - accuracy: 0.8010 - val_loss: 1.7268 - val_accuracy: 0.6531\n",
      "Epoch 57/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.9506 - accuracy: 0.8265 - val_loss: 1.6804 - val_accuracy: 0.6531\n",
      "Epoch 58/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.9116 - accuracy: 0.8265 - val_loss: 1.6488 - val_accuracy: 0.6531\n",
      "Epoch 59/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.8747 - accuracy: 0.8265 - val_loss: 1.6167 - val_accuracy: 0.6531\n",
      "Epoch 60/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.8416 - accuracy: 0.8214 - val_loss: 1.5817 - val_accuracy: 0.6531\n",
      "Epoch 61/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.8051 - accuracy: 0.8316 - val_loss: 1.5530 - val_accuracy: 0.6531\n",
      "Epoch 62/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.7715 - accuracy: 0.8367 - val_loss: 1.5211 - val_accuracy: 0.6531\n",
      "Epoch 63/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.7433 - accuracy: 0.8316 - val_loss: 1.4798 - val_accuracy: 0.6735\n",
      "Epoch 64/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.7173 - accuracy: 0.8418 - val_loss: 1.4484 - val_accuracy: 0.6735\n",
      "Epoch 65/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6908 - accuracy: 0.8520 - val_loss: 1.4371 - val_accuracy: 0.6735\n",
      "Epoch 66/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6550 - accuracy: 0.8571 - val_loss: 1.4154 - val_accuracy: 0.6735\n",
      "Epoch 67/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6283 - accuracy: 0.8571 - val_loss: 1.4030 - val_accuracy: 0.6939\n",
      "Epoch 68/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6048 - accuracy: 0.8673 - val_loss: 1.3905 - val_accuracy: 0.6939\n",
      "Epoch 69/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5839 - accuracy: 0.8673 - val_loss: 1.3766 - val_accuracy: 0.6735\n",
      "Epoch 70/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5611 - accuracy: 0.8724 - val_loss: 1.3603 - val_accuracy: 0.6735\n",
      "Epoch 71/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5391 - accuracy: 0.8878 - val_loss: 1.3429 - val_accuracy: 0.6735\n",
      "Epoch 72/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5127 - accuracy: 0.8827 - val_loss: 1.3220 - val_accuracy: 0.6939\n",
      "Epoch 73/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4941 - accuracy: 0.8827 - val_loss: 1.3083 - val_accuracy: 0.6939\n",
      "Epoch 74/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4773 - accuracy: 0.8878 - val_loss: 1.2998 - val_accuracy: 0.7143\n",
      "Epoch 75/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4598 - accuracy: 0.8929 - val_loss: 1.2900 - val_accuracy: 0.7143\n",
      "Epoch 76/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4419 - accuracy: 0.8929 - val_loss: 1.2835 - val_accuracy: 0.7143\n",
      "Epoch 77/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4209 - accuracy: 0.9031 - val_loss: 1.2509 - val_accuracy: 0.7347\n",
      "Epoch 78/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4060 - accuracy: 0.9133 - val_loss: 1.2256 - val_accuracy: 0.7551\n",
      "Epoch 79/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3909 - accuracy: 0.9235 - val_loss: 1.2037 - val_accuracy: 0.7551\n",
      "Epoch 80/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3763 - accuracy: 0.9286 - val_loss: 1.1883 - val_accuracy: 0.7551\n",
      "Epoch 81/10000\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3648 - accuracy: 0.9286 - val_loss: 1.1721 - val_accuracy: 0.7551\n",
      "Epoch 82/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.3530 - accuracy: 0.9286 - val_loss: 1.1551 - val_accuracy: 0.7551\n",
      "Epoch 83/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.3400 - accuracy: 0.9337 - val_loss: 1.1433 - val_accuracy: 0.7551\n",
      "Epoch 84/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3285 - accuracy: 0.9388 - val_loss: 1.1322 - val_accuracy: 0.7551\n",
      "Epoch 85/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3187 - accuracy: 0.9388 - val_loss: 1.1171 - val_accuracy: 0.7755\n",
      "Epoch 86/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3088 - accuracy: 0.9388 - val_loss: 1.0823 - val_accuracy: 0.7959\n",
      "Epoch 87/10000\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3010 - accuracy: 0.9388 - val_loss: 1.0661 - val_accuracy: 0.7959\n",
      "Epoch 88/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.2913 - accuracy: 0.9337 - val_loss: 1.0598 - val_accuracy: 0.7959\n",
      "Epoch 89/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.2800 - accuracy: 0.9286 - val_loss: 1.0501 - val_accuracy: 0.8163\n",
      "Epoch 90/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.2687 - accuracy: 0.9439 - val_loss: 1.0473 - val_accuracy: 0.8163\n",
      "Epoch 91/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.2566 - accuracy: 0.9388 - val_loss: 1.0414 - val_accuracy: 0.8163\n",
      "Epoch 92/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.2479 - accuracy: 0.9388 - val_loss: 1.0294 - val_accuracy: 0.8163\n",
      "Epoch 93/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2385 - accuracy: 0.9388 - val_loss: 1.0234 - val_accuracy: 0.8163\n",
      "Epoch 94/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2298 - accuracy: 0.9337 - val_loss: 1.0204 - val_accuracy: 0.8163\n",
      "Epoch 95/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2239 - accuracy: 0.9388 - val_loss: 1.0104 - val_accuracy: 0.8163\n",
      "Epoch 96/10000\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.2166 - accuracy: 0.9439 - val_loss: 1.0043 - val_accuracy: 0.8163\n",
      "Epoch 97/10000\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.2104 - accuracy: 0.9439 - val_loss: 0.9972 - val_accuracy: 0.8163\n",
      "Epoch 98/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.2049 - accuracy: 0.9439 - val_loss: 0.9861 - val_accuracy: 0.8163\n",
      "Epoch 99/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1993 - accuracy: 0.9490 - val_loss: 0.9799 - val_accuracy: 0.8163\n",
      "Epoch 100/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.1941 - accuracy: 0.9490 - val_loss: 0.9731 - val_accuracy: 0.8163\n",
      "Epoch 101/10000\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1887 - accuracy: 0.9541 - val_loss: 0.9676 - val_accuracy: 0.8163\n",
      "Epoch 102/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1840 - accuracy: 0.9541 - val_loss: 0.9607 - val_accuracy: 0.8367\n",
      "Epoch 103/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1801 - accuracy: 0.9541 - val_loss: 0.9564 - val_accuracy: 0.8367\n",
      "Epoch 104/10000\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1755 - accuracy: 0.9592 - val_loss: 0.9484 - val_accuracy: 0.8367\n",
      "Epoch 105/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1715 - accuracy: 0.9592 - val_loss: 0.9438 - val_accuracy: 0.8367\n",
      "Epoch 106/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1670 - accuracy: 0.9592 - val_loss: 0.9419 - val_accuracy: 0.8367\n",
      "Epoch 107/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1636 - accuracy: 0.9541 - val_loss: 0.9353 - val_accuracy: 0.8367\n",
      "Epoch 108/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.1606 - accuracy: 0.9592 - val_loss: 0.9288 - val_accuracy: 0.8367\n",
      "Epoch 109/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1570 - accuracy: 0.9592 - val_loss: 0.9222 - val_accuracy: 0.8367\n",
      "Epoch 110/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1532 - accuracy: 0.9592 - val_loss: 0.9156 - val_accuracy: 0.8367\n",
      "Epoch 111/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.1488 - accuracy: 0.9643 - val_loss: 0.9115 - val_accuracy: 0.8367\n",
      "Epoch 112/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.1452 - accuracy: 0.9694 - val_loss: 0.9046 - val_accuracy: 0.8367\n",
      "Epoch 113/10000\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1436 - accuracy: 0.9694 - val_loss: 0.8916 - val_accuracy: 0.8367\n",
      "Epoch 114/10000\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1393 - accuracy: 0.9694 - val_loss: 0.8856 - val_accuracy: 0.8367\n",
      "Epoch 115/10000\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1363 - accuracy: 0.9694 - val_loss: 0.8823 - val_accuracy: 0.8367\n",
      "Epoch 116/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1332 - accuracy: 0.9745 - val_loss: 0.8776 - val_accuracy: 0.8367\n",
      "Epoch 117/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.1305 - accuracy: 0.9745 - val_loss: 0.8751 - val_accuracy: 0.8571\n",
      "Epoch 118/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.1278 - accuracy: 0.9745 - val_loss: 0.8702 - val_accuracy: 0.8571\n",
      "Epoch 119/10000\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1244 - accuracy: 0.9745 - val_loss: 0.8652 - val_accuracy: 0.8571\n",
      "Epoch 120/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1218 - accuracy: 0.9796 - val_loss: 0.8607 - val_accuracy: 0.8571\n",
      "Epoch 121/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.1196 - accuracy: 0.9796 - val_loss: 0.8555 - val_accuracy: 0.8571\n",
      "Epoch 122/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1162 - accuracy: 0.9796 - val_loss: 0.8529 - val_accuracy: 0.8571\n",
      "Epoch 123/10000\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1118 - accuracy: 0.9796 - val_loss: 0.8492 - val_accuracy: 0.8571\n",
      "Epoch 124/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1083 - accuracy: 0.9796 - val_loss: 0.8439 - val_accuracy: 0.8571\n",
      "Epoch 125/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.1054 - accuracy: 0.9796 - val_loss: 0.8399 - val_accuracy: 0.8571\n",
      "Epoch 126/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1029 - accuracy: 0.9796 - val_loss: 0.8366 - val_accuracy: 0.8571\n",
      "Epoch 127/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1011 - accuracy: 0.9847 - val_loss: 0.8314 - val_accuracy: 0.8571\n",
      "Epoch 128/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0978 - accuracy: 0.9847 - val_loss: 0.8271 - val_accuracy: 0.8571\n",
      "Epoch 129/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0964 - accuracy: 0.9847 - val_loss: 0.8235 - val_accuracy: 0.8571\n",
      "Epoch 130/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0942 - accuracy: 0.9847 - val_loss: 0.8202 - val_accuracy: 0.8571\n",
      "Epoch 131/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0922 - accuracy: 0.9847 - val_loss: 0.8172 - val_accuracy: 0.8571\n",
      "Epoch 132/10000\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.0900 - accuracy: 0.9847 - val_loss: 0.8183 - val_accuracy: 0.8571\n",
      "Epoch 133/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0879 - accuracy: 0.9847 - val_loss: 0.8190 - val_accuracy: 0.8571\n",
      "Epoch 134/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0869 - accuracy: 0.9847 - val_loss: 0.8159 - val_accuracy: 0.8776\n",
      "Epoch 135/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0817 - accuracy: 0.9898 - val_loss: 0.8133 - val_accuracy: 0.8776\n",
      "Epoch 136/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0859 - accuracy: 0.9898 - val_loss: 0.8082 - val_accuracy: 0.8776\n",
      "Epoch 137/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0862 - accuracy: 0.9898 - val_loss: 0.8026 - val_accuracy: 0.8776\n",
      "Epoch 138/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0836 - accuracy: 0.9898 - val_loss: 0.7982 - val_accuracy: 0.8776\n",
      "Epoch 139/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0798 - accuracy: 0.9898 - val_loss: 0.7931 - val_accuracy: 0.8776\n",
      "Epoch 140/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0769 - accuracy: 0.9898 - val_loss: 0.7881 - val_accuracy: 0.8776\n",
      "Epoch 141/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0756 - accuracy: 0.9898 - val_loss: 0.7851 - val_accuracy: 0.8776\n",
      "Epoch 142/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0735 - accuracy: 0.9898 - val_loss: 0.7833 - val_accuracy: 0.8776\n",
      "Epoch 143/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0737 - accuracy: 0.9898 - val_loss: 0.7813 - val_accuracy: 0.8776\n",
      "Epoch 144/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0721 - accuracy: 0.9898 - val_loss: 0.7769 - val_accuracy: 0.8776\n",
      "Epoch 145/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0719 - accuracy: 0.9898 - val_loss: 0.7758 - val_accuracy: 0.8776\n",
      "Epoch 146/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0710 - accuracy: 0.9898 - val_loss: 0.7750 - val_accuracy: 0.8776\n",
      "Epoch 147/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0688 - accuracy: 0.9898 - val_loss: 0.7712 - val_accuracy: 0.8776\n",
      "Epoch 148/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0658 - accuracy: 0.9898 - val_loss: 0.7708 - val_accuracy: 0.8776\n",
      "Epoch 149/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0653 - accuracy: 0.9898 - val_loss: 0.7680 - val_accuracy: 0.8776\n",
      "Epoch 150/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0622 - accuracy: 0.9898 - val_loss: 0.7667 - val_accuracy: 0.8776\n",
      "Epoch 151/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0595 - accuracy: 0.9898 - val_loss: 0.7661 - val_accuracy: 0.8776\n",
      "Epoch 152/10000\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.0596 - accuracy: 0.9898 - val_loss: 0.7635 - val_accuracy: 0.8776\n",
      "Epoch 153/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0594 - accuracy: 0.9898 - val_loss: 0.7609 - val_accuracy: 0.8776\n",
      "Epoch 154/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0587 - accuracy: 0.9949 - val_loss: 0.7577 - val_accuracy: 0.8776\n",
      "Epoch 155/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0573 - accuracy: 0.9949 - val_loss: 0.7537 - val_accuracy: 0.8776\n",
      "Epoch 156/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0552 - accuracy: 0.9949 - val_loss: 0.7507 - val_accuracy: 0.8776\n",
      "Epoch 157/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0529 - accuracy: 0.9949 - val_loss: 0.7464 - val_accuracy: 0.8776\n",
      "Epoch 158/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0520 - accuracy: 0.9949 - val_loss: 0.7439 - val_accuracy: 0.8776\n",
      "Epoch 159/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0495 - accuracy: 0.9949 - val_loss: 0.7418 - val_accuracy: 0.8776\n",
      "Epoch 160/10000\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0486 - accuracy: 0.9898 - val_loss: 0.7402 - val_accuracy: 0.8776\n",
      "Epoch 161/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0481 - accuracy: 0.9847 - val_loss: 0.7394 - val_accuracy: 0.8776\n",
      "Epoch 162/10000\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0470 - accuracy: 0.9847 - val_loss: 0.7372 - val_accuracy: 0.8776\n",
      "Epoch 163/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0454 - accuracy: 0.9847 - val_loss: 0.7349 - val_accuracy: 0.8776\n",
      "Epoch 164/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0440 - accuracy: 0.9898 - val_loss: 0.7330 - val_accuracy: 0.8776\n",
      "Epoch 165/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0434 - accuracy: 0.9898 - val_loss: 0.7324 - val_accuracy: 0.8776\n",
      "Epoch 166/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0414 - accuracy: 0.9898 - val_loss: 0.7258 - val_accuracy: 0.8776\n",
      "Epoch 167/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0413 - accuracy: 0.9898 - val_loss: 0.7217 - val_accuracy: 0.8776\n",
      "Epoch 168/10000\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0410 - accuracy: 0.9898 - val_loss: 0.7196 - val_accuracy: 0.8776\n",
      "Epoch 169/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0399 - accuracy: 0.9898 - val_loss: 0.7187 - val_accuracy: 0.8571\n",
      "Epoch 170/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0386 - accuracy: 0.9898 - val_loss: 0.7182 - val_accuracy: 0.8776\n",
      "Epoch 171/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0376 - accuracy: 0.9898 - val_loss: 0.7184 - val_accuracy: 0.8776\n",
      "Epoch 172/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0367 - accuracy: 0.9898 - val_loss: 0.7181 - val_accuracy: 0.8776\n",
      "Epoch 173/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0355 - accuracy: 0.9949 - val_loss: 0.7181 - val_accuracy: 0.8776\n",
      "Epoch 174/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0353 - accuracy: 0.9949 - val_loss: 0.7202 - val_accuracy: 0.8571\n",
      "Epoch 175/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0344 - accuracy: 0.9949 - val_loss: 0.7230 - val_accuracy: 0.8571\n",
      "Epoch 176/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0387 - accuracy: 0.9949 - val_loss: 0.7235 - val_accuracy: 0.8571\n",
      "Epoch 177/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0370 - accuracy: 0.9949 - val_loss: 0.7187 - val_accuracy: 0.8571\n",
      "Epoch 178/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0341 - accuracy: 0.9949 - val_loss: 0.7132 - val_accuracy: 0.8571\n",
      "Epoch 179/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0324 - accuracy: 0.9949 - val_loss: 0.7077 - val_accuracy: 0.8571\n",
      "Epoch 180/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0296 - accuracy: 0.9949 - val_loss: 0.7055 - val_accuracy: 0.8571\n",
      "Epoch 181/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0290 - accuracy: 1.0000 - val_loss: 0.7049 - val_accuracy: 0.8571\n",
      "Epoch 182/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0284 - accuracy: 1.0000 - val_loss: 0.7030 - val_accuracy: 0.8571\n",
      "Epoch 183/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0288 - accuracy: 1.0000 - val_loss: 0.7006 - val_accuracy: 0.8571\n",
      "Epoch 184/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0281 - accuracy: 1.0000 - val_loss: 0.6972 - val_accuracy: 0.8571\n",
      "Epoch 185/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0273 - accuracy: 1.0000 - val_loss: 0.6943 - val_accuracy: 0.8571\n",
      "Epoch 186/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 1.0000 - val_loss: 0.6920 - val_accuracy: 0.8571\n",
      "Epoch 187/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0260 - accuracy: 1.0000 - val_loss: 0.6899 - val_accuracy: 0.8571\n",
      "Epoch 188/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0257 - accuracy: 1.0000 - val_loss: 0.6869 - val_accuracy: 0.8571\n",
      "Epoch 189/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0251 - accuracy: 1.0000 - val_loss: 0.6838 - val_accuracy: 0.8571\n",
      "Epoch 190/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0248 - accuracy: 1.0000 - val_loss: 0.6819 - val_accuracy: 0.8571\n",
      "Epoch 191/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0244 - accuracy: 1.0000 - val_loss: 0.6809 - val_accuracy: 0.8571\n",
      "Epoch 192/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0240 - accuracy: 1.0000 - val_loss: 0.6791 - val_accuracy: 0.8571\n",
      "Epoch 193/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0238 - accuracy: 1.0000 - val_loss: 0.6776 - val_accuracy: 0.8571\n",
      "Epoch 194/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0233 - accuracy: 1.0000 - val_loss: 0.6757 - val_accuracy: 0.8571\n",
      "Epoch 195/10000\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0230 - accuracy: 1.0000 - val_loss: 0.6745 - val_accuracy: 0.8776\n",
      "Epoch 196/10000\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0227 - accuracy: 1.0000 - val_loss: 0.6718 - val_accuracy: 0.8776\n",
      "Epoch 197/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0224 - accuracy: 1.0000 - val_loss: 0.6702 - val_accuracy: 0.8776\n",
      "Epoch 198/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0221 - accuracy: 1.0000 - val_loss: 0.6677 - val_accuracy: 0.8776\n",
      "Epoch 199/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0217 - accuracy: 1.0000 - val_loss: 0.6623 - val_accuracy: 0.8776\n",
      "Epoch 200/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0217 - accuracy: 1.0000 - val_loss: 0.6587 - val_accuracy: 0.8776\n",
      "Epoch 201/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0214 - accuracy: 1.0000 - val_loss: 0.6547 - val_accuracy: 0.8776\n",
      "Epoch 202/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0213 - accuracy: 1.0000 - val_loss: 0.6556 - val_accuracy: 0.8776\n",
      "Epoch 203/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0209 - accuracy: 1.0000 - val_loss: 0.6562 - val_accuracy: 0.8776\n",
      "Epoch 204/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0212 - accuracy: 1.0000 - val_loss: 0.6571 - val_accuracy: 0.8776\n",
      "Epoch 205/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0209 - accuracy: 1.0000 - val_loss: 0.6577 - val_accuracy: 0.8776\n",
      "Epoch 206/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0205 - accuracy: 1.0000 - val_loss: 0.6580 - val_accuracy: 0.8776\n",
      "Epoch 207/10000\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0199 - accuracy: 1.0000 - val_loss: 0.6574 - val_accuracy: 0.8776\n",
      "Epoch 208/10000\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0193 - accuracy: 1.0000 - val_loss: 0.6570 - val_accuracy: 0.8776\n",
      "Epoch 209/10000\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0190 - accuracy: 1.0000 - val_loss: 0.6562 - val_accuracy: 0.8776\n",
      "Epoch 210/10000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0186 - accuracy: 1.0000 - val_loss: 0.6561 - val_accuracy: 0.8776\n",
      "Epoch 211/10000\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0184 - accuracy: 1.0000 - val_loss: 0.6559 - val_accuracy: 0.8776\n",
      "訓練時間：4.80 秒\n",
      "584/584 [==============================] - 0s 823us/step - loss: 0.6814 - accuracy: 0.8777\n",
      "Evaluation on 90% unused data - Loss: 0.6814, Accuracy: 0.8777\n",
      "592/592 [==============================] - 1s 905us/step\n",
      "Test Data MDE report saved to: 5 data per RP transfer revised/DNN 4 mcAPs BEST_5data_2025_02_28.json\n",
      "\n",
      "Test Data Mean MDE: 0.1807 meters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcslab/anaconda3/envs/yang_cuda/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "for i in range(number_of_week):\n",
    "\n",
    "    # 載入模型與標準化器\n",
    "    base_model = load_model(f'{root}/DNN_best_model4mcAP_{weekrepresent[i][0]}week_to_{weekrepresent[i][1]}week_{dataamount}dataPerRP.h5')\n",
    "    scaler = joblib.load(f'{root}/scaler_4mcAP.pkl')\n",
    "\n",
    "    # 讀取測試資料 2024_12_21   2024_12_27   2025_01_03   2025_01_10   2025_02_28\n",
    "    test_file_path = f\"timestamp_allignment_Balanced_{alldatadate[i]}_rtt_logs.csv\"  # 測試資料的檔案名稱\n",
    "    date_test = f\"{alldatadate[i]}\"\n",
    "    modelname = f\"DNN 4 mcAPs BEST_{dataamount}data_{alldatadate[i]}\"\n",
    "    test_data = pd.read_csv(test_file_path, usecols=selected_columns)\n",
    "    # test_data\n",
    "    \n",
    "\n",
    "    # 凍結所有層\n",
    "    for layer in base_model.layers[:-1]:  # 除了最後一層 (Output Layer)\n",
    "        layer.trainable = False\n",
    "\n",
    "    # 確認哪些層可訓練\n",
    "    base_model.summary()\n",
    "\n",
    "    base_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 資料前處理 (一): 刪除前後n筆資料\n",
    "    n = 10\n",
    "    # 確保依據Label排序\n",
    "    test_data = test_data.sort_values(by=label_column).reset_index(drop=True)\n",
    "\n",
    "    # 建立一個空的 DataFrame 用於存放處理後的資料\n",
    "    test_processed_data = pd.DataFrame(columns=test_data.columns)\n",
    "\n",
    "    # 針對每個Label群組進行處理\n",
    "    for label, group in test_data.groupby(label_column):\n",
    "        # 刪除前n筆和後n筆資料\n",
    "        if len(group) > 2 * n:  # 確保群組資料足夠\n",
    "            group = group.iloc[n:-n]\n",
    "        else:\n",
    "            group = pd.DataFrame()  # 若資料不足，刪除整個群組\n",
    "        # 將處理後的群組資料加入\n",
    "        test_processed_data = pd.concat([test_processed_data, group], ignore_index=True)\n",
    "\n",
    "    # test_processed_data\n",
    "    # Calculate the number of rows with NaN values\n",
    "    nan_rows = test_processed_data.isnull().any(axis=1).sum()\n",
    "\n",
    "    # Print the result\n",
    "    print(f\"Number of rows with NaN values: {nan_rows}\")\n",
    "\n",
    "    # 找出包含 NaN 的列\n",
    "    rows_with_nan = test_processed_data[test_processed_data.isnull().any(axis=1)]\n",
    "\n",
    "    # # 印出這些列\n",
    "    # print(\"Rows with NaN values:\")\n",
    "    # print(rows_with_nan)\n",
    "    test_data_imputed = test_processed_data.groupby(label_column).apply(\n",
    "        lambda group: group.fillna(group.mean())\n",
    "    ).reset_index()\n",
    "\n",
    "    # Calculate the number of rows with NaN values\n",
    "    nan_rows = test_data_imputed.isnull().any(axis=1).sum()\n",
    "\n",
    "    # Print the result\n",
    "    print(f\"Number of rows with NaN values: {nan_rows}\")\n",
    "\n",
    "    # 找出包含 NaN 的列\n",
    "    rows_with_nan = test_data_imputed[test_data_imputed.isnull().any(axis=1)]\n",
    "\n",
    "    test_data_imputed\n",
    "\n",
    "    reverse_label_mapping = {v: int(k) - 1 for k, v in label_mapping.items()}  # 讓數字標籤 -1\n",
    "\n",
    "    # 建立 Label 映射\n",
    "    y_test = test_data_imputed[target_column]\n",
    "    y_test_numeric = y_test.map(reverse_label_mapping)\n",
    "\n",
    "    print(\"Final reverse_label_mapping in DNN:\", reverse_label_mapping)\n",
    "    print(\"y_numeric unique values in DNN:\", y_test_numeric.unique())\n",
    "\n",
    "    y_test_numeric\n",
    "\n",
    "    # 把label部分拿掉\n",
    "    X_test = test_data_imputed.drop(columns=['level_1','Label'])\n",
    "\n",
    "    # 確保測試資料的特徵與訓練資料的特徵一致\n",
    "    X_test = X_test[X_testing_selected_columns]  # 選取相同的特徵\n",
    "\n",
    "    print(type(X_test))\n",
    "\n",
    "    # 使用之前訓練時的標準化器 (scaler) 來標準化測試數據\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # 會發現如果用 train_test_split的方法會有資料分布不平均問題，解決辦法如下\n",
    "    from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "\n",
    "    # 轉為 DataFrame 方便操作\n",
    "    data = pd.DataFrame(X_test_scaled)\n",
    "    data['label'] = y_test_numeric  # 加入 label 欄位\n",
    "\n",
    "    print((data['label'] == 10).sum())  # 直接計算 True 的數量\n",
    "\n",
    "    # 轉為 DataFrame 方便操作\n",
    "    data = pd.DataFrame(X_test_scaled)\n",
    "    data['label'] = y_test_numeric  # 加入 label 欄位\n",
    "\n",
    "    # data\n",
    "    # 儲存訓練集（但這時包含 validation 的部分）\n",
    "    # train_data_full = data.groupby('label', group_keys=False).sample(n=N_train, replace=False, random_state=42)\n",
    "    # 儲存訓練集（確保每個類別只選 1 筆，若某類少於 N_train，則取全部）\n",
    "    # train_data_full = data.groupby('label', group_keys=False).apply(lambda x: x.sample(n=min(N_train, len(x)), replace=False, random_state=42)).reset_index(drop=True)\n",
    "    train_data_full = data.groupby('label', group_keys=False).sample(n=N_train, replace=False, random_state=42)\n",
    "    # 確保 `train_data_full` 內部的 `label` 數量正確\n",
    "    print(train_data_full['label'].value_counts())  # 每個類別應該剛好 1 筆\n",
    "    # train_data_full\n",
    "\n",
    "\n",
    "    if N_val > 0:\n",
    "        sss = StratifiedShuffleSplit(n_splits=1, test_size=N_val / N_train, random_state=42)\n",
    "        train_index, val_index = next(sss.split(train_data_full.drop(columns=['label']), train_data_full['label']))\n",
    "        train_data = train_data_full.iloc[train_index]\n",
    "        val_data = train_data_full.iloc[val_index]\n",
    "        \n",
    "    else:\n",
    "        val_data = pd.DataFrame(columns=data.columns)  # 若沒有 validation data，建立空 DataFrame\\\n",
    "        train_data = train_data_full\n",
    "\n",
    "    # 剩下的資料（未被抽入 train_data_full 的部分）直接作為 test set\n",
    "    remaining_data = data.drop(train_data_full.index)\n",
    "    print(len(remaining_data))\n",
    "\n",
    "\n",
    "    # **轉換為 NumPy 陣列**\n",
    "    X_train, y_train = train_data.drop(columns=['label']).values, train_data['label'].values\n",
    "    X_val, y_val = val_data.drop(columns=['label']).values, val_data['label'].values\n",
    "    X_test, y_test = remaining_data.drop(columns=['label']).values, remaining_data['label'].values\n",
    "\n",
    "    # **確認數據切分結果**\n",
    "    print(f\"Training set: {len(X_train)} samples, {len(np.unique(y_train))} unique labels\")\n",
    "    print(f\"Validation set: {len(X_val)} samples, {len(np.unique(y_val))} unique labels\")\n",
    "    print(f\"Test set: {len(X_test)} samples, {len(np.unique(y_test))} unique labels\")\n",
    "\n",
    "   # **計算每個 Set 內各 Label 的資料數量**\n",
    "    train_label_counts = pd.Series(y_train).value_counts().sort_index()\n",
    "    val_label_counts = pd.Series(y_val).value_counts().sort_index()\n",
    "    test_label_counts = pd.Series(y_test).value_counts().sort_index()\n",
    "\n",
    "    # **確保所有 Labels 都有出現在三個 Set 裡**\n",
    "    all_labels = sorted(set(train_label_counts.index) | set(val_label_counts.index) | set(test_label_counts.index))\n",
    "    label_distribution = pd.DataFrame(index=all_labels)\n",
    "\n",
    "    label_distribution[\"Training Set\"] = train_label_counts\n",
    "    label_distribution[\"Validation Set\"] = val_label_counts\n",
    "    label_distribution[\"Test Set\"] = test_label_counts\n",
    "\n",
    "    # **用 0 填補缺失值（表示該 Label 在該 Set 中沒有數據）**\n",
    "    label_distribution = label_distribution.fillna(0).astype(int)\n",
    "\n",
    "    from IPython.display import display\n",
    "    display(label_distribution)\n",
    "\n",
    "    import time\n",
    "    # 記錄開始時間\n",
    "    start_time = time.time()\n",
    "\n",
    "\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "    if N_val > 0:\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    else:\n",
    "        early_stop = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "\n",
    "    # 確保變數命名一致\n",
    "    X_train_small = X_train  # 確保這裡用的變數和前面一致\n",
    "    y_train_small = y_train\n",
    "\n",
    "    # 設定 batch_size\n",
    "    batch_size = min(32, max(8, len(X_train_small) // 2))  # 避免 batch size 過大\n",
    "    print(batch_size)\n",
    "\n",
    "    if N_val > 0:\n",
    "        base_model.fit(X_train_small, y_train_small,validation_data=(X_val, y_val), epochs=10000, batch_size=batch_size, callbacks=[early_stop])\n",
    "    else:\n",
    "        base_model.fit(X_train_small, y_train_small, epochs=10000, batch_size=batch_size, callbacks=[early_stop])\n",
    "\n",
    "\n",
    "    # 記錄結束時間\n",
    "    end_time = time.time()\n",
    "\n",
    "\n",
    "    # Needsave\n",
    "    # 計算訓練時間（秒）\n",
    "    training_time = end_time - start_time\n",
    "    print(f\"訓練時間：{training_time:.2f} 秒\")\n",
    "    ALL_trainingtime.append(training_time)\n",
    "\n",
    "    # Needsave\n",
    "    loss, accuracy = base_model.evaluate(X_test, y_test)\n",
    "    print(f\"Evaluation on 90% unused data - Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    ALL_loss.append(loss)\n",
    "    ALL_accuracy.append(accuracy)\n",
    "\n",
    "    # 預測測試資料\n",
    "    y_test_pred_numeric = base_model.predict(X_test_scaled)\n",
    "    y_pred_classes = np.argmax(y_test_pred_numeric, axis=1)\n",
    "\n",
    "    # 轉換為原本的 Label\n",
    "    y_test_pred_labels = [label_mapping[str(num + 1)] for num in y_pred_classes]  # 補回 +1\n",
    "    y_test_pred_labels\n",
    "\n",
    "    # 讀取測試資料的實際 Label\n",
    "    y_test_actual = test_data_imputed[target_column]\n",
    "    test_data_imputed\n",
    "\n",
    "    # 取得預測與實際座標\n",
    "    y_test_pred_coordinates = np.array([label_to_coordinates[label] for label in y_test_pred_labels])\n",
    "    y_test_actual_coordinates = np.array([label_to_coordinates[label] for label in y_test_actual])\n",
    "\n",
    "    # 計算 MDE (Mean Distance Error)\n",
    "    distances = np.linalg.norm(y_test_pred_coordinates - y_test_actual_coordinates, axis=1)\n",
    "    mean_mde = np.mean(distances)\n",
    "\n",
    "    # 記錄每個 RP 的 MDE\n",
    "    mde_report_test = {}\n",
    "    for true_label, distance in zip(y_test_actual, distances):\n",
    "        if true_label not in mde_report_test:\n",
    "            mde_report_test[true_label] = []\n",
    "        mde_report_test[true_label].append(distance)\n",
    "\n",
    "    # 計算測試資料的 MDE 平均值\n",
    "    mde_report_test_avg = {label: {\"mde\": np.mean(dists), \"count\": len(dists)} for label, dists in mde_report_test.items()}\n",
    "\n",
    "    # 儲存 MDE 結果到 JSON 檔案\n",
    "    test_file_path = f\"{root}/{modelname}.json\"\n",
    "    with open(test_file_path, \"w\") as f:\n",
    "        json.dump(mde_report_test_avg, f, indent=4)\n",
    "\n",
    "    # Needsave\n",
    "    print(f\"Test Data MDE report saved to: {test_file_path}\")\n",
    "    print(f\"\\nTest Data Mean MDE: {mean_mde:.4f} meters\")\n",
    "    ALL_mean_mde.append(mean_mde)\n",
    "\n",
    "\n",
    "    base_model.save(f'{root}/DNN_best_model4mcAP_{weekrepresent[i+1][0]}week_to_{weekrepresent[i+1][1]}week_{dataamount}dataPerRP.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.522179126739502, 3.409670829772949, 2.1575117111206055, 3.7845749855041504, 4.799942255020142]\n",
      "[0.6241098642349243, 0.6044995784759521, 0.6342905759811401, 0.5949484705924988, 0.6813639402389526]\n",
      "[0.8876984119415283, 0.8904116153717041, 0.8591161370277405, 0.8986788392066956, 0.8776581287384033]\n",
      "[0.16201424854050941, 0.1372144266536673, 0.19882523020417187, 0.13925343658455305, 0.18065784810943838]\n"
     ]
    }
   ],
   "source": [
    "print(ALL_trainingtime)\n",
    "print(ALL_loss)\n",
    "print(ALL_accuracy) \n",
    "print(ALL_mean_mde) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yang_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
