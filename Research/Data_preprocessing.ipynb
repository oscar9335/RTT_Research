{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data prepricessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1 : txt to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = '2025_02_28'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for input directory\n",
    "input_directory = \"txt_files_RAW_data\"  # Replace with your directory path\n",
    "output_directory = os.path.join(input_directory, f'{date}\\\\raw data')  # Output directory path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved at: txt_files_RAW_data\\2025_02_28\\raw data\\1-10_rtt_log.csv\n",
      "File saved at: txt_files_RAW_data\\2025_02_28\\raw data\\1-11_rtt_log.csv\n",
      "File saved at: txt_files_RAW_data\\2025_02_28\\raw data\\1-1_rtt_log.csv\n",
      "File saved at: txt_files_RAW_data\\2025_02_28\\raw data\\1-2_rtt_log.csv\n",
      "File saved at: txt_files_RAW_data\\2025_02_28\\raw data\\1-3_rtt_log.csv\n",
      "File saved at: txt_files_RAW_data\\2025_02_28\\raw data\\1-4_rtt_log.csv\n",
      "File saved at: txt_files_RAW_data\\2025_02_28\\raw data\\1-5_rtt_log.csv\n",
      "File saved at: txt_files_RAW_data\\2025_02_28\\raw data\\1-6_rtt_log.csv\n",
      "File saved at: txt_files_RAW_data\\2025_02_28\\raw data\\1-7_rtt_log.csv\n",
      "File saved at: txt_files_RAW_data\\2025_02_28\\raw data\\1-8_rtt_log.csv\n",
      "File saved at: txt_files_RAW_data\\2025_02_28\\raw data\\1-9_rtt_log.csv\n",
      "File saved at: txt_files_RAW_data\\2025_02_28\\raw data\\10-11_rtt_log.csv\n",
      "File saved at: txt_files_RAW_data\\2025_02_28\\raw data\\10-1_rtt_log.csv\n",
      "File saved at: txt_files_RAW_data\\2025_02_28\\raw data\\11-10_rtt_log.csv\n",
      "File saved at: txt_files_RAW_data\\2025_02_28\\raw data\\11-11_rtt_log.csv\n",
      "File saved at: txt_files_RAW_data\\2025_02_28\\raw data\\11-1_rtt_log.csv\n",
      "File saved at: txt_files_RAW_data\\2025_02_28\\raw data\\11-2_rtt_log.csv\n",
      "File saved at: txt_files_RAW_data\\2025_02_28\\raw data\\11-3_rtt_log.csv\n",
      "File saved at: txt_files_RAW_data\\2025_02_28\\raw data\\11-4_rtt_log.csv\n",
      "File saved at: txt_files_RAW_data\\2025_02_28\\raw data\\11-5_rtt_log.csv\n",
      "File saved at: txt_files_RAW_data\\2025_02_28\\raw data\\11-6_rtt_log.csv\n",
      "File saved at: txt_files_RAW_data\\2025_02_28\\raw data\\11-7_rtt_log.csv\n",
      "File saved at: txt_files_RAW_data\\2025_02_28\\raw data\\11-8_rtt_log.csv\n",
      "File saved at: txt_files_RAW_data\\2025_02_28\\raw data\\11-9_rtt_log.csv\n",
      "File saved at: txt_files_RAW_data\\2025_02_28\\raw data\\2-11_rtt_log.csv\n",
      "File saved at: txt_files_RAW_data\\2025_02_28\\raw data\\2-1_rtt_log.csv\n",
      "File saved at: txt_files_RAW_data\\2025_02_28\\raw data\\3-11_rtt_log.csv\n",
      "File saved at: txt_files_RAW_data\\2025_02_28\\raw data\\3-1_rtt_log.csv\n",
      "File saved at: txt_files_RAW_data\\2025_02_28\\raw data\\4-11_rtt_log.csv\n",
      "File saved at: txt_files_RAW_data\\2025_02_28\\raw data\\4-1_rtt_log.csv\n",
      "File saved at: txt_files_RAW_data\\2025_02_28\\raw data\\5-11_rtt_log.csv\n",
      "File saved at: txt_files_RAW_data\\2025_02_28\\raw data\\5-1_rtt_log.csv\n",
      "File saved at: txt_files_RAW_data\\2025_02_28\\raw data\\6-10_rtt_log.csv\n",
      "File saved at: txt_files_RAW_data\\2025_02_28\\raw data\\6-11_rtt_log.csv\n",
      "File saved at: txt_files_RAW_data\\2025_02_28\\raw data\\6-1_rtt_log.csv\n",
      "File saved at: txt_files_RAW_data\\2025_02_28\\raw data\\6-2_rtt_log.csv\n",
      "File saved at: txt_files_RAW_data\\2025_02_28\\raw data\\6-3_rtt_log.csv\n",
      "File saved at: txt_files_RAW_data\\2025_02_28\\raw data\\6-4_rtt_log.csv\n",
      "File saved at: txt_files_RAW_data\\2025_02_28\\raw data\\6-5_rtt_log.csv\n",
      "File saved at: txt_files_RAW_data\\2025_02_28\\raw data\\6-6_rtt_log.csv\n",
      "File saved at: txt_files_RAW_data\\2025_02_28\\raw data\\6-7_rtt_log.csv\n",
      "File saved at: txt_files_RAW_data\\2025_02_28\\raw data\\6-8_rtt_log.csv\n",
      "File saved at: txt_files_RAW_data\\2025_02_28\\raw data\\6-9_rtt_log.csv\n",
      "File saved at: txt_files_RAW_data\\2025_02_28\\raw data\\7-11_rtt_log.csv\n",
      "File saved at: txt_files_RAW_data\\2025_02_28\\raw data\\7-1_rtt_log.csv\n",
      "File saved at: txt_files_RAW_data\\2025_02_28\\raw data\\8-11_rtt_log.csv\n",
      "File saved at: txt_files_RAW_data\\2025_02_28\\raw data\\8-1_rtt_log.csv\n",
      "File saved at: txt_files_RAW_data\\2025_02_28\\raw data\\9-11_rtt_log.csv\n",
      "File saved at: txt_files_RAW_data\\2025_02_28\\raw data\\9-1_rtt_log.csv\n"
     ]
    }
   ],
   "source": [
    "# Create the output directory if it doesn't exist\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "\n",
    "# Regular expression pattern to match each line of data\n",
    "pattern = re.compile(\n",
    "    r\"Label: (.*?), Timestamp: (.*?), AP SSID: (.*?), BSSID: (.*?), Rssi: (-?\\d+), Distance: (-?\\d+) mm, StdDev: (\\d+) mm, timeStemp: (\\d+), mcOn: (true|false)\"\n",
    ")\n",
    "\n",
    "\n",
    "# Iterate through all files in the input directory\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith(\".txt\"):  # Process only .txt files\n",
    "        input_file_path = os.path.join(input_directory, filename)\n",
    "        \n",
    "        # Generate output file name based on input file name (replace .txt with .csv)\n",
    "        output_file_name = filename.replace(\".txt\", \".csv\")\n",
    "        output_file_path = os.path.join(output_directory, output_file_name)\n",
    "        \n",
    "        # Open the input text file and corresponding output CSV file\n",
    "        with open(input_file_path, 'r') as infile, open(output_file_path, 'w', newline='') as outfile:\n",
    "            # Define the CSV writer\n",
    "            writer = csv.writer(outfile)\n",
    "\n",
    "            # Write the CSV header\n",
    "            writer.writerow([\"Label\", \"Timestamp\", \"AP SSID\", \"BSSID\", \"Rssi\", \"Distance (mm)\", \"StdDev (mm)\", \"timeStemp\", \"mcOn\"])\n",
    "\n",
    "            # Process each line\n",
    "            for line in infile:\n",
    "                match = pattern.match(line)\n",
    "                if match:\n",
    "                    # Extract data using groups\n",
    "                    writer.writerow(match.groups())\n",
    "                    \n",
    "        # Output the path to the saved CSV file\n",
    "        print(f\"File saved at: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2 : using timestamp to alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定來源資料夾和目標資料夾\n",
    "input_folder = output_directory  # 資料來源資料夾路徑\n",
    "output_folder = f'{date}\\\\timestamp allign data'  # 處理後檔案的存放資料夾路徑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved: 2025_02_28\\timestamp allign data\\processed_1-10_rtt_log.csv\n",
      "Processed and saved: 2025_02_28\\timestamp allign data\\processed_1-11_rtt_log.csv\n",
      "Processed and saved: 2025_02_28\\timestamp allign data\\processed_1-1_rtt_log.csv\n",
      "Processed and saved: 2025_02_28\\timestamp allign data\\processed_1-2_rtt_log.csv\n",
      "Processed and saved: 2025_02_28\\timestamp allign data\\processed_1-3_rtt_log.csv\n",
      "Processed and saved: 2025_02_28\\timestamp allign data\\processed_1-4_rtt_log.csv\n",
      "Processed and saved: 2025_02_28\\timestamp allign data\\processed_1-5_rtt_log.csv\n",
      "Processed and saved: 2025_02_28\\timestamp allign data\\processed_1-6_rtt_log.csv\n",
      "Processed and saved: 2025_02_28\\timestamp allign data\\processed_1-7_rtt_log.csv\n",
      "Processed and saved: 2025_02_28\\timestamp allign data\\processed_1-8_rtt_log.csv\n",
      "Processed and saved: 2025_02_28\\timestamp allign data\\processed_1-9_rtt_log.csv\n",
      "Processed and saved: 2025_02_28\\timestamp allign data\\processed_10-11_rtt_log.csv\n",
      "Processed and saved: 2025_02_28\\timestamp allign data\\processed_10-1_rtt_log.csv\n",
      "Processed and saved: 2025_02_28\\timestamp allign data\\processed_11-10_rtt_log.csv\n",
      "Processed and saved: 2025_02_28\\timestamp allign data\\processed_11-11_rtt_log.csv\n",
      "Processed and saved: 2025_02_28\\timestamp allign data\\processed_11-1_rtt_log.csv\n",
      "Processed and saved: 2025_02_28\\timestamp allign data\\processed_11-2_rtt_log.csv\n",
      "Processed and saved: 2025_02_28\\timestamp allign data\\processed_11-3_rtt_log.csv\n",
      "Processed and saved: 2025_02_28\\timestamp allign data\\processed_11-4_rtt_log.csv\n",
      "Processed and saved: 2025_02_28\\timestamp allign data\\processed_11-5_rtt_log.csv\n",
      "Processed and saved: 2025_02_28\\timestamp allign data\\processed_11-6_rtt_log.csv\n",
      "Processed and saved: 2025_02_28\\timestamp allign data\\processed_11-7_rtt_log.csv\n",
      "Processed and saved: 2025_02_28\\timestamp allign data\\processed_11-8_rtt_log.csv\n",
      "Processed and saved: 2025_02_28\\timestamp allign data\\processed_11-9_rtt_log.csv\n",
      "Processed and saved: 2025_02_28\\timestamp allign data\\processed_2-11_rtt_log.csv\n",
      "Processed and saved: 2025_02_28\\timestamp allign data\\processed_2-1_rtt_log.csv\n",
      "Processed and saved: 2025_02_28\\timestamp allign data\\processed_3-11_rtt_log.csv\n",
      "Processed and saved: 2025_02_28\\timestamp allign data\\processed_3-1_rtt_log.csv\n",
      "Processed and saved: 2025_02_28\\timestamp allign data\\processed_4-11_rtt_log.csv\n",
      "Processed and saved: 2025_02_28\\timestamp allign data\\processed_4-1_rtt_log.csv\n",
      "Processed and saved: 2025_02_28\\timestamp allign data\\processed_5-11_rtt_log.csv\n",
      "Processed and saved: 2025_02_28\\timestamp allign data\\processed_5-1_rtt_log.csv\n",
      "Processed and saved: 2025_02_28\\timestamp allign data\\processed_6-10_rtt_log.csv\n",
      "Processed and saved: 2025_02_28\\timestamp allign data\\processed_6-11_rtt_log.csv\n",
      "Processed and saved: 2025_02_28\\timestamp allign data\\processed_6-1_rtt_log.csv\n",
      "Processed and saved: 2025_02_28\\timestamp allign data\\processed_6-2_rtt_log.csv\n",
      "Processed and saved: 2025_02_28\\timestamp allign data\\processed_6-3_rtt_log.csv\n",
      "Processed and saved: 2025_02_28\\timestamp allign data\\processed_6-4_rtt_log.csv\n",
      "Processed and saved: 2025_02_28\\timestamp allign data\\processed_6-5_rtt_log.csv\n",
      "Processed and saved: 2025_02_28\\timestamp allign data\\processed_6-6_rtt_log.csv\n",
      "Processed and saved: 2025_02_28\\timestamp allign data\\processed_6-7_rtt_log.csv\n",
      "Processed and saved: 2025_02_28\\timestamp allign data\\processed_6-8_rtt_log.csv\n",
      "Processed and saved: 2025_02_28\\timestamp allign data\\processed_6-9_rtt_log.csv\n",
      "Processed and saved: 2025_02_28\\timestamp allign data\\processed_7-11_rtt_log.csv\n",
      "Processed and saved: 2025_02_28\\timestamp allign data\\processed_7-1_rtt_log.csv\n",
      "Processed and saved: 2025_02_28\\timestamp allign data\\processed_8-11_rtt_log.csv\n",
      "Processed and saved: 2025_02_28\\timestamp allign data\\processed_8-1_rtt_log.csv\n",
      "Processed and saved: 2025_02_28\\timestamp allign data\\processed_9-11_rtt_log.csv\n",
      "Processed and saved: 2025_02_28\\timestamp allign data\\processed_9-1_rtt_log.csv\n",
      "所有檔案處理完成！\n"
     ]
    }
   ],
   "source": [
    "# 確保目標資料夾存在\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# 讀取資料夾中的所有檔案\n",
    "for file_name in os.listdir(input_folder):\n",
    "    if file_name.endswith('.csv'):  # 檢查是否為 CSV 檔案\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        data = pd.read_csv(file_path)\n",
    "\n",
    "        # Step 1: 修改 BSSID 對應的 AP SSID\n",
    "        data.loc[data['BSSID'] == '24:29:34:e2:4c:36', 'AP SSID'] = 'AP1'\n",
    "        data.loc[data['BSSID'] == '24:29:34:e1:ef:d4', 'AP SSID'] = 'AP2'\n",
    "        data.loc[data['BSSID'] == 'e4:5e:1b:a0:5e:85', 'AP SSID'] = 'AP3'\n",
    "        data.loc[data['BSSID'] == 'b0:e4:d5:88:16:86', 'AP SSID'] = 'AP4'\n",
    "\n",
    "        # Step 2: 忽略 timeStemp 欄位的最後一位數\n",
    "        data['timeStemp'] = data['timeStemp'].astype(str).str[:-1]  # 刪除最後一位數\n",
    "        data['timeStemp'] = data['timeStemp'].astype(int)  # 轉回數字型別（如果需要）\n",
    "\n",
    "        # Step 3: Group by Timestamp 和 AP SSID，計算平均值\n",
    "        grouped_data = (\n",
    "            data.groupby(['timeStemp','Label' ,'AP SSID'])\n",
    "            .agg({\n",
    "                # 'Label': 'first',\n",
    "                'Distance (mm)': 'mean',\n",
    "                'Rssi': 'mean',\n",
    "                'StdDev (mm)': 'mean'\n",
    "            })\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        # Step 3: 將資料轉換成每個 Timestamp 一 row\n",
    "        pivoted_data = grouped_data.pivot(\n",
    "            \n",
    "            index=['timeStemp','Label'],\n",
    "            columns='AP SSID',\n",
    "            values=['Distance (mm)', 'Rssi', 'StdDev (mm)']\n",
    "        )\n",
    "\n",
    "        # 展平多層欄位名稱\n",
    "        pivoted_data.columns = [f\"{ap}_{metric}\" for metric, ap in pivoted_data.columns]\n",
    "        pivoted_data.reset_index(inplace=True)\n",
    "\n",
    "        # 將處理後的結果存成新的 CSV 檔案\n",
    "        output_file_path = os.path.join(output_folder, f\"processed_{file_name}\")\n",
    "        pivoted_data.to_csv(output_file_path, index=False)\n",
    "\n",
    "        print(f\"Processed and saved: {output_file_path}\")\n",
    "\n",
    "print(\"所有檔案處理完成！\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3 : Combine all csv to one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有檔案已合併並儲存為: 2025_02_28\\timestamp_allignment_2025_02_28_rtt_logs.csv\n"
     ]
    }
   ],
   "source": [
    "# 指定資料夾路徑\n",
    "input_folder = output_folder  # 替換為你的資料夾路徑\n",
    "output_file = f'{date}\\\\timestamp_allignment_{date}_rtt_logs.csv'  # 合併後的輸出檔案名稱\n",
    "\n",
    "# 獲取資料夾內所有 CSV 檔案的路徑\n",
    "file_paths = [os.path.join(input_folder, file) for file in os.listdir(input_folder) if file.endswith('.csv')]\n",
    "\n",
    "# 合併所有 CSV 檔案\n",
    "combined_data = pd.concat([pd.read_csv(file_path) for file_path in file_paths], ignore_index=True)\n",
    "\n",
    "# 儲存合併後的檔案\n",
    "combined_data.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"所有檔案已合併並儲存為: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 4 : make all number of data in each RP the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   timeStemp Label  AP1_Distance (mm)  AP2_Distance (mm)  AP3_Distance (mm)  \\\n",
      "0   33824855  1-10             -541.0             9339.0            10213.0   \n",
      "1   33824868  1-10             -386.0            10796.0            14834.0   \n",
      "2   33824879  1-10             -180.0             9829.0            12568.0   \n",
      "3   33824891  1-10             -297.0             9214.0             8688.0   \n",
      "4   33824920  1-10             -180.0             8071.0             9163.0   \n",
      "\n",
      "   AP4_Distance (mm)  AP1_Rssi  AP2_Rssi  AP3_Rssi  AP4_Rssi  AP1_StdDev (mm)  \\\n",
      "0             5907.0     -57.0     -80.0     -67.0     -57.0             97.0   \n",
      "1             6643.0     -57.0     -80.0     -68.0     -56.0            106.0   \n",
      "2             6730.0     -58.0     -79.0     -67.0     -53.0            375.0   \n",
      "3             4442.0     -59.0     -77.0     -61.0     -61.0            378.0   \n",
      "4             5633.0     -59.0     -75.0     -62.0     -56.0            253.0   \n",
      "\n",
      "   AP2_StdDev (mm)  AP3_StdDev (mm)  AP4_StdDev (mm)  \n",
      "0            705.0           1783.0             95.0  \n",
      "1            120.0            115.0            196.0  \n",
      "2            600.0           2304.0           1563.0  \n",
      "3           1307.0            883.0            410.0  \n",
      "4           1292.0            545.0            217.0  \n",
      "處理後的資料已儲存至 2025_02_28\\timestamp_allignment_Balanced_2025_02_28_rtt_logs.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\吳定洋\\AppData\\Local\\Temp\\ipykernel_18584\\77147034.py:17: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_balanced = df.groupby('Label').apply(lambda x: x.sample(n=min_count, random_state=42)).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "# 讀取 Excel 檔案\n",
    "file_path = output_file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "# 假設 label 的欄位名稱為 'label'\n",
    "# 計算每個 label 的資料筆數\n",
    "label_counts = df['Label'].value_counts()\n",
    "\n",
    "# 找出最少的資料筆數\n",
    "min_count = label_counts.min()\n",
    "max_count = label_counts.max()\n",
    "\n",
    "\n",
    "# 隨機抽取每個 label 的資料，使其數量等於 min_count\n",
    "df_balanced = df.groupby('Label').apply(lambda x: x.sample(n=min_count, random_state=42)).reset_index(drop=True)\n",
    "\n",
    "# 儲存處理後的資料\n",
    "output_path = f'{date}\\\\timestamp_allignment_Balanced_{date}_rtt_logs.csv'\n",
    "df_balanced.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"處理後的資料已儲存至 {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 406\n",
      "Max: 615\n"
     ]
    }
   ],
   "source": [
    "print(\"Min: \" + str(min_count))\n",
    "print(\"Max: \" + str(max_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALL finished\n"
     ]
    }
   ],
   "source": [
    "print(\"ALL finished\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
