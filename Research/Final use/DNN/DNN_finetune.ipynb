{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_coordinates = {\n",
    "    \"1-1\": (0, 0), \"1-2\": (0.6, 0), \"1-3\": (1.2, 0), \"1-4\": (1.8, 0), \"1-5\": (2.4, 0), \"1-6\": (3.0, 0),\"1-7\": (3.6, 0), \"1-8\": (4.2, 0), \"1-9\": (4.8, 0), \"1-10\": (5.4, 0), \"1-11\": (6.0, 0),\n",
    "    \"2-1\": (0, 0.6), \"2-11\": (6.0, 0.6),\n",
    "    \"3-1\": (0, 1.2), \"3-11\": (6.0, 1.2),\n",
    "    \"4-1\": (0, 1.8), \"4-11\": (6.0, 1.8),\n",
    "    \"5-1\": (0, 2.4), \"5-11\": (6.0, 2.4),\n",
    "    \"6-1\": (0, 3.0), \"6-2\": (0.6, 3.0), \"6-3\": (1.2, 3.0), \"6-4\": (1.8, 3.0), \"6-5\": (2.4, 3.0),\"6-6\": (3.0, 3.0), \"6-7\": (3.6, 3.0), \"6-8\": (4.2, 3.0), \"6-9\": (4.8, 3.0), \"6-10\": (5.4, 3.0), \"6-11\": (6.0, 3.0),\n",
    "    \"7-1\": (0, 3.6), \"7-11\": (6.0, 3.6),\n",
    "    \"8-1\": (0, 4.2), \"8-11\": (6.0, 4.2),\n",
    "    \"9-1\": (0, 4.8), \"9-11\": (6.0, 4.8),\n",
    "    \"10-1\": (0, 5.4), \"10-11\": (6.0, 5.4),\n",
    "    \"11-1\": (0, 6.0), \"11-2\": (0.6, 6.0), \"11-3\": (1.2, 6.0), \"11-4\": (1.8, 6.0), \"11-5\": (2.4, 6.0),\"11-6\": (3.0, 6.0), \"11-7\": (3.6, 6.0), \"11-8\": (4.2, 6.0), \"11-9\": (4.8, 6.0), \"11-10\": (5.4, 6.0), \"11-11\": (6.0, 6.0)\n",
    "}\n",
    "label_mapping = {\n",
    "    '11': '1-1','10': '1-2','9': '1-3','8': '1-4','7': '1-5','6': '1-6','5': '1-7','4': '1-8','3': '1-9','2': '1-10','1': '1-11',\n",
    "    '12': '2-1','30': '2-11',\n",
    "    '13': '3-1','29': '3-11',\n",
    "    '14': '4-1','28': '4-11',\n",
    "    '15': '5-1','27': '5-11',\n",
    "    '16': '6-1','17': '6-2','18': '6-3','19': '6-4','20': '6-5','21': '6-6','22': '6-7','23': '6-8','24': '6-9','25': '6-10','26': '6-11',\n",
    "    '49': '7-1','31': '7-11',\n",
    "    '48': '8-1','32': '8-11',\n",
    "    '47': '9-1','33': '9-11',\n",
    "    '46': '10-1','34': '10-11',\n",
    "    '45': '11-1','44': '11-2','43': '11-3','42': '11-4','41': '11-5','40': '11-6','39': '11-7','38': '11-8','37': '11-9','36': '11-10','35': '11-11'\n",
    "}\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import joblib  # 用於保存模型\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "selected_columns = ['Label',\n",
    "                        'AP1_Distance (mm)','AP4_Distance (mm)',\n",
    "                        'AP1_StdDev (mm)','AP4_StdDev (mm)',\n",
    "                                'AP1_Rssi','AP2_Rssi','AP3_Rssi','AP4_Rssi']  \n",
    "\n",
    "X_testing_selected_columns = [\n",
    "                        'AP1_Distance (mm)','AP4_Distance (mm)',\n",
    "                        'AP1_StdDev (mm)','AP4_StdDev (mm)'\n",
    "                                'AP1_Rssi','AP2_Rssi','AP3_Rssi','AP4_Rssi']  \n",
    "\n",
    "label_column = 'Label'\n",
    "target_column = 'Label'\n",
    "\n",
    "\n",
    "#  'AP1_Rssi','AP2_Rssi','AP3_Rssi','AP4_Rssi'\n",
    "# 'AP1_Distance (mm)','AP2_Distance (mm)','AP3_Distance (mm)','AP4_Distance (mm)',\n",
    "# 'AP1_StdDev (mm)','AP2_StdDev (mm)','AP3_StdDev (mm)','AP4_StdDev (mm)',\n",
    "dataamount_set = [1,5,10,20,40]\n",
    "N_val_set = [0,1,2,4,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dada in range(5):\n",
    "\n",
    "    dataamount = dataamount_set[dada]\n",
    "    N_val = N_val_set[dada]\n",
    "\n",
    "    N_train = dataamount # 訓練集每個類別至少要有 N_train 筆資料\n",
    "    test_val_ratio = 1  # 剩餘資料中，50% 作為驗證集，50% 作為測試集\n",
    "\n",
    "    weekrepresent = [['0','0'],['0','1'],['1','2'],['2','3'],['3','4'],['4','5']]\n",
    "    alldatadate = ['2024_12_21','2024_12_27','2025_01_03','2025_01_10','2025_02_28']\n",
    "    number_of_week = len(alldatadate)\n",
    "\n",
    "    ALL_model_loss = []\n",
    "    ALL_model_accuracy = []\n",
    "    ALL_model_mean_mde = []\n",
    "\n",
    "    for mmm in range(10):\n",
    "\n",
    "        ALL_trainingtime = []\n",
    "        ALL_loss = []\n",
    "        ALL_accuracy = []\n",
    "        ALL_mean_mde = []\n",
    "\n",
    "        for i in range(number_of_week):\n",
    "\n",
    "            if i == 0:\n",
    "                base_model = load_model(f'2mcAPbestbset_{mmm}.h5')\n",
    "                \n",
    "            else: \n",
    "                base_model = load_model(f'DNN_best_model_{weekrepresent[i][0]}to{weekrepresent[i][1]}_{mmm}.h5')\n",
    "\n",
    "            # 標準化器不會有差別\n",
    "            scaler = joblib.load(f'scaler_test.pkl')\n",
    "\n",
    "            # 讀取測試資料 2024_12_21   2024_12_27   2025_01_03   2025_01_10   2025_02_28\n",
    "            test_file_path = f\"timestamp_allignment_Balanced_{alldatadate[i]}_rtt_logs.csv\"  # 測試資料的檔案名稱\n",
    "            date_test = f\"{alldatadate[i]}\"\n",
    "            # modelname = f\"DNN {ap}s BEST_{dataamount}data_{alldatadate[i]}\"\n",
    "            test_data = pd.read_csv(test_file_path, usecols=selected_columns)\n",
    "            # test_data\n",
    "            \n",
    "\n",
    "            # 凍結所有層\n",
    "            for layer in base_model.layers[:-1]:  # 除了最後一層 (Output Layer)\n",
    "                layer.trainable = False\n",
    "\n",
    "            # 確認哪些層可訓練\n",
    "            base_model.summary()\n",
    "\n",
    "            base_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "                        loss='sparse_categorical_crossentropy',\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # 資料前處理 (一): 刪除前後n筆資料\n",
    "            n = 10\n",
    "            # 確保依據Label排序\n",
    "            test_data = test_data.sort_values(by=label_column).reset_index(drop=True)\n",
    "\n",
    "            # 建立一個空的 DataFrame 用於存放處理後的資料\n",
    "            test_processed_data = pd.DataFrame(columns=test_data.columns)\n",
    "\n",
    "            # 針對每個Label群組進行處理\n",
    "            for label, group in test_data.groupby(label_column):\n",
    "                # 刪除前n筆和後n筆資料\n",
    "                if len(group) > 2 * n:  # 確保群組資料足夠\n",
    "                    group = group.iloc[n:-n]\n",
    "                else:\n",
    "                    group = pd.DataFrame()  # 若資料不足，刪除整個群組\n",
    "                # 將處理後的群組資料加入\n",
    "                test_processed_data = pd.concat([test_processed_data, group], ignore_index=True)\n",
    "\n",
    "            # test_processed_data\n",
    "            # Calculate the number of rows with NaN values\n",
    "            nan_rows = test_processed_data.isnull().any(axis=1).sum()\n",
    "\n",
    "            # Print the result\n",
    "            # print(f\"Number of rows with NaN values: {nan_rows}\")\n",
    "\n",
    "            # 找出包含 NaN 的列\n",
    "            rows_with_nan = test_processed_data[test_processed_data.isnull().any(axis=1)]\n",
    "\n",
    "            # # 印出這些列\n",
    "            # print(\"Rows with NaN values:\")\n",
    "            # print(rows_with_nan)\n",
    "            test_data_imputed = test_processed_data.groupby(label_column).apply(\n",
    "                lambda group: group.fillna(group.mean())\n",
    "            ).reset_index()\n",
    "\n",
    "            # Calculate the number of rows with NaN values\n",
    "            nan_rows = test_data_imputed.isnull().any(axis=1).sum()\n",
    "\n",
    "            # Print the result\n",
    "            # print(f\"Number of rows with NaN values: {nan_rows}\")\n",
    "\n",
    "            # 找出包含 NaN 的列\n",
    "            rows_with_nan = test_data_imputed[test_data_imputed.isnull().any(axis=1)]\n",
    "\n",
    "            test_data_imputed\n",
    "\n",
    "            reverse_label_mapping = {v: int(k) - 1 for k, v in label_mapping.items()}  # 讓數字標籤 -1\n",
    "\n",
    "            # 建立 Label 映射\n",
    "            y_test = test_data_imputed[target_column]\n",
    "            y_test_numeric = y_test.map(reverse_label_mapping)\n",
    "\n",
    "            # print(\"Final reverse_label_mapping in DNN:\", reverse_label_mapping)\n",
    "            # print(\"y_numeric unique values in DNN:\", y_test_numeric.unique())\n",
    "\n",
    "            y_test_numeric\n",
    "\n",
    "            # 把label部分拿掉\n",
    "            X_test = test_data_imputed.drop(columns=['level_1','Label'])\n",
    "            print(X_test.head())\n",
    "\n",
    "            # 確保測試資料的特徵與訓練資料的特徵一致\n",
    "            X_test = X_test[X_testing_selected_columns]  # 選取相同的特徵\n",
    "\n",
    "            # print(type(X_test))\n",
    "\n",
    "            # 使用之前訓練時的標準化器 (scaler) 來標準化測試數據\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "            # 會發現如果用 train_test_split的方法會有資料分布不平均問題，解決辦法如下\n",
    "            from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "\n",
    "            # 轉為 DataFrame 方便操作\n",
    "            data = pd.DataFrame(X_test_scaled)\n",
    "            data['label'] = y_test_numeric  # 加入 label 欄位\n",
    "\n",
    "            # print((data['label'] == 10).sum())  # 直接計算 True 的數量\n",
    "\n",
    "            # 轉為 DataFrame 方便操作\n",
    "            data = pd.DataFrame(X_test_scaled)\n",
    "            data['label'] = y_test_numeric  # 加入 label 欄位\n",
    "\n",
    "            # data\n",
    "            # 儲存訓練集（但這時包含 validation 的部分）\n",
    "            # train_data_full = data.groupby('label', group_keys=False).sample(n=N_train, replace=False, random_state=42)\n",
    "            # 儲存訓練集（確保每個類別只選 1 筆，若某類少於 N_train，則取全部）\n",
    "            # train_data_full = data.groupby('label', group_keys=False).apply(lambda x: x.sample(n=min(N_train, len(x)), replace=False, random_state=42)).reset_index(drop=True)\n",
    "            train_data_full = data.groupby('label', group_keys=False).sample(n=N_train, replace=False, random_state=42) # \n",
    "            # 確保 `train_data_full` 內部的 `label` 數量正確\n",
    "            # print(train_data_full['label'].value_counts())  # 每個類別應該剛好 1 筆\n",
    "            # train_data_full\n",
    "\n",
    "\n",
    "            if N_val > 0:\n",
    "                sss = StratifiedShuffleSplit(n_splits=1, test_size=N_val / N_train, random_state=42) # \n",
    "                train_index, val_index = next(sss.split(train_data_full.drop(columns=['label']), train_data_full['label']))\n",
    "                train_data = train_data_full.iloc[train_index]\n",
    "                val_data = train_data_full.iloc[val_index]\n",
    "                \n",
    "            else:\n",
    "                val_data = pd.DataFrame(columns=data.columns)  # 若沒有 validation data，建立空 DataFrame\\\n",
    "                train_data = train_data_full\n",
    "\n",
    "            # 剩下的資料（未被抽入 train_data_full 的部分）直接作為 test set\n",
    "            remaining_data = data.drop(train_data_full.index)\n",
    "            # print(len(remaining_data))\n",
    "\n",
    "\n",
    "            # **轉換為 NumPy 陣列**\n",
    "            X_train, y_train = train_data.drop(columns=['label']).values, train_data['label'].values\n",
    "            X_val, y_val = val_data.drop(columns=['label']).values, val_data['label'].values\n",
    "            X_test, y_test = remaining_data.drop(columns=['label']).values, remaining_data['label'].values\n",
    "\n",
    "            # **確認數據切分結果**\n",
    "            # print(f\"Training set: {len(X_train)} samples, {len(np.unique(y_train))} unique labels\")\n",
    "            # print(f\"Validation set: {len(X_val)} samples, {len(np.unique(y_val))} unique labels\")\n",
    "            # print(f\"Test set: {len(X_test)} samples, {len(np.unique(y_test))} unique labels\")\n",
    "\n",
    "        # **計算每個 Set 內各 Label 的資料數量**\n",
    "            train_label_counts = pd.Series(y_train).value_counts().sort_index()\n",
    "            val_label_counts = pd.Series(y_val).value_counts().sort_index()\n",
    "            test_label_counts = pd.Series(y_test).value_counts().sort_index()\n",
    "\n",
    "            # **確保所有 Labels 都有出現在三個 Set 裡**\n",
    "            all_labels = sorted(set(train_label_counts.index) | set(val_label_counts.index) | set(test_label_counts.index))\n",
    "            label_distribution = pd.DataFrame(index=all_labels)\n",
    "\n",
    "            label_distribution[\"Training Set\"] = train_label_counts\n",
    "            label_distribution[\"Validation Set\"] = val_label_counts\n",
    "            label_distribution[\"Test Set\"] = test_label_counts\n",
    "\n",
    "            # **用 0 填補缺失值（表示該 Label 在該 Set 中沒有數據）**\n",
    "            label_distribution = label_distribution.fillna(0).astype(int)\n",
    "\n",
    "            from IPython.display import display\n",
    "            display(label_distribution)\n",
    "\n",
    "            import time\n",
    "            # 記錄開始時間\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "            from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "            if N_val > 0:\n",
    "                early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "            else:\n",
    "                early_stop = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "\n",
    "            # 確保變數命名一致\n",
    "            X_train_small = X_train  # 確保這裡用的變數和前面一致\n",
    "            y_train_small = y_train\n",
    "\n",
    "            # 設定 batch_size\n",
    "            batch_size = min(32, max(8, len(X_train_small) // 2))  # 避免 batch size 過大\n",
    "            # batch_size = 32\n",
    "            print(batch_size)\n",
    "\n",
    "            if N_val > 0:\n",
    "                base_model.fit(X_train_small, y_train_small,validation_data=(X_val, y_val), epochs=10000, batch_size=batch_size, callbacks=[early_stop])\n",
    "            else:\n",
    "                base_model.fit(X_train_small, y_train_small, epochs=10000, batch_size=batch_size, callbacks=[early_stop])\n",
    "\n",
    "\n",
    "            # 記錄結束時間\n",
    "            end_time = time.time()\n",
    "\n",
    "\n",
    "            # Needsave\n",
    "            # 計算訓練時間（秒）\n",
    "            training_time = end_time - start_time\n",
    "            print(f\"訓練時間：{training_time:.2f} 秒\")\n",
    "            ALL_trainingtime.append(training_time)\n",
    "\n",
    "            # Needsave\n",
    "            loss, accuracy = base_model.evaluate(X_test, y_test)\n",
    "            print(f\"Evaluation on 90% unused data - Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "            ALL_loss.append(loss)\n",
    "            ALL_accuracy.append(accuracy)\n",
    "\n",
    "            # 預測測試資料\n",
    "            y_test_pred_numeric = base_model.predict(X_test_scaled)\n",
    "            y_pred_classes = np.argmax(y_test_pred_numeric, axis=1)\n",
    "\n",
    "            # 轉換為原本的 Label\n",
    "            y_test_pred_labels = [label_mapping[str(num + 1)] for num in y_pred_classes]  # 補回 +1\n",
    "            y_test_pred_labels\n",
    "\n",
    "            # 讀取測試資料的實際 Label\n",
    "            y_test_actual = test_data_imputed[target_column]\n",
    "            test_data_imputed\n",
    "\n",
    "            # 取得預測與實際座標\n",
    "            y_test_pred_coordinates = np.array([label_to_coordinates[label] for label in y_test_pred_labels])\n",
    "            y_test_actual_coordinates = np.array([label_to_coordinates[label] for label in y_test_actual])\n",
    "\n",
    "            # 計算 MDE (Mean Distance Error)\n",
    "            distances = np.linalg.norm(y_test_pred_coordinates - y_test_actual_coordinates, axis=1)\n",
    "            mean_mde = np.mean(distances)\n",
    "\n",
    "            # 記錄每個 RP 的 MDE\n",
    "            mde_report_test = {}\n",
    "            for true_label, distance in zip(y_test_actual, distances):\n",
    "                if true_label not in mde_report_test:\n",
    "                    mde_report_test[true_label] = []\n",
    "                mde_report_test[true_label].append(distance)\n",
    "\n",
    "            # 計算測試資料的 MDE 平均值\n",
    "            mde_report_test_avg = {label: {\"mde\": np.mean(dists), \"count\": len(dists)} for label, dists in mde_report_test.items()}\n",
    "\n",
    "            # # 儲存 MDE 結果到 JSON 檔案\n",
    "            # test_file_path = f\"{root}/{modelname}.json\"\n",
    "            # with open(test_file_path, \"w\") as f:\n",
    "            #     json.dump(mde_report_test_avg, f, indent=4)\n",
    "\n",
    "            # Needsave\n",
    "            print(f\"Test Data MDE report saved to: {test_file_path}\")\n",
    "            print(f\"\\nTest Data Mean MDE: {mean_mde:.4f} meters\")\n",
    "            ALL_mean_mde.append(mean_mde)\n",
    "\n",
    "\n",
    "            base_model.save(f'DNN_best_model_{weekrepresent[i+1][0]}to{weekrepresent[i+1][1]}_{mmm}.h5')\n",
    "        \n",
    "        ALL_model_accuracy.append(ALL_accuracy)\n",
    "        ALL_model_loss.append(ALL_loss)\n",
    "        ALL_model_mean_mde.append(ALL_mean_mde)\n",
    "\n",
    "    \n",
    "    output_lines = []\n",
    "\n",
    "    # 印出每個模型\n",
    "    for count, (m, n, p) in enumerate(zip(ALL_model_loss, ALL_model_accuracy, ALL_model_mean_mde)):\n",
    "        output_lines.append(f\"model_{count}\")\n",
    "\n",
    "        output_lines.append(\"Loss\")\n",
    "        line = \"[\" + \", \".join(f\"{l:.4f}\" for l in m) + \"]\"\n",
    "        output_lines.append(line)\n",
    "\n",
    "        output_lines.append(\"Accuracy\")\n",
    "        line = \"[\" + \", \".join(f\"{a:.4f}\" for a in n) + \"]\"\n",
    "        output_lines.append(line)\n",
    "\n",
    "        output_lines.append(\"MDE\")\n",
    "        line = \"[\" + \", \".join(f\"{sasa:.4f}\" for sasa in p) + \"]\"\n",
    "        output_lines.append(line)\n",
    "\n",
    "        output_lines.append(\" \")  # 空行\n",
    "\n",
    "    # 計算平均\n",
    "    loss_arr = np.array(ALL_model_loss)\n",
    "    acc_arr = np.array(ALL_model_accuracy)\n",
    "    mde_arr = np.array(ALL_model_mean_mde)\n",
    "\n",
    "    loss_avg = np.mean(loss_arr, axis=0)\n",
    "    acc_avg = np.mean(acc_arr, axis=0)\n",
    "    mde_avg = np.mean(mde_arr, axis=0)\n",
    "\n",
    "    output_lines.append(\"Average Loss\")\n",
    "    output_lines.append(\"[\" + \", \".join(f\"{v:.4f}\" for v in loss_avg) + \"]\")\n",
    "\n",
    "    output_lines.append(\"Average Accuracy\")\n",
    "    output_lines.append(\"[\" + \", \".join(f\"{v:.4f}\" for v in acc_avg) + \"]\")\n",
    "\n",
    "    output_lines.append(\"Average MDE\")\n",
    "    output_lines.append(\"[\" + \", \".join(f\"{v:.4f}\" for v in mde_avg) + \"]\")\n",
    "\n",
    "    # 印出到螢幕\n",
    "    for line in output_lines:\n",
    "        print(line)\n",
    "\n",
    "    # 存檔到文字檔\n",
    "    with open(f\"model_metrics_{dada}.txt\", \"w\") as f:\n",
    "        for line in output_lines:\n",
    "            f.write(line + \"\\n\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
